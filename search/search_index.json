{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Earth Observation Data Hub","text":"<p>The EODH is a UK Pathfinder project delivering access to Earth Observation data for effective decision making across government, business and academia, supported by key stakeholders UKRI NERC, DSIT and UKSA.</p> <p>The project brings together a strong team of partners from across the public sector. Lead by National Centre for Earth Observation (NCEO), specifically with members situated within the Centre for Environmental Data Analysis (CEDA) and the University of Leicester, the EODH project team provide oversight and management of a consortium of experts from across research and the commercial sector, including National Physical Laboratory, Met Office and Satellite Applications Catapult.</p> <p></p>"},{"location":"#overview-of-the-project","title":"Overview of the project","text":"<p>The overall goal of the project is to develop and operate a new centralised software infrastructure \u2013 DataHub \u2013 to provide a new \u2018single point\u2019 of access for UK EO data offerings from distributed public and commercial centres. By providing a single point, the objective is to provide a standard common set of services and APIs upon which new EO services and tools can be developed and accessed by the UK EO data community. This pathfinder project will bring new thinking and experimental developments that will result in practical services for users in a short period of time.</p> <p>By the end of the project, we expect to have a community of researchers, industry and government working together to provide earth observation data in new and innovative ways. The scope of the project covers all Earth Observation data for the UK. Users of the Hub will be able to explore areas of interest in the UK and across the globe, harnessing space derived data to develop an improved understanding of our environment and create insights to support commercial and government decision-making. In the initial phase, the Hub will additionally provide service interfaces to particular climate and land applications. It will also enable selected users to support their own analyses, services and tools using the Hub\u2019s workflow and compute environments.</p>"},{"location":"#capabilities-of-the-hub","title":"Capabilities of the Hub","text":"<p>The Hub Platform provides a software toolkit to facilitate the development of applications to exploit EO Data.\u200b The platform integrates multiple data sources.\u200b It provides tools to transform and store data in analysis-ready formats.\u200b It provides the ability for users to integrate their own workflows. It will provide quality information to help users judge fitness of purpose for a given application. The Hub capabilities will facilitate:</p> <ul> <li>Time critical Sentinel processing</li> <li>Generate time-critical environmental analyses for monitoring systems by actioning workflows in real time as new image acquisitions arrive</li> <li>Sharing new analysis</li> <li>Carry out novel research, derive insights, and publish outputs, assisted by the capabilities and datasets provided by the hub, through developing EO workflows</li> <li>Enabling commercial applications</li> <li>Utilise the provision of satellite imagery and modelled EO products to power custom web applications to serve user needs, from climate risk alerts to site monitoring</li> <li>Pay per use commercial data</li> <li>Supply commercial satellite datasets to a broad end user community by hosting on the Hub</li> </ul>"},{"location":"#users-of-the-hub","title":"Users of the Hub","text":"<p>The Hub is suitable for a range of user types and roles across the government, academic and commercial disciplines. Data access can support operational government needs, as well as provide EO insights to inform policy, supporting the ability to build custom dashboard applications powered by the Hub. The Hub provides quality-assured data access for academic researchers and students, acting as a central teaching resource suitable for beginners in UK EO data processing, as well as technically advanced users. There are opportunities for commercial users to supply EO datasets to enhance the Resource Catalogue, or utilise existing imagery as part of operations and reporting.</p> <p></p>"},{"location":"#use-cases-of-the-hub","title":"Use cases of the Hub","text":"<p>Data search and browse can be carried out through the Resource Catalogue interface, within QGIS, and programmatically through our Python API client, pyeodh.</p> <p>For GIS data analysis, you will be able to interact using the EODH plugin to pull data directly into QGIS, or utilise services to integrate with other GIS software.</p> <p>Proficient programmers can point Python notebooks to run through the AppHub, a dedicated JupyterHub on the EODH, to use pyeodh or other Python libraries.</p> <p>Software developers can connect to the Hub via the dedicated APIs, either directly or through pyeodh, to link in directly with the Hub. Custom-built applications, developed as part of the wider project, and powered by the Hub, are an example of how this can enhance tailored data insights.</p> <p>Dev-Ops, and other advanced programmers, can generate workflows in CWL using the eoap-gen library, which are compatible to run on the Hub. This is a scalable solution to processing large quantities of EO data, from input to desired output.</p> <p></p>"},{"location":"#project-funding","title":"Project funding","text":"<p>Natural Environment Research Council (NERC) through the Department of Science Innovation and Technology Earth Observation Investment programme act as the key funding body. \u00a39.93 million was awarded to the lead organisation: National Centre for Earth Observation (NCEO). The project will run from February 2023 to March 2025 as a Pathfinder, with applications for extension of project funding ongoing.</p> <p>Go to FAQs </p>"},{"location":"faqs/","title":"Frequently Asked Questions","text":"<p>Find answers to the most commonly asked questions about the EO DataHub. If you have a question that has not been listed, or need further support, please contact us directly at enquiries@eodatahub.org.uk, or raise a discussion topic in our community forum.</p>"},{"location":"faqs/#where-can-i-raise-an-issue","title":"Where can I raise an issue?","text":"<p>To get support with access or account issues and share general feedback, please direct your query to enquiries@eodatahub.org.uk in the first instance. </p> <p>To report a bug, please contact enquiries@eodatahub.org.uk with the subject 'Reported bug', and the team will open a new ticket in the GitHub issue tracker under the relevant repository. Please include a clear description outlining steps taken that lead to the error being flagged or issue occurring, including screenshots where possible. This will make it as easy as possible for the team to replicate and troubleshoot the issue.</p>"},{"location":"faqs/#what-can-i-do-with-the-platform","title":"What can I do with the platform?","text":"<p>The Hub Platform and ecosystem of tools and interfaces supports a wide range of use cases, outlined below:</p> <ul> <li>Scalable EO and Climate data processing. <ul> <li>Generate environmental analyses for monitoring systems by actioning workflows with scalable compute infrastructure in real time</li> </ul> </li> <li>Share new analysis. <ul> <li>Carry out novel research, derive insights, and publish outputs, assisted by the capabilities and datasets provided by the Hub, through developing EO workflows</li> </ul> </li> <li>Analysis Ready Data pipelines with Quality Assurance. <ul> <li>Collaborate on pipelines and algorithms to generate novel ARD products to be hosted on the Hub</li> </ul> </li> <li>Enabling commercial applications. <ul> <li>Utilise the provision of satellite imagery and modelled EO data products to power custom web applications to serve end-user needs, from climate risk alerts to site monitoring</li> </ul> </li> <li>Pay per use commercial data. <ul> <li>Get centralised access to a supply of commercial satellite datasets which are hosted on the Hub</li> </ul> </li> <li>Enhance organisational reporting. <ul> <li>Produce map visualisations in light-touch user interfaces or external GIS software to showcase geospatial insights for project reports</li> </ul> </li> <li>Migrate operational workflows. <ul> <li>Connect operational workflows hosted in compatible platforms to your Hub workspace, to automate processing of dataflows directly from the Resource Catalogue</li> </ul> </li> <li>Notebooks for education. <ul> <li>Create educational resources for EO analysis and applications, powered by the Hub API, by writing or exploiting community-developed notebooks</li> </ul> </li> </ul>"},{"location":"faqs/#where-can-i-find-help","title":"Where can I find help?","text":"<p>All training materials for the DataHub are consolidated in these documentation pages.</p> <p>To start a discussion with the user community, contribute to our community forum.</p> <p>Users can contact the helpdesk at enquiries@eodatahub.org.uk for support with individual access and account issues, or custom queries.</p>"},{"location":"faqs/#how-can-i-work-in-my-team","title":"How can I work in my team?","text":"<p>Group workspaces can be created on behalf of an organisation, where all the members of a workspace can access the workflows and results stored in this location. This allows for sharing and collaboration across teams. To request a group workspace, follow the guidance provided here.</p> <p>Users can create workspaces using the Workspace UI and then grant access to other EODH users by adding members to the workspace.</p> <p></p> <p>Add new workspace</p>"},{"location":"faqs/#how-can-i-login-to-my-workspace","title":"How can I login to my workspace?","text":"<p>Instructions on how to set up an account with the EODH and request a user workspace can be found here. An individual workspace can be accessed through a range of access points.</p>"},{"location":"faqs/#hub-platform","title":"Hub platform","text":"<p>The primary point of access for a Hub workspace is through the Hub platform. To access your workspace through this entry point, you must be signed into the Hub platform as an EODH user. At present, you need to first create an account on GitHub. GitHub is an identity provider which the EODH Platform is integrated with. Once you have a GitHub account, you can use the credentials to sign in to the EODH Platform.\u200b</p>"},{"location":"faqs/#single-sign-on","title":"Single Sign-On","text":"<p>The EODH Platform is integrated with GitHub and additional identity providers (Google or Microsoft).\u200b</p>"},{"location":"faqs/#qgis-plugin","title":"QGIS Plugin","text":"<p>A user can login to their workspace through the QGIS Plugin interface, by carrying over an API key, generated here, and utilising their Hub login username (matching their GitHub username).</p>"},{"location":"faqs/#applications","title":"Applications","text":"<p>A user will be able to create and see existing workflows in their workspace through the UI of custom-built applications powered by the EODH.</p>"},{"location":"faqs/#how-can-i-access-my-results","title":"How can I access my results?","text":"<p>When you execute a workflow, the outputs will automatically be populated into your workspace AWS S3 Storage and personal Resource Catalogue. A user will be able to search for their workflow results using the workspace user interface.</p> <p>The job results can be access via the ADES, using the following endpoint format, given your workspace and job-id which produced the outputs:</p> <pre><code>staging.eodatahub.org.uk/ades/&lt;workspace&gt;/ogc-api/jobs/&lt;job-id&gt;/results\n</code></pre> <p>This will return the generated collection for the results, you can then use the links contained within this dataset to access the data directly from S3, where it is hosted. Your results will also be available in your resource catalogue catalog:</p> <pre><code>https://staging.eodatahub.org.uk/api/catalogue/stac/catalogs/user-datasets/&lt;workspace&gt;/processing-results\n</code></pre> <p>You can search for the job-id that produced the resulting data.</p>"},{"location":"faqs/#who-is-the-datahub-suitable-for","title":"Who is the DataHub suitable for?","text":"<p>Through its various access points, the EO DataHub was built to support a range of Earth Observation professionals, including GIS users, Web Developers, Dev-Ops, Researchers and Data Scientists, by integrating with widely-adopted standardised tools and APIs.</p>"},{"location":"faqs/#what-data-is-offered","title":"What data is offered?","text":"<p>The resource catalogue contains both open access and commercial Earth Observation datasets. Optical imagery (including Sentinels), Synthetic Aperture RADAR (SAR), observed and modelled climate products, are available. No LiDAR datasets are currently available in the resource catalogue.</p>"},{"location":"faqs/#open-access-data","title":"Open access data","text":"<p>The catalogue of open access imagery includes publicly available Sentinel 1 and Sentinel 2 Analysis-Ready Data, provisioned by the CEDA Data Archive catalogue. The Earth Observation Climate Information Service (EOCIS), ESA Climate Change Initiative (CCI), and HadUK-Grid collectively provide the climate observation-derived data products available on the Hub. UK Climate Projections (UKCP), The Co-Ordinated Regional Downscaling Experiment (CORDEX), and CMIP6 supply modelled climate variable products to the Resource Catalogue, detailed specifications both of which can be found here.</p>"},{"location":"faqs/#commercial-data","title":"Commercial data","text":"<p>The Hub platform facilitates purchasing of commercial imagery from Airbus and Planet, on the condition that users hold an existing user account with these providers. PlanetScope and SkySat imagery can be purchased from Planet. Pl\u00e9iades, Pl\u00e9iades Neo, SPOT and TerraSAR-X imagery can be purchased from Airbus. More information on these data specifications, as well as guidance on purchasing and licensing restrictions, can be found here.</p>"},{"location":"faqs/#can-i-access-data-from-my-infrastructure-how","title":"Can I access data from my infrastructure? How?","text":"<p>Yes, you will be able to upload your own data to your personal AWS S3 storage as long as you have an active Hub Platform account and a workspace.</p>"},{"location":"faqs/#how-can-i-access-data-from-my-workspace","title":"How can I access data from my workspace?","text":"<p>Your data will be saved in your personal Resource Catalogue as well as the AWS S3 storage associated with your workspace. You will be able to search your data using the workspace user interface. The Hub Platform will attempt to convert all data in your workspace into STAC records so that it can be publicly published if a user wishes to do so.</p>"},{"location":"faqs/#can-i-trust-this-platform","title":"Can I trust this platform?","text":""},{"location":"faqs/#security","title":"Security","text":"<p>The EODH platform implements the OIDC spec to manage user authentication and authorisation securely. Users log into the platform using third party Identity Providers (IdPs). This brings the benefit of using 3rd party IdPs\u2019 production hardened authentication mechanisms, such as GitHubs MFA functionality, as well as their secure storage of user credentials. Currently only GitHub IdP is integrated with the platform, but other integrations are planned.</p> <p>User access to workspaces is controlled by group membership. Users cannot access workspaces for which they are not an owner or a member, and workspace group membership is controlled by workspace owners through the platform web UI.</p> <p>User workspace data is protected by tested platform level authentication and authorisation policies. Where the data store is backed by an AWS resource, such as S3 or EFS, then they are also protected by scoped AWS IAM policies tied to a user\u2019s platform web identity.</p>"},{"location":"faqs/#sustainability","title":"Sustainability","text":"<p>The project is currently in a Pathfinder phase (2023-2025) that brings new thinking and experimental developments, resulting in practical services for a variety of users in a short period of time. By the end of the pathfinder phase (March 2025), it is expected that there will be a community of researchers, industry and government users working together to provide and interact with EO data in new and innovative ways.</p> <p>Funding for the Earth Observation DataHub project has not currently been extended beyond the Pathfinder phase, ending on 31st March 2025, with applications for extension of project funding ongoing.</p>"},{"location":"faqs/#quality-assurance","title":"Quality Assurance","text":"<p>The EODH Quality Assurance (QA) Service aims to give users confidence in the quality of the data they\u2019re accessing, as an integral part of the Hub offering.</p> <p>QA checks consist of both a review of the processes by which data products are created, and quantitative validation. The aim is to ensure that data are fit for purpose and performing at the level they claim.</p> <p>The QA results are stored as annotations in the product catalogue. When searching for data, the QA annotations can be used in the query parameters so you can find data that fits your needs. Detailed QA information is available on a specific page for a given collection \u2013 outlining the results for documentation reviews, when checks were last run and whether they passed.</p> <p>In the EODH Pathfinder, these checks have been carried out for various satellite data collections from the initial optical missions available on the Hub (Sentinel-2, Planet SuperDove, and Airbus Pl\u00e9iades).</p>"},{"location":"faqs/#whats-the-minimum-area-for-a-commercial-data-order","title":"What's the minimum area for a commercial data order?","text":""},{"location":"faqs/#airbus","title":"Airbus","text":"<p>For Airbus, Optical PPO contracts have a minimum archive order size of 25km2. Radar PPO contracts have a minimum order size of one full scene, meaning Radar imagery orders cannot be clipped to an AOI. There are options available for very granual data access - please contact andrew.tewkesbury@airbus.com to discuss.</p>"},{"location":"faqs/#planet","title":"Planet","text":"<p>For Planet, we would recommend people with specific needs contact Planet directly. Planet do have a minimum order size, but that could for example refer to a data distributed over a long period. And for that case only SkySat would work.</p>"},{"location":"Getting-Started/access-levels/","title":"Access levels","text":"<p>Access levels for users</p> <p>There are three main Access Levels for the Hub. These are listed below along with the main  features of each. As a user interacts with the Hub more, then they will move through the three access levels to create a more powerful account.</p>"},{"location":"Getting-Started/access-levels/#open-access","title":"Open Access","text":"<ul> <li>Browse the web presence</li> <li>Access the documentation tabs (About, Data, Getting Started)</li> <li>Access and use the resource catalogue UI (STAC Browser)</li> </ul>"},{"location":"Getting-Started/access-levels/#user-account-only","title":"User Account only","text":"<ul> <li>Access the Workspace UI (but no permissions to create or manage a workspace)</li> <li>Be added to a EODH workspace managed by another EODH user which would provide the same access as users with a billing account</li> </ul>"},{"location":"Getting-Started/access-levels/#user-account-billing-account","title":"User Account + Billing Account","text":"<ul> <li>Create and manage their own workspace (e.g. add members) in the Workspace UI</li> <li>Access the Notebook service and be able to create and run data processing / analysis workflows *Use Application Services</li> </ul>"},{"location":"Getting-Started/billing-accounts/","title":"Billing accounts","text":"<p>Info</p> <p>You must have a User Account before you can create a billing account.</p> <p>Info</p> <p>Billing accounts currently do not involve any payments or charges. They simply act as a setup step for future billing features.</p>"},{"location":"Getting-Started/billing-accounts/#why-do-i-need-a-billing-account","title":"Why do I need a billing account?","text":"<p>Billing accounts are a required step before creating a workspace and accessing compute capabilities like Notebooks or Application Services. While billing itself is not yet active, creating an account ensures you're ready once billing features are introduced.</p>"},{"location":"Getting-Started/billing-accounts/#how-to-create-a-billing-account","title":"How to create a billing account","text":"<p>Click on your profile icon in the top right corner of the homepage and Manage Account. Fill out the form as instructed Submit the form</p> <p></p> <p>Once submitted, your billing account request will be sent for moderator approval. This is expected to take up to 2 working days. After it is approved, you'll be able to create and manage your own workspaces. You will receive a notification via the email associated with your user account when your request has been processed. If you cannot see the email, please check your junk mail folder.</p> <p>Tip</p> <p>If your account hasn't been approved within several days, please contact us at enquiries@eodatahub.org.uk for assistance.</p> <p>Create a workspace </p>"},{"location":"Getting-Started/user-accounts/","title":"User accounts","text":"<p>This guide walks you through the steps to register as a user to the platform using an external Identity Provider (IdP).</p>"},{"location":"Getting-Started/user-accounts/#sign-in","title":"Sign in","text":"<p>To register, click on the 'Sign in' button on the homepage.</p> <p> </p> <p>This will bring you to a sign in form. To create an account on the Hub you must have either a GitHub or Gmail account. If you don't already have one of the above, first create a GitHub or Gmail account. Click on either IdP at the bottom of the sign in form.</p> <p> </p> <p>This will prompt you to input your chosen IdPs user credentials. Logging in for the first time will also prompt you to update account information, as shown below.</p> <p></p>"},{"location":"Getting-Started/user-accounts/#authorise","title":"Authorise","text":"<p>When logging into the hub for the first time via GitHub, you will be asked to authorise EO-DataHub using GitHub keycloak, so the account can be used to login.</p> <p></p>"},{"location":"Getting-Started/user-accounts/#access-permissions","title":"Access permissions","text":"<p>Once signed in to the platform, you can access the Workspaces tab, but will not have permissions to create or manage a workspace. You can also be added to an EODH workspace managed by another EODH user, which would provide the same access as users with a billing account.</p>"},{"location":"Getting-Started/workspaces/create-workspace/","title":"Create a Workspace","text":"<p>A workspace will allow workflows, data and results to be stored on the Hub. It provides the facility for users to analyze data, process datasets, make commercial orders and generate value-added outputs within the hosted Hub environment. Having a workspace also allows you to use the application services.</p> <p>Info</p> <p>You must have a billing account to create a workspace</p> <p>To create a workspace click on the Workspaces page in the top nav bar. Then click the '+' icon as shown below.</p> <p></p> <p>You must choose a name for the workspace. Workspace names can only include lowercase letters, numbers, and hyphens. It must start and end with a letter or a number. The account list shows which billing account you want to associate your workspace with. Only approved billing accounts will be able to have workspaces created.</p> <p></p>"},{"location":"Getting-Started/workspaces/create-workspace/#role-permissions","title":"Role permissions","text":"<p>When created, the workspace should be ready to use and can be seen in the interface, as shown below. By creating the workspace, you become the Account Owner of it. You cannot remove yourself as a member of a workspace you own, but you can add users to it.</p> <p></p>"},{"location":"Getting-Started/workspaces/linked-accounts/","title":"Linked Accounts","text":"<p>Note</p> <p>Linked accounts allow you to store your provider API key in the Hub allowing you to order commercial data within the context of a specific workspace. The platform currently supports two commercial data providers for this, namely Airbus and Planet.</p>"},{"location":"Getting-Started/workspaces/linked-accounts/#connect-up-to-your-commercial-data-provider","title":"Connect up to your commercial data provider","text":"<p>Navigate to the workspaces tab and click on Linked Accounts as shown below.</p> <p></p>"},{"location":"Getting-Started/workspaces/linked-accounts/#link-your-airbus-account","title":"Link your Airbus account","text":"<p>Required for accessing Airbus data on the Hub. Request a OneAtlas account via the website. Configure your OneAtlas account to send data to the EODH by following these instructions. The API should be entered as a one-time input in Workspaces &gt; Linked accounts as seen below.</p> <p></p> <p>After clicking 'Validate API key', you should be presented with your contract ID options for PNEO and Legacy orders, and an indication if your account is set up for Radar orders. Select appropriate contracts if required, and then link your account.</p> <p></p> <p>Contact ukintelligence-imagerysupport@airbus.com for dedicated support with Airbus access. If you are not presented with contracts, please ensure you have followed these steps to set up your Airbus account before contacting support.</p>"},{"location":"Getting-Started/workspaces/linked-accounts/#link-your-planet-account","title":"Link your Planet account","text":"<p>Required for accessing Planet data on the Hub. Create a Planet account, if you don't currently have one, by emailing the Planet team. Connect to the EODH using your API key, which can be found here. Input the API key under Workspaces &gt; Linked accounts and click Link Account, as shown below. This is a one-time input.</p> <p></p> <p>Once you have input a valid API key and validated it, you can then proceed to link it to your workspace.</p> <p></p> <p>Contact eodatahub@planet.com for dedicated support with Planet access.</p>"},{"location":"Getting-Started/workspaces/linked-accounts/#ready-to-order","title":"Ready to order","text":"<p>Once you have connected your account with the relevant data provider to your workspace, you will now be able to order data via the EODH from that provider, either within the Resource Catalogue, or from a notebook in JupyterHub.</p> <p>Order commercial data </p>"},{"location":"Getting-Started/workspaces/member-management/","title":"Member Management","text":""},{"location":"Getting-Started/workspaces/member-management/#role-permissions","title":"Role Permissions","text":"<p>Each workspace has 2 possible roles</p> <ul> <li>Admin</li> <li>Member</li> </ul> <p>The Admin is the owner of the workspace. They can add/remove members, link commercial accounts (see Linked Accounts) and are responsible for any billing events.</p> <p>The Member is a user that has been invited to collaborate within the workspace, with more limited permissions. They cannot take part in any administrative tasks of owning the workspace but can contribute to the workspaces data, access shared resources and use tools and integrations that are available in the platform.</p>"},{"location":"Getting-Started/workspaces/member-management/#add-a-user-to-my-workspace","title":"Add a user to my workspace","text":"<p>Click Add Member button in the workspaces UI for a workspace you have selected on the left. Then you add the user by statement username they have provided you with. If successful, they will appear in the UI, with key information about them such as Email, Name and Date Added.</p> <p>When the other user logs in they will then see your workspace in their UI and be able to utilize it within the platform.</p> <p></p>"},{"location":"Getting-Started/workspaces/member-management/#remove-a-user-from-my-workspace","title":"Remove a user from my workspace","text":"<p>If you want to remove a member from your workspace you must click the Bin icon within the workspace UI. This will prompt you for confirmation of doing so. The user will immediately be forbidden from utilizing the workspaces features. Only members can be removed from the workspace.</p> <p></p>"},{"location":"Getting-Started/workspaces/workspace-credentials/","title":"Workspace Credentials","text":"<p>To connect securely to your EODH workspace from external tools and services\u2014such as APIs, scripts, data processing pipelines, or GIS applications like QGIS\u2014you need to generate appropriate credentials that authenticate your access.</p>"},{"location":"Getting-Started/workspaces/workspace-credentials/#datahub-api-key","title":"DataHub API Key","text":"<p>You need to generate an API key scoped to your workspace to securely authenticate with the DataHubs API and related tools, such as the QGIS plugin. This key acts as a unique identifier that verifies your access rights and ensures that only authorized users can interact with the data and services tied to your workspace. By scoping the key specifically to your workspace, you maintain control over what resources can be accessed, and you ensure that the authentication process is both secure and aligned with your organization\u2019s data governance policies. This is especially important when using integrations like the QGIS plugin, which rely on authenticated API access to fetch, visualize, and manage geospatial data directly from your DataHub environment.</p> <p>To obtain an API token, visit the Workspaces tab. Choose the desired workspace and then select Credentials on the left hand panel. Under DataHub API Key click the Request New Token button. Enter a name and expiry time.</p> <p></p> <p>Warning</p> <p>The maximum duration of an API token is 30 days.</p> <p>Copy the Token ID and API Key, these will not be available to view later. These API tokens will be used to authenticate your workspace with external and internal services within the platform.</p> <p></p>"},{"location":"Getting-Started/workspaces/workspace-credentials/#s3-token","title":"S3 Token","text":"<p>Your workspace includes an object store. By generating temporary S3 credentials, you are provided with secure, time-limited access to your workspace\u2019s data buckets, enabling you to upload, download, or stream data files directly. This is particularly useful when working with large datasets in external applications or when integrating with cloud-based processing tools.</p> <p>You can generate temporary tokens to access your workspaces S3 bucket programmatically. Please see Using Your Temporary AWS S3 Credentials for more information.</p>"},{"location":"case-studies/dalcour-maclaren-bng/","title":"Measurements Supporting Biodiversity Net Gain (BNG)","text":"<p>Authored by Bonnie Pickard</p> <p>Keywords: Habitat, Biodiversity Net Gain, Sustainable Development, Airbus Pl\u00e9iades Neo, Planet PlanetScope, Sentinel-2, Pixel Classification, NDVI, Change Detection, Deep Learning Models</p>"},{"location":"case-studies/dalcour-maclaren-bng/#supporting-measurements-of-biodiversity-net-gain-using-high-resolution-commercial-earth-observation-data","title":"Supporting Measurements of Biodiversity Net Gain Using High-Resolution Commercial Earth Observation Data","text":"<p>Recent advancements in the resolution and availability of satellite imagery have the potential to aid the UK\u2019s goals for sustainable development in the construction industry, by helping to measure and understand habitat biodiversity. This case study demonstrates how high-resolution commercial satellite data, obtained via the EO DataHub, can support the broad identification and monitoring of existing habitats, and highlight other areas for potential habitat creation. This therefore helps to mitigate against negative impacts from development, in the effort to promote Biodiversity Net Gain.  Leaving nature better than you found it Biodiversity Net Gain (BNG) is an approach to development and land management, which aims to leave the natural environment in a measurably better state than it was beforehand, by creating or enhancing habitats. Under the Environment Act 2021, it is now a legal requirement for developers across England to deliver at least 10% BNG for most major developments. BNG has become mandatory for all small sites from April 2024 and Nationally Significant Infrastructure Projects (NSIPs) from November 2025.</p> <p>The statutory biodiversity metric, published by the Department for Environment, Food and Rural Affairs (DEFRA), provides the means to measure standardised biodiversity units for the purposes of BNG. A habitat will contain a number of biodiversity units, depending on its size, quality, location, and type. The number of biodiversity units present before and after a development are typically measured during surveys conducted by ecologists. The aim of the BNG initiative is to ensure a net increase in the number of biodiversity units around the development, thus delivering a positive impact on habitat biodiversity.</p> <p>The EO DataHub provided the opportunity to access and analyse both commercial and open-source satellite data from Sentinel, Airbus, and Planet. The data was analysed using geoprocessing tools and deep learning packages in ArcGIS Pro (Esri). Figure 1 shows the example region of interest around Stonehouse, Gloucestershire.</p> <p>INSERT IMAGE</p> <p>Figure 1: Area of interest around Stonehouse, Gloucestershire, UK \u00a9 OpenStreetMap contributors.</p>"},{"location":"case-studies/dalcour-maclaren-bng/#using-high-resolution-commercial-data-to-identify-habitat-type","title":"Using high resolution commercial data to identify habitat type","text":"<p>Deep learning computer models that use pixel-based classification (also known as segmentation), can be trained on satellite data to identify habitat and land cover type. The model or \u2018neural network\u2019 is ideally trained on thousands of labelled image \u2018chips\u2019, which are representative samples of each object or class in the chosen imagery, and are selected by the user. For example, these could be buildings, lakes, road segments, or forests. The model then seeks to classify each pixel of an image according to the spatial and spectral properties of the object or class it\u2019s been trained on. The size of each identified habitat can then be calculated.</p> <p>Figure 2 depicts the results of running a pre-trained deep learning model on commercial 30cm resolution Airbus Pl\u00e9iades Neo imagery. This model, available from ArcGIS Living Atlas, has been trained on the Chesapeake Bay high-resolution 2013/2014 NAIP Landcover dataset (produced by Chesapeake Conservancy with University of Vermont Spatial Analysis Lab (UVM SAL), and Worldview Solutions, Inc. (WSI)). Although the results of the analysis provide a fair approximation of habitat type, further fine-tuning of the model will be required to generate a more accurate representation across the region of interest. This could be achieved by the user generating their own training dataset with more diverse habitat classes, using the Pl\u00e9iades Neo imagery to retrain the model.</p> <p>INSERT IMAGE</p> <p>Figure 2: Land cover type identified using a pre-trained deep learning model on a 30cm resolution Pl\u00e9iades Neo image \u00a9 Airbus DS (2023).</p> <p>The application of remote sensing data to identify habitat type may help ecologists to broadly map the spatial distribution of habitats before conducting on-site surveys, and thus increase efficiency. This may also aid the detection of land parcels suitable for habitat creation both on and off-site.</p>"},{"location":"case-studies/dalcour-maclaren-bng/#using-high-resolution-commercial-data-to-assess-habitat-condition","title":"Using high resolution commercial data to assess habitat condition","text":"<p>The Normalized Difference Vegetation Index (NDVI) is a standardised metric to measure vegetation health or \u2018greenness\u2019. NDVI is calculated by comparing the red and near-infrared bands from multispectral satellite data and its value can vary from 1 to -1. Negative NDVI values indicate no vegetation at all, such as water, and values close to 0 represent concrete, rock, or bare soil. Moderate to high positive values indicate grasslands, forests and lush vegetation.</p> <p>INSERT IMAGE</p> <p>Figure 3a &amp; 3b: NDVI values of 3m resolution PlanetScope images \u00a9 2022 &amp; 2024 Planet Labs PBC. Figure 3c: Amount of change in NDVI values between summer 2022 to 2024. </p> <p>Figures 3a &amp; 3b illustrate the measurement of NDVI values for two commercial Planet PlanetScope images, two years apart, during the summer season. Higher (green) values relate to areas of healthy woodland and agriculture whilst lower (red to yellow) values indicate buildings and sparse vegetation. Regular revisit times of satellites allow multiple images to be captured quickly. Change detection is the process with which to quantify the type, magnitude, and location of change by comparing multiple satellite images through time. Figure 3c shows the net change in NDVI values for the same area over the two-year period. Please note that in this example, the change in NDVI has been measured from two discrete images and does not represent an average or take seasonal fluctuations into account. The area of each type of change can also be calculated.</p> <p>NDVI, as well as other remote sensing-derived indexes, can therefore be used to broadly determine the condition of each habitat. NDVI calculations from satellite data, coupled with change detection methods can also allow the habitat condition to be monitored remotely over the life cycle of the development. However, information derived from satellite imagery should be validated and used in conjunction with on the ground surveys.</p>"},{"location":"case-studies/dalcour-maclaren-bng/#powered-by-the-eo-datahub","title":"Powered by the EO DataHub","text":"<p>In summary, this study demonstrates how high-resolution satellite data has the increasing potential to support the measurement of biodiversity units alongside on the ground surveys, to ensure BNG, and prevent habitat loss.</p> <p>The Earth Observation DataHub delivers the unique capability to quickly access and obtain Earth Observation data from multiple open source and commercial sources in one centralised platform. The EODH resource catalogue provides an easy-to-use, no-code interface within which to browse and identify relevant EO data, using filters to search various collections. Images from different data providers can be compared using the ability of the resource catalogue to pin and view multiple images. The EODH is an effective way to improve awareness of, and access to, a wide range of EO data, without the need for any programming experience.</p>"},{"location":"case-studies/dalcour-maclaren-bng/#about-us","title":"About us","text":"<p>Dalcour Maclaren specialise in providing BNG services tailored to the unique needs of the utilities and infrastructure sectors, offering expertise in ecology, surveying, planning and land. Find out more about solution-led approaches to delivering BNG at Dalcour Maclaren.</p> <p>INSERT IMAGE</p>"},{"location":"case-studies/npl-cal-val/","title":"Calibration and Validation","text":"<p>Authored by Madeline Stedman at National Physical Laboratory</p> <p>Keywords: Uncertainty, Cross-calibration, Satellite Intercalibration, Climate, TRUTHS, Sentinel-2, Landsat, Copernicus, CAL/VAL, Calibration, Validation, Planet SuperDove, Airbus Pl\u00e9iades</p>"},{"location":"case-studies/npl-cal-val/#enhancing-satellite-calibration-and-validation-methodologies-with-high-resolution-commercial-earth-observation-data","title":"Enhancing Satellite Calibration and Validation Methodologies with High-Resolution Commercial Earth Observation Data","text":"<p>A new generation of high-accuracy SI-Traceable satellite \u201cSITSat\u201d missions aim to serve as \u201cgold-standard\u201d references for satellite calibration. This case study demonstrates how high-resolution commercial satellite data via EO DataHub can support development of more detailed models to improve understanding of satellite calibration and validation uncertainties. This therefore helps to refine best practices in preparation for upcoming SITSat missions.</p> <p>Remote sensing plays a crucial role in monitoring the Earth's climate, providing critical data to support decision-making. As our global Earth observing system expands to include a rapidly increasing number of both space agency missions and low-cost commercial constellations, ensuring data consistency across different sensors is a key challenge. For example, calibration biases make it difficult to integrate data from multiple sources seamlessly, limiting our ability to achieve comprehensive environmental monitoring.</p> <p>INSERT IMAGE</p> <p>Figure 1: Libya-4, desert Pseudo-Invariant Calibration Site (PICS), visualised in true colour with Pl\u00e9iades imagery \u00a9 CNES (2024), Distribution Airbus DS</p> <p>To address this challenge, radiometric harmonisation can be performed to ensure consistency between different satellite sensors, by attempting to minimise the biases between their measurements. This is typically achieved through: * Vicarious calibration, using reference data from ground-based instrumented sites, like RadCalNet. * On-orbit cross-calibration, where images taken by satellites when their orbits cross paths are compared directly \u2013 called matchups.</p> <p>The next generation of high-accuracy SI-traceable satellites (SITSats) aim to serve as \u201cgold-standard\u201d calibration references for on-orbit cross-calibration. This includes missions such as ESA\u2019s planned TRUTHS (Traceable Radiometry Underpinning Terrestrial- and Helio- Studies) and NASA\u2019s CLARREO (Climate Absolute Radiance and Refractivity Observatory) Pathfinder (CPF) missions.</p> <p>However, differences in sensor designs, such as in their spectral response or spatial resolution, mean that comparisons of raw matchup images are not truly \u201capples to apples\u201d. These observational differences between sensors must be compensated for to estimate fundamental calibration biases. For example, the differences in spatial resolution can be compensated by aggregating the pixels from each sensor to form a sufficiently large common comparison field-of-view, such that the aggregated pixels from both sensors now cover approximately the same area.</p> <p>INSERT IMAGE</p> <p>Figure 2: Illustration of spatial resampling in satellite matchups. The orange region shows collocated observations, with swaths from a lower resolution (green) and higher resolution (blue) satellite. Thick dark green and blue lines represent the lower-resolution sensor\u2019s comparison field-of-view and the higher-resolution sensor\u2019s reconstruction, respectively.</p>"},{"location":"case-studies/npl-cal-val/#using-high-resolution-commercial-data-to-assess-spatial-uncertainties","title":"Using high resolution commercial data to assess spatial uncertainties","text":"<p>Despite this processing, satellite mis-pointing and the variability of the underlying landscape limits how well the aggregated measurements in a comparison field-of-view match \u2013 ultimately limiting the performance achievable in any comparison.</p> <p>To quantify and mitigate these effects, we have used high-resolution Airbus Pl\u00e9iades (2m native resolution) and Planet SuperDove (3m native sampling) imagery. The fine spatial information provided by this imagery enabled us to produce detailed simulations of matchup scenarios over key Cal/Val (Calibration/Validation) sites, including Libya-4 (a widely used desert Pseudo-Invariant Calibration Site (PICS)), and RadCalNet sites (mentioned above). We could then make use of these high-resolution simulations to test how sensor geolocation uncertainty, surface heterogeneity, and field-of-view selection impact spatial resampling performance.</p> <p>INSERT IMAGE</p> <p>Figure 3: 2 m resolution Airbus Pl\u00e9iades TOA radiance image (left), and subset of simulated 10 m (middle) and 50 m (right) TOA radiance images for Libya-4 desert scene at 500 nm, with a 20 degree relative rotation.</p> <p>Our results show that selecting an appropriate region of interest (ROI) for the comparison, based on sensor geolocation accuracy (from mis-pointing) and surface variability, can reduce uncertainty. The figure below illustrates this for a simulated comparison between TRUTHS (50 m resolution) and ESA\u2019s Sentinel-2 (10 m resolution). Plotted is the uncertainty on the simulated aggregated comparison field-of-views (200 m resolution), caused by sensor noise and geolocation knowledge. The geolocation uncertainties of Sentinel-2 and TRUTHS are 0.5 pixels, i.e. 5 m and 25 m respectively. These results show that this geolocation uncertainty dominates in this processing step across the spectral range, contributing uncertainties of up to 0.25 %.</p> <p>INSERT IMAGE</p> <p>Figure 4: Uncertainty components for spatially resampled Sentinel-2 (target) and TRUTHS (reference) simulated radiances over a Libyan desert scene, derived from similar Pl\u00e9iades spectral bands. Combined uncertainty is obtained by combining noise and geolocation uncertainties in quadrature.</p>"},{"location":"case-studies/npl-cal-val/#refining-on-orbit-calibration-best-practices","title":"Refining on-orbit calibration best practices","text":"<p>This study demonstrates how high-resolution commercial imagery supports the development of more detailed models to better understand satellite comparison uncertainties. These insights help refine best practices for on-orbit calibration, ensuring readiness for upcoming SITSat references like TRUTHS and CPF. As Earth Observation continues to evolve, analyses such as this can therefore help to maximise the effectiveness of SITSat missions, ultimately contributing to a more robust, harmonised global monitoring system.</p>"},{"location":"case-studies/npl-cal-val/#about-us","title":"About Us","text":"<p>Find out more about research at the National Physical Laboratory.</p>"},{"location":"case-studies/seagrass-naturescot/","title":"Seagrass monitoring in the Sound of Barra","text":"<p>Authored by Karen Frake and Chris Nall at NatureScot</p> <p>Keywords: Environment, Seagrass, ArcGIS Pro, Airbus, SPOT, Pl\u00e9iades, Pl\u00e9iades Neo, Special Area of Conservation</p>"},{"location":"case-studies/seagrass-naturescot/#seagrass-monitoring-in-the-sound-of-barra-outer-hebrides-scotland","title":"Seagrass monitoring in the Sound of Barra, Outer Hebrides, Scotland","text":"<p>High resolution imagery is essential for monitoring the seagrass at the Sound of Barra, on the west coast of Scotland in the Outer Hebrides. The EO DataHub was used to determine the availability of commercial satellite imagery for this area, and to download samples for further analysis.</p> <p>The Sound of Barra, located off the Scottish coastline in the Outer Hebrides, is a Special Area of Conservation (SAC). It was designated for two Annex I European Habitats, including sandbanks that are slightly covered by seawater all the time, which encompasses seagrass beds. In addition, seagrass beds are a Priority Marine Feature, and contribute to carbon sequestration, provide shelter to a range of other species and prevent erosion of the seabed. However, in the Sound of Barra the subtidal sandbanks are in unfavourable condition. This classification reflects the loss and degradation of seagrass beds and maerl beds within the site. Tracking the health and extent of these beds is, therefore, essential for long-term habitat management and restoration.</p> <p>INSERT IMAGE</p> <p>Figure 1: Underwater view of healthy seagrass beds, a Priority Marine Feature</p>"},{"location":"case-studies/seagrass-naturescot/#the-need-for-improved-mapping","title":"The need for improved mapping","text":"<p>Mapping extent of seagrass beds is important for monitoring the condition of beds in the SAC, to help protect these habitats. The seagrass reduces in coverage over the winter and increases over the summer, with a maximum coverage reached in August to September. Peak coverage conditions are best for delineating the seagrass bed extents, making the months of August and September a suitable date constraint for the image search.</p> <p>While aerial imagery is available through national monitoring contracts, it's typically collected only every three to four years. Infrequency of aerial survey flights means this data doesn't provide the temporal resolution needed to capture seasonal changes. In addition, the aerial imagery contract is confined to inshore waters, precluding capture for some seagrass beds that are further offshore. To supplement this, high resolution satellite imagery offers another way to monitor the extent of seagrass between these years, and a way to monitor seasonal change in coverage within a single year.</p> <p>INSERT IMAGE </p> <p>Figure 2: Map of area of interest in Sound of Barra, Outer Hebrides, Scotland</p>"},{"location":"case-studies/seagrass-naturescot/#seagrass-conservation-and-protection","title":"Seagrass conservation and protection","text":"<p>Seagrass beds (Zostera spp.) are an OSPAR threatened and / or declining habitat and a Priority Marine Feature (PMF) in Scotland. Seagrass beds are a protected feature in a number of Marine Protected Areas (MPAs). The MPA network includes Nature Conservation MPAs, Special Areas of Conservation (SACs), Special Protection Areas (SPAs) and Sites of Special Scientific Interest (SSSI). Sites with seagrass beds include Sound of Arisaig; Sound of Barra; Loch nam Madadh; Dornoch Firth and South Arran. The feature can be part of the following broadscale habitats: Lagoons; Mudflats and sandflats; Sandbanks which are slightly covered by sea water all the time; and, Estuaries. European sites (SACs and SPAs) are internationally important for threatened habitats and species.</p>"},{"location":"case-studies/seagrass-naturescot/#streamlining-image-search","title":"Streamlining image search","text":"<p>The EO DataHub was used to search for suitable cloud-free, high-revisit imagery. By determining the availability of useful Airbus data for the application area using EODH search functionality, a fitness-for-purpose assessment could be carried out prior to analysis. The commercial data catalogue was used to define a targeted image search, including parameters such as:</p> <ul> <li>Specified date ranges (months of August and September)</li> <li>Defined area of interest (as a bounding box)</li> <li>Cloud coverage area range constraints</li> </ul> <p>The search returned a range of candidate images, which were initially assessed through thumbnails. The results included imagery from three commercial data collections: * Airbus SPOT6 (1.5m resolution) * Airbus PHR (0.5m resolution) * Airbus PNEO (30cm resolution)</p> <p>Four high-quality images captured between 2023 and 2024 were selected for further analysis. Python notebooks were used to automate and streamline the ordering process, simplifying the transition from search to acquisition.</p> <p>INSERT IMAGE</p> <p>Figure 3: Seagrass in the Sound of Barra, seen from a cloud-free Pl\u00e9iades Neo acquisition \u00a9 Airbus DS 2023</p>"},{"location":"case-studies/seagrass-naturescot/#analysis-continuity-in-arcgis-pro","title":"Analysis continuity in ArcGIS Pro","text":"<p>The next step was to download these images to a local machine in order to work with them directly in ArcGIS Pro. Once the image files were imported into the software, a direct visual comparison was made between the commercial satellite data and an aerial imagery dataset. The Classification Wizard in ArcGIS Pro, an inbuilt GIS tool for image classification, was used to preprocess the aerial imagery for quantification over a long time range prior to carrying out the intercomparison. Minimal visible seasonal change was found to be present. Synergising commercial data provided by the EO DataHub with aerial photography and other datasets external to the Hub allowed for advanced insights to be derived from combined sources to inform future conservation initiatives.</p>"},{"location":"case-studies/seagrass-naturescot/#powered-by-the-eodh","title":"Powered by the EODH","text":"<p>Using the EODH as a single point of access for high resolution and high revisit cloud-free commercial data simplified the approach to seasonal seagrass extent monitoring in the Sound of Barra. Search workflows were easy to follow and rapid to execute. Refining search constraints with useful filters such as temporal range, spatial extent, collection type and cloud cover returned a valuable selection of imagery specific to the use case. Finally, access to both commercial and open data in one place allowed for a useful comparison of fitness-for-purpose, and facilitated easy piloting of a potential project, study, or analysis.</p>"},{"location":"case-studies/seagrass-naturescot/#about-us","title":"About Us","text":"<p>Read more about environmental management and protection of Scottish natural landscapes at NatureScot.</p>"},{"location":"data/quality-assurance/","title":"Quality Assurance","text":"<p>Supporting the ambition to \u201cmake the UK a global centre for trusted EO data\u201d, a unique aspect of the EODH will be its attention to data quality and quality assurance (QA). It will provide easily understood data quality information built into user interactions via simple query options, aiming to raise awareness of data quality and foster data literacy for non-experts. This will lead to the provision of trustable insights from EO data that are fit-for-purpose. Led by NPL, the EODH QA Service aims to give users confidence in the quality of both:</p> <ul> <li>Data Products in the EODH Catalogue, through independent review of the processes by which Data Products are created, underpinned by quantitative validation.</li> <li>Applications that build on the EODH platform, by review through a similar process of underpinning evidencing relevant quality assurance information.</li> </ul>"},{"location":"data/quality-assurance/#data-products","title":"Data Products","text":"<p>The EODH Data Product QA Process is composed of two core components. Results from both components are published in the EODH Catalogue for user interaction.</p> <p>Quality Processes Review. Verifies that data products are developed according to best practices by reviewing provider documentation against established standards.</p> <p>Validation. Quantitatively validates data product performance through automated peer-reviewed workflows, comparing observed performance against provider claims for defined metrics.</p> <p>During the pathfinder phase of the programme the EODH QA Service piloted these QA processes for a set of core Data Products. The Data Products reviewed include: Sentinel-2 MSI L1C, Planet SuperDove constellation L3B, Airbus Pl\u00e9iades MS-ORTHO.</p>"},{"location":"data/quality-assurance/#applications","title":"Applications","text":"<p>Applications powered by the EODH must meet the following standards to be 'quality assured'. This framework ensures developers demonstrate the quality of their products to users, guaranteeing trusted data and outputs, and building confidence that results meet the necessary standards for their intended applications.</p> <p>Self-evaluation. Review and collate evidencing documentation of processing methodologies and generated data for user transparency.</p> <p>Evaluate Application input data. Using existing data product quality checks, defined by the QA Service, on the datasets being accessed from the EODH.</p> <p>QA Service Review. Finalise QA reporting and publish application on the EODH website.</p> <p>During the pathfinder phase of the programme the EODH QA Service piloted these QA processes for a set of core Applications built on top of the Hub. Find out more about how EO Pro and the Climate Asset Risk Analysis Tool (CLARAT) are quality-assured.</p>"},{"location":"data/quality-assurance/#calibration-dashboard-application","title":"Calibration Dashboard Application","text":"<p>Building on the capability developed to validate optical product absolute radiometric calibration uncertainty, a beta version of a Calibration Dashboard Application for CEOS was developed to allow users to explore in more detail the validation results. Once released, the link to the dashboard will be accessible here.</p> <p>View the dashboard application</p>"},{"location":"data/commercial/applications/","title":"Applications","text":"<p>Note</p> <p>Commercial datasets available on the Hub enhance the potential to combine high-resolution and high-revisit imagery, for advanced spatial analysis that can build a more holistic analysis and inform decision-making.</p> <p>The user case studies presented in this section showcase how others have leveraged the power of satellite data acquisitions provided by Airbus and Planet to create novel insights and carry out reliable asset monitoring. The commercial datasets available are suitable for the purposes of both general users and EO specialists, where basic users can utilise imagery for mapping and GIS analysis, image classification, change detection and AI computer vision applications, while advanced users can carry out empirical and physical modelling, data calibration, atmospheric correction and 3D modelling or photogrammetry applications.</p>"},{"location":"data/commercial/applications/#planet","title":"Planet","text":"<p>Explore how Planet imagery insights drive sustainable agriculture practises, flood management, conservation efforts, and urban planning.</p>"},{"location":"data/commercial/applications/#skysat","title":"SkySat","text":"<p>SkySat can support many application areas including:</p> <ul> <li>Monitoring Threats to Wildlife and Ecosystems</li> </ul>"},{"location":"data/commercial/applications/#planetscope","title":"PlanetScope","text":"<p>PlanetScope can support many application areas including:</p> <ul> <li>Smart Cities - Wroc\u0142aw, Poland: How Satellite Data Supports the City Budget</li> <li>Estimating Volume Changes for Floodplain Harvesting in New South Wales, Australia</li> <li>Regenerative Agriculture Science With Planet Data</li> </ul>"},{"location":"data/commercial/applications/#synthesising-planetscope-and-skysat","title":"Synthesising PlanetScope and SkySat","text":"<p>PlanetScope and SkySat imagery can be sythesised to provide insights for application areas in:</p> <ul> <li>Planet Imagery in Sustainability Scientific Research</li> <li>Tackling Deforestation in Cambodia</li> </ul>"},{"location":"data/commercial/applications/#airbus","title":"Airbus","text":"<p>See how high-resolution Airbus data can be utilised for potential applications across maritime surveillance, agriculture and vegetation monitoring.</p>"},{"location":"data/commercial/applications/#pleiades-neo","title":"Pl\u00e9iades Neo","text":"<p>Pl\u00e9iades Neo can support many application areas including:</p> <ul> <li>Defence. Discover a new dimension for achieving geospatial intelligence analysis or critical mission planning.</li> <li>Urban, Land Administration and Mapping. Optimum reactivity and accuracy to address today's land administration and city management challenges</li> <li>Civil Engineering and Infrastucture. Reliable and precise data supply to support project planning and monitoring of assets</li> <li>Oil, Gas, Mining and Energy. Precise and fresh data to optimise project planning and asset exploration, production, and monitoring</li> <li>Maritime. Providing reliable data for enhanced maritime surveillance, ensuring safer operations and facilitating swift decision-making</li> <li>Agriculture. Offering accurate and precise data to address growing agricultural challenges and meet sustainability demands</li> </ul>"},{"location":"data/commercial/applications/#pleiades-neo-land-administration-and-mapping","title":"Pl\u00e9iades Neo, Land Administration and Mapping","text":"<p>Mapping very dense urban areas in Mayotte with Pl\u00e9iades Neo HD15 imagery</p> <p>As a result of incorporating recent HD15 imagery into their mapping updates, the local authority has been able to rehabilitate substandard housing and relocate households from high-risk areas.</p>"},{"location":"data/commercial/applications/#pleiades-neo-agriculture","title":"Pl\u00e9iades Neo, Agriculture","text":"<p>Detection of Vine Rows using Pl\u00e9iades Neo Imagery</p> <p>Understanding the inter and intra-plot heterogeneity of vineyards is a crucial issue in ensuring the stability of wine production and income.</p>"},{"location":"data/commercial/applications/#pleiades-neo-land-administration-and-mapping_1","title":"Pl\u00e9iades Neo, Land Administration and Mapping","text":"<p>Precision Mapping of Urban Vegetation using Pl\u00e9iades Neo Imagery</p> <p>In 2022, Airbus launched the Pl\u00e9iades Neo Challenge, a call for projects offering free access to Pl\u00e9iades Neo imagery.</p>"},{"location":"data/commercial/applications/#pleiades","title":"Pl\u00e9iades","text":"<p>Pl\u00e9iades can support many application areas including:</p> <ul> <li>Defence. Available satellite resource with a true daily revisit capability, enabling routine or intensive monitoring of regional conflicts, border areas, and early detection and prevention of armed conflicts and humanitarian crises. The satellite\u2019s advanced agility is particularly well-suited for updating military maps in strategic areas.</li> <li>Land administration. Particularly adapted to urban planning, housing and transportation management, land use mapping (at scales of up to 1:10,000 and 1:5,000), assessing urban sprawl, and monitoring new infrastructure.</li> <li>Agriculture. Especially well-suited for monitoring fragmented landscapes, small parcels, experimental micro-plots, and groves/permanent crops, but also for crop yields estimation, irrigation recommendations, agricultural inputs optimization.</li> <li>Forest and environment. Valuable tool for running forest inventories and monitoring changes, forest operations (Wood planting, harvesting\u2026).</li> </ul>"},{"location":"data/commercial/applications/#pleiades-defence-maritime-mobility-and-transportation","title":"Pl\u00e9iades, Defence, Maritime, Mobility and Transportation","text":"<p>Hijacked Tanker Aris 13 Located with the Help of Pl\u00e9iades</p> <p>By combining the Automatic Identification System (AIS) and the Pl\u00e9iades satellites, Airbus helped locating the ARIS 13 oil tanker attacked by Somali pirates in only 2 days.</p>"},{"location":"data/commercial/applications/#pleiades-land-administration-and-mapping","title":"Pl\u00e9iades, Land Administration and Mapping","text":"<p>Supporting Polio Eradication with help of Airbus Pl\u00e9iades satellite data</p> <p>Within the record timeframe of 30 days, an immense area of 50,000km\u00b2 was acquired with Pl\u00e9iades in Nigeria.</p>"},{"location":"data/commercial/applications/#pleiades-agriculture","title":"Pl\u00e9iades, Agriculture","text":"<p>Simplot develops a scalable system for valuable agronomy</p> <p>Simplot and Airbus jointly developed a scalable, automated system to collect, process, analyse and digitally display satellite imagery from Pl\u00e9iades and SPOT 6/7.</p>"},{"location":"data/commercial/applications/#spot","title":"SPOT","text":"<p>SPOT can support many application areas including:</p> <ul> <li>Forestry and environment. Detect anomalies in forest texture, identify degradation from unauthorised logging, or measure green corridors and hedges with higher precision</li> <li>Maritime surveillance. Conduct ship reconnaissance, covering 100,000 km2 in a single pass, with the flexibility for reactive tasking up to 2 hours before acquisition as well as ultra-fast deliveries</li> <li>Agriculture. Access to information inside small parcels &lt;0.05ha, from panchromatic channels to extract textural information and derive biophysical parameters, get an accurate characterisation of crop conditions</li> <li>National mapping. Rapid and broad coverage capability minimising seasonal effects on vegetation, and reducing operational costs by requiring less processing work over large areas; feature extraction with up to 1.2m CE90 geometric accuracy, as well as contour lines extraction and stereo plotting</li> </ul>"},{"location":"data/commercial/applications/#spot-agriculture","title":"SPOT, Agriculture","text":"<p>Simplot develops a scalable system for valuable agronomy</p> <p>Simplot and Airbus jointly developed a scalable, automated system to collect, process, analyse and digitally display satellite imagery from Pl\u00e9iades and SPOT 6/7.</p>"},{"location":"data/commercial/applications/#spot-security","title":"SPOT, Security","text":"<p>2018 Hokkaido Earthquake and Post-disaster Monitoring</p> <p>Japan is constantly under threats from natural disasters. To protect people\u2019s lives and minimise economic losses, the authorities need immediate information from the affected areas.</p>"},{"location":"data/commercial/applications/#spot-maritime-forestry-and-environment-mobility-and-transportation","title":"SPOT, Maritime, Forestry and Environment, Mobility and Transportation","text":"<p>SPOT enables the detection and monitoring of toxic Gulfweed</p> <p>The gorgeous island of Martinique in the Eastern Caribbean Sea has been hit by a plague, called sargasse.</p>"},{"location":"data/commercial/applications/#radar-constellation","title":"Radar Constellation","text":"<p>Radar Constellation can support many application areas including:</p> <ul> <li>Geospatial/image Intelligence (GEOINT/IMINT) and analytics, e.g. for Defence and Security</li> <li>Emergency response and Mapping as well as change detection, e.g. for crisis management</li> <li>Interferometric (InSAR) analysis of ground surface motion in support of OGME and Civil Engineering companies</li> <li>Maritime surveillance with large area coverage at unique resolution and near-real-time delivery</li> </ul>"},{"location":"data/commercial/applications/#radar-constellation-civil-engineering-and-infrastructure","title":"Radar Constellation, Civil Engineering and Infrastructure","text":"<p>Interferometric (InSAR) Analysis with TerraSAR-X.</p>"},{"location":"data/commercial/licensing-restrictions/","title":"License restrictions","text":""},{"location":"data/commercial/licensing-restrictions/#airbus","title":"Airbus","text":"<p>Airbus imagery is available against a comprehensive set of licenses covering a range of different use cases and industrial organisations. Airbus\u2019 standard licence set are published on our website. See below for a one-page simplified licence matrix demonstrates the license features at a glance.</p> <p>INSERT IMAGE</p> <p>Question</p> <p>Want to you know more about our licences?   1) Download the Airbus guide!  2) Discover the current Airbus licence set  3) Understand the main elements of Airbus licences  4) Become a licence expert</p>"},{"location":"data/commercial/licensing-restrictions/#planet","title":"Planet","text":"<p>Planet offers standard licenses to suit different use cases e.g., Commercial, R&amp;D, Education, Publication. More detailed definitions for the Planet license agreements can be found on the Planet website.</p> <p>For new Planet customers to arrange for a Planet license to access Planet data via EODH, please contact eodatahub@planet.com. Existing Planet accounts can be accessed via the EODH by using their API key. Please use these instructions if you need help locating your API key.</p>"},{"location":"data/commercial/link-accounts/","title":"Linking Planet and Airbus accounts to the EODH","text":"<p>Info</p> <p>Users are able to associate their commercial data account tokens with their EODH account in the workspaces page as long as those providers are supported by the Hub. Airbus and Planet are currently supported. The process for each supported data provider, respectively, is outlined below.</p>"},{"location":"data/commercial/link-accounts/#airbus","title":"Airbus","text":"<p>To access Airbus content on the EODH, your OneAtlas account must be configured with a contract that will send ordered data to the EODH.</p> <ul> <li>Request a OneAtlas account via the website if you are not already a customer. New customers are first onboarded by the Airbus customer service team.</li> <li>Contact the Airbus customer service team at ukintelligence-imagerysupport@airbus.com stating that you would like access to Airbus Optical, and/or Radar data via the EODH.</li> <li>The Airbus team will create new contracts to operate on the EODH.</li> <li>Once this is complete, please self-serve an Airbus API Key via the OneAtlas Developer Portal \u2018Get your API Key\u2019.</li> </ul> <p>The Airbus API key then be registered to a workspace in the EODH platform under 'Linked accounts' for a selected workspace on the workspaces page. If you have multiple contracts associated with the key then you must choose which ones to assign to the workspace. By default these will be auto assigned based on the input API key. The linked API key and contracts will then be used for orders placed by members of the workspace, and data will be delivered directly to the workspace. See here for more details.</p>"},{"location":"data/commercial/link-accounts/#planet","title":"Planet","text":"<p>Existing Planet accounts can be accessed via the EODH by using their API key. Please use these instructions if you need help locating your API key. To create a Planet account, please email the Planet team. Contact eodatahub@planet.com for dedicated support with Planet access.</p> <p>The Planet API key can then be registered to a workspace in the EODH platform under \u2019Linked accounts\u2019 for a selected workspace on the \u2019Workspaces\u2019 page. The linked API key will then be used for orders placed by members of the workspace, and data will be delivered directly to the workspace. See here for more details.</p>"},{"location":"data/commercial/ordering-commercial-data/","title":"Ordering commercial data","text":""},{"location":"data/commercial/ordering-commercial-data/#find-the-data-you-need","title":"Find the data you need","text":""},{"location":"data/commercial/ordering-commercial-data/#search-and-browse","title":"Search and browse","text":"<p>Searching for the imagery you are looking to purchase is currently possible within our Resource Catalogue interface or directly through our APIs.</p> <p>This guide walks you through using the Resource Catalogue to find the data you need. You can filter on commercial collections within the Resource Catalogue to discover which collections are available from the currently supported commercial providers. Alternatively, we have an example notebook script to help you get started searching for data programmatically, using pyeodh, available within our training materials.</p>"},{"location":"data/commercial/ordering-commercial-data/#using-the-resource-catalogue","title":"Using the Resource Catalogue","text":"<p>Once you have opened a commercial data collection, using the left-hand panel, search for the data you are looking to find by using the available filters. An area of interest should be drawn on the map, and a temporal extent for the data search can be selected via calendar input. The user can also refine the parameters based on max cloud cover and look angle using the slider widgets. The search is carried out automatically once you have applied the chosen filters by selecting the blue Apply button.</p> <p>All imagery from the Resource Catalogue matching the search parameters is then displayed in a pop out panel adjacent to the left hand panel. The user can scroll through the list to search the available data. Acquisition date and time are displayed per image, as well as the cloud coverage (%), and a thumbnail snapshot of the imagery.</p>"},{"location":"data/commercial/ordering-commercial-data/#checking-thumbnails","title":"Checking thumbnails","text":"<p>A thumbnail or quicklook is a low-resolution preview version of the commercial image, typically in JPEG or PNG format, that provides a visual summary of the full-resolution satellite data. It can be viewed prior to placing an order. It allows the user to visually check the content of the image for cloud, quality, or coverage of a specific asset, before purchasing. This allows the user to make informed purchasing decisions.</p> <p>INSERT IMAGE</p> <p>Figure: Low-resolution quicklook thumbnails for Cornwall, UK (left) and Caringorms, Scotland (right)</p> <p>Note</p> <p>Some data thumbnails for commercial imagery provided by Airbus require the user to be logged in to their EODH account.</p> <p>Tip</p> <p>To download a quicklook thumbnail, open the data item by clicking the 'i' icon, and got to 'Assets'. If you open the available dropdown menus, you should be given the option to download. From there, you can view the low-resolution thumbnail as you would a normal image file. INSERT IMAGE</p>"},{"location":"data/commercial/ordering-commercial-data/#ordering-interface","title":"Ordering interface","text":"<p>From here, the user can purchase an item from the catalogue by selecting 'Purchase' on the item card on the left-hand panel.</p> <p>INSERT IMAGE</p> <p>This will take you through to the ordering interface, which should look something like the image below. The available ordering fields will vary depending on the data type and the data provider. For example, a SAR data order requests additional, more specific field options. The 'Purchase full scene' checkbox will only be visible if you have drawn an AOI polygon i.e. allowing you to clip your order to the AOI which was drawn.</p> <p>INSERT IMAGE</p> <p>The next page in the guide will outline each order field. For help understanding the specific requirements of the order needed for your application, it is best to contact support for the data provider directly. Alternatively, reach out to enquiries@eodatahub.org.uk who will be able to put you in touch with the data specialists.</p>"},{"location":"data/commercial/ordering-commercial-data/#understanding-the-ordering-options","title":"Understanding the ordering options","text":""},{"location":"data/commercial/ordering-commercial-data/#getting-started","title":"Getting started","text":"<p>The following guidance walks through the requirements for each field in the ordering interface, step by step. Before attempting to order any commercial data, you must first link your account using an API key which is self-served from the commercial provider. This allows you to fetch a quote for the image you are looking to purchase. If your workspace is not linked with an API key, an error message will flag at the point of purchase, preventing you from fetching a quote.</p>"},{"location":"data/commercial/ordering-commercial-data/#filling-out-the-order-options","title":"Filling out the order options","text":""},{"location":"data/commercial/ordering-commercial-data/#workspace","title":"[Workspace]","text":"<p>Select a workspace for the private data delivery. You can select any workspace from those that you have been added to or created yourself. The data you purchase will be shared with, and accessible to, all members who are in the selected workspace. Make sure the workspace you select is linked via a valid API key, so that the commercial data quote can be fetched. If you still have issues fetching a quote despite your account being linked, it could be that your API key has expired and you need to refresh it.</p>"},{"location":"data/commercial/ordering-commercial-data/#product-bundle","title":"[Product bundle]","text":"<p>The product bundle is selected from a dropdown menu of 4 available options, outlined below. This field indicates the level of preprocessing required from your imagery order. For example, Basic is the most raw version of the image with the least preprocessing, generally more useful for scientists, while Visual has the most preprocessing stages, enhanced to support easy map visualisations for novice users.</p> <p>INSERT IMAGE</p> <p>Note</p> VisualGeneral UseAnalyticBasic <p>A simple map accurate image analogous to an aerial photo. Immediately recognisable, with no specialist EO data expertise. Natural colour, 8-bit RGB image - should be opened by any system - just like a consumer digital camera image, only georeferenced. Orthorectified, pansharpened, natural colour. Suitable for visual mapping applications, image backing for GIS applications, AI computer vision analysis - training and deployment. Suitable for all users.</p> <p>Orthorectified, pansharpened (where applicable) multi-spectral imagery to support general mapping and analytic applications. Calibrated to reflectance. Ready for immediate use - no calibration or data fusion required. Suitable for general image analysis and classification, land cover and land use analysis, and visual mapping applications.</p> <p>Orthorectified, multispectral imagery to support scientific applications. Calibrated to reflectance. Supplied as a bundle (where applicable) to maintain the radiometric integrity of each band. Orthorectified, reflectance. Suitable for spectroscopy and physical modelling, empirical modelling, precision agriculture and biophysical modelling, and image classification. More aligned to EO specialists.</p> <p>A multi-spectral image close to the natural image aquired by the sensor, aimed to give the user close to full automomy over the data processing chain. Imagery in sensor geometry and corrected for sensor distortions, and co-registration of spectral bands (multispectral and panchromatic). Contains RPCs and sensor model. Imagery is calibrated to remove sensor affects (such as CCD array equalisation), but has no further radiometric processing and can be considered 'Raw'. Not orthorectified or radiometrically corrected. Suitable for precision ortho-rectification, photogrammetry, data calibration and atmospheric correction, and 3D modelling. For EO and photogrammetry specialists.</p>"},{"location":"data/commercial/ordering-commercial-data/#end-user-country","title":"[End user country]","text":"<p>The user must input their country from the dropdown menu.</p>"},{"location":"data/commercial/ordering-commercial-data/#license","title":"[License]","text":"<p>Select the license from a dropdown menu of the following options:</p> <ul> <li>Single Use - An individual user</li> <li>Multi Use - An organisational license for a team of users</li> </ul>"},{"location":"data/commercial/ordering-commercial-data/#clip-the-ordered-image-to-your-aoi","title":"Clip the ordered image to your AOI","text":"<p>If you have drawn an area of interest, you have the option to clip the delivered image to the drawn polygon area. This will happen automatically if the checkbox remains unticked.</p> <p>If the box is checked, you are confirming you want to order the full scene, unclipped. You can check this has been actioned by confirming that the quote fetched has gone up in cost (assuming you will now be purchasing a larger area of imagery).</p> <p>INSERT IMAGE</p>"},{"location":"data/commercial/ordering-commercial-data/#placing-an-order","title":"Placing an order","text":"<p>Once you are happy that all of the above fields are populated as per your purchase request, ensuring there are no errors in the inputs, proceed to make the purchase by selecting the blue Place Order button. Before you carry out the purchase, review all of the fields displayed and check you are happy with all elements of the order, including the metadata, acquisition date, and image ID, as a purchase cannot be reversed once it has been made. If you have any queries before carrying out the order, don\u2019t hesitate to get in contact with enquiries@eodatahub.org.uk where the team will be happy to assist you.</p>"},{"location":"data/commercial/ordering-commercial-data/#order-confirmation","title":"Order confirmation","text":"<p>If the purchase is successful, the Order Confirmation pop out should appear. The status of the commercial data request, as well as access to your purchased imagery, can now be viewed in your EODH user workspace by going to the 'My Data' section within the catalogue. Reach out to enquiries@eodatahub.org.uk if you experience issues with the delivery status of your order.</p>"},{"location":"data/commercial/ordering-commercial-data/#ordering-data-programmatically","title":"Ordering data programmatically","text":"<p>An example Commercial Data Ordering Notebook is available to guide you through the process of placing an order programmatically. This is the best method to place a bulk order of multiple images in one go. More guidance can be found within the notebook itself, which you can run by starting a Jupyter notebook server instance on the Hub, and uploading the downloaded .ipynb file.</p>"},{"location":"data/commercial/ordering-faqs/","title":"Ordering faqs","text":"<p>Commercial data ordering FAQs</p> <p>What is the minimum area AOI for data orders? Airbus For Airbus, Optical PPO contracts have a minimum archive order size of 25km2. Radar PPO contracts have a minimum order size of one full scene, meaning Radar imagery orders cannot be clipped to an AOI. There are options available for very granual data access - please contact andrew.tewkesbury@airbus.com to discuss.</p> <p>Planet For Planet, we would recommend people with specific needs contact Planet directly. Planet do have a minimum order size, but that could for example refer to a data distributed over a long period. And for that case only SkySat would work.</p> <p>Can I accidentally purchase an image? Before a transaction can carried out to purchase a commercial image, the user is first met with the following checkboxes (see pictured). These are in place to ensure the user understands the licensing agreement they are accepting by making the purchase. The user must agree that they have:</p> <p>Read and accepted the product license Read and accepted the vendor terms and conditions Give their consent to the EODH to place the order on behalf of the user</p> <p>Please ensure you have read and consented to all the agreements before placing the purchase, the relevant documents for which are linked under Read more. If you require more information regarding the license agreements, these are published by the data providers, Planet and Airbus, respectively. Alternatively, check back to our documentation section on licensing restrictions.</p> <p>Cost of the imagery selection is clearly displayed at the bottom right of the checkout page in green. It is important to also ensure you are happy with the total cost before proceeding with the purchase.</p> <p>Figure: Confirmation checkboxes at checkout screen, with image cost displayed at the bottom right of the panel</p> <p>In the case where a user makes an accidental order by proceeding through the checkout screen to point of purchase, please note that the order cannot be changed after delivery. As such, the user should ensure thorough checks are carried out before purchasing to ensure the correct product, resolution, date, and image provider have been selected as intended.</p> <p>Where can I find my imagery once purchased? Navigate to your personal workspace catalogue, found in the Catalogue tab of the EODH website, where any purchased imagery will appear. This can be associated with your individual or organisational workspace account. Purchased imagery specifically will be found within the commercial-data catalogue, within the workspace catalogue. Note that item assets may take a while to appear after purchasing. Check the item in your workspace for the status of the order.</p> <p>Do the commercial datasets have global coverage, or UK only? The commercial data spatial coverage is not limited in availability to the UK only. There is data available for areas of interest worldwide. Open a data collection in the map search window of the Resource Catalogue to explore coverage for a specific area of interest. You can do this by drawing a polygon on the map to constrain your search, whereby the item list will only return imagery with footprints overlapping the drawn polygon. Find out more about how to search data collections for spatial coverage of a specific area of interest here.</p>"},{"location":"data/commercial/what-is-available/","title":"Commercial data catalogue","text":"<p>Note</p> <p>These commercial data are available to selected users as part of the EODH Pathfinder Project</p>"},{"location":"data/commercial/what-is-available/#airbus","title":"Airbus","text":""},{"location":"data/commercial/what-is-available/#supported-collections","title":"Supported Collections","text":"<p>Airbus operates the most comprehensive constellation of optical and radar commercial Earth observation satellites available on the market today. The EODH provides global access to archive data of Airbus optical imagery with 30cm to 1.5m spatial resolution, and radar imagery from 25cm to 40m.</p> Pl\u00e9iades NeoPl\u00e9iadesSPOTRadar Constellation <p>Airbus' most advanced optical satellites with the highest spatial resolution and capacity. Two identical satellites launched in 2021 with seven spectral bands, 30cm native resolution and daily revisit.</p> <p>Very high-resolution optical satellites with a global archive back to 2012. Two identical satellites with five spectral bands, daily revisit delivering 50cm image products.</p> <p>Efficiently image large areas at 1.5m resolution. Revisit every other day and a global archive back to 2012.</p> <p>Reliably acquires the widest range of radar satellite images, from the highest resolution to wide area coverage - regardless of weather and daylight conditions. Imagery in 6 different imaging modes with a flexible spatial resolution (from 25cm to 40m) and area coverage (footprint). Global mean revisit time &lt; 24 hours, and an archive back to 2007.</p>"},{"location":"data/commercial/what-is-available/#pleiades-neo","title":"Pl\u00e9iades Neo","text":"<p>Experience the forefront of Earth observation innovation with Pl\u00e9iades Neo Airbus's leading optical constellation featuring two identical 30cm resolution satellites renowned for their ultimate reactivity. Since its launch in 2021, this innovative constellation has revolutionised the landscape of Earth observation-based services, setting new standards of precision and reliability.</p> <ul> <li>Two identical satellites with 30cm native resolution imagery</li> <li>Location accuracy: 3.5m CE90</li> <li>Reactive tasking, rapid delivery</li> <li>Acquisition capacity of 1 million km\u00b2 per day and daily revisit, anywhere</li> <li>Multispectral imagery (including Deep Blue / Red Edge)</li> <li>Mono, stereo &amp; tri-stereo acquisition capability</li> </ul>"},{"location":"data/commercial/what-is-available/#pleiades","title":"Pl\u00e9iades","text":"<p>The identical Pl\u00e9iades 1A and Pl\u00e9iades 1B satellites deliver 50cm imagery products with a 20km swath. Their location accuracy and image quality are excellent, making them an ideal source of data for both civil or military projects. The space and ground segment have been designed to provide data in record time, ensuring daily revisit capacity anywhere on the globe and unrivalled reliability when it comes to collecting new images.</p> <ul> <li>Very high-resolution multispectral twin satellites (50cm products)</li> <li>Daily revisit capacity and highly reactive tasking</li> <li>Advanced agility including 5 collection scenarios: target, strip mapping, tri-stereo, corridor and persistent monitoring</li> <li>Stereo / tristereo capacity and fresh archive</li> </ul>"},{"location":"data/commercial/what-is-available/#spot","title":"SPOT","text":"<p>The ideal solution for country-wide, demanding applications. Designed to efficiently cover huge areas in record time, making it a perfect choice for cartography and monitoring applications.</p> <ul> <li>1.5m resolution products, ideal for topographic mapping: from 1:30,000 to 1:7,500 scale maps</li> <li>Twin satellites with revisit every other day (SPOT 6 and 71), and a global archive back to 2012.</li> <li>Stereo and tri-stereo capacity</li> <li>2D and 3D stereo plotting, geometric accuracy down to 6 to 10m CE90, elevation and contour lines</li> <li>3 Million km\u00b2 per day, per satellite</li> </ul>"},{"location":"data/commercial/what-is-available/#radar-constellation","title":"Radar Constellation","text":"<p>Radar image acquisition in a unique constellation. The German TerraSAR-X / TanDEM-X satellites are being operated in the same orbit tube and feature identical ground swaths and imaging modes. The satellites carry a high frequency X-band Synthetic Aperture Radar (SAR) sensor to acquire datasets ranging from very high-resolution imagery to wide area coverage.</p> <ul> <li>Radar Constellation 'toolbox': Imagery in 6 different imaging modes with a flexible spatial resolution (from 25cm to 40m) and area coverage (footprint)</li> <li>Weather and daylight independent site access to any point on Earth</li> <li>High agility (rapid switches between different imaging modes and polarisations)</li> <li>TerraSAR-X, and TanDEM-X have an orbit repeat of 11 days for InSAR applications, and a global mean revist time &lt; 24 hrs</li> <li>Unrivalled geometric accuracy and excellent radiometric accuracy</li> </ul>"},{"location":"data/commercial/what-is-available/#planet","title":"Planet","text":""},{"location":"data/commercial/what-is-available/#supported-collections_1","title":"Supported Collections","text":"<p>Planet operates the PlanetScope (PS) and SkySat (SS) Earth-imaging constellations. Imagery is collected and processed in a variety of formats to serve different use cases, be it mapping, deep learning, disaster response, precision agriculture, or simple temporal image analytics to create rich information products.</p> PlanetScope (Download and Tile View)SkySat (Visual) <ul> <li>PlanetScope Visual</li> <li>PlanetScope: analytic_sr_udm2, analytic_8b_sr_udm2 (Surface Reflectance images)</li> <li>PlanetScope: basic_analytic_udm2, basic_analytic_8b_udm2 (TOAR images)</li> <li>Skysat Archive Download and Tile View</li> </ul> <ul> <li>SkySat: pansharpened_udm2</li> <li>SkySat: analytic_sr_udm2 (Surface Reflectance images)</li> <li>SkySat: basic_analytic_udm2 (TOAR images)</li> </ul>"},{"location":"data/commercial/what-is-available/#planetscope","title":"PlanetScope","text":"<p>PlanetScope satellite imagery is captured as a continuous strip of single frame images known as \u201cscenes.\u201d Scenes are derived from multiple generations of PlanetScope satellites capturing images with a single RGB (red, green, blue) frame or a split-frame with a RGB half and a NIR (near-infrared) half, with a multistripe frame with bands divided between RGBNIR or RGBNIR, red edge, green I, yellow and coastal blue. Planet offers two product lines for PlanetScope imagery: a Basic Scene product and Ortho Scene product. The Basic Scene product is a scaled Top of Atmosphere Radiance (at sensor) and sensor-corrected product. The Basic Scene product is designed for users with advanced image processing and geometric correction capabilities. The product is not orthorectified or corrected for terrain distortions. Ortho Scenes represent the single-frame image captures as acquired by a PlanetScope satellite with additional post processing applied.</p>"},{"location":"data/commercial/what-is-available/#skysat","title":"SkySat","text":"<p>SkySat imagery is captured similarly to PlanetScope in a continuous strip of single frame images known as \u201cscenes,\u201d which are all acquired in the blue, green, red, nir-infrared, and panchromatic bands. SkySat data is available on the Hub as the SkySat Collect product.</p>"},{"location":"data/open-access/climate-projections/","title":"Climate Projections","text":"<p>Climate Projections include simulated data of the climate from different models with various conditions and radiative forcings. These climate model projections show how climate is likely to change in the future, based on given scenarios. Temporal extent of these collections runs into the future, starting from present day.</p> <p>To construct a climate model, scientists create a three-dimensional grid covering the Earth's surface and atmosphere, and simulate interactions over time between grid cells. Simulations are run under different Representative Concentration Pathways (RCPs) or Shared Socioeconomic Pathways (SSPs), which represent varying assumptions about future emissions, land use, and policy decisions. This data is then spatially gridded as a modelled image product.</p> <p>INSERT IMAGE</p> <p>Explore Climate Projections</p>"},{"location":"data/open-access/climate-variables/","title":"Climate Variables","text":""},{"location":"data/open-access/climate-variables/#introduction","title":"Introduction","text":"<p>Satellite-Derived climate datasets are collections of measurements and observations obtained from Earth-observing satellites that help monitor and understand the Earth\u2019s climate system. These datasets provide long-term, consistent, and global coverage, which is essential for climate research and policy-making. These include datasets from the EOCIS project and shortly the ESA CCI project, measuring variables such as land surface temperature, sea surface temperature, aerosol and artic sea ice thickness. Observations derived from orbiting missions cover a historic temporal extent, spanning up to the present day.</p> <p>INSERT IMAGE</p>"},{"location":"data/open-access/climate-variables/#earth-observation-climate-information-service-eocis","title":"Earth Observation Climate Information Service (EOCIS)","text":"<p>The UK Earth Observation Climate Information Service (EOCIS) offers diverse datasets for climate and environmental monitoring. The project addresses 12 categories of global and regional essential climate variables, covering a broad scope of applications.</p> <p>The global datasets generated by EOCIS include atmospheric, land surface, and ocean essential climate variables. Regional datasets focus on high-latitude observables, methane observations in the lower atmosphere, and soil water balance observations across Africa. These datasets are selected for their scientific significance and societal relevance, such as food security in a changing climate. Climate variables and indices for the UK are produced at high resolution (~100m), including near-real-time fire and urban flood monitoring, high-resolution surface temperature mapping, analysis of coastal water colour changes due to extreme rainfall, and environmental assessments of vegetation, lake changes, and aerosols for ecosystem and human health impact studies.</p> <p>The project exploits the observations available from environmental sensors orbiting in space to create climate data records and climate information. EOCIS is a collaboration led by the National Centre for Earth Observation, and involving over a dozen research organisations.</p> <p>For more information on the project and datasets, please visit the EOCIS website, or discover EOCIS within the data catalogue.</p> <p>INSERT IMAGE</p>"},{"location":"data/open-access/climate-variables/#esa-climate-change-initiative-cci","title":"ESA Climate Change Initiative (CCI)","text":"<p>The European Space Agency (ESA)'s Climate Change Initiative (CCI) programme kicked off in 2010. The programme goal is to provide stable, long-term, satellite-based Essential Climate Variable (ECV) data products for climate modellers and researchers. The ECVs will be derived from multiple satellite data sets (not just ESA but all sources via international collaboration) and include specific information on the errors and uncertainties of the data set. Comprehensive information will also be provided on calibration and validation, long term algorithm maintenance, data curation and reprocessing.</p> <p>The Climate Change Initiative brings together European expertise covering the full range of scientific, technical and development specialisations available within the European Earth Observation community, and will establish lasting and transparent access for global climate scientific and operational communities to its results.</p> <p>INSERT IMAGE</p> <p>Explore Climate Data</p>"},{"location":"data/open-access/land-cover/","title":"Land Cover","text":""},{"location":"data/open-access/land-cover/#introduction","title":"Introduction","text":"<p>Land Cover satellite data products are datasets derived from satellite imagery that classify the Earth's surface into different categories such as forests, water bodies, urban areas, croplands, and barren land. These products are essential for environmental monitoring, resource management, and climate studies.</p> <p>INSERT IMAGE</p>"},{"location":"data/open-access/land-cover/#classification-methods","title":"Classification methods","text":"<p>The classification of land cover is commonly produced using supervised or unsupervised machine learning algorithms applied to multispectral or hyperspectral satellite imagery.</p> <p>In supervised classification, analysts train models using ground truth data collected in the field to teach the algorithm how to recognize patterns in spectral signatures. Popular methods include decision trees, support vector machines, and increasingly, deep learning approaches such as convolutional neural networks (CNNs). Unsupervised classification, on the other hand, groups pixels based on statistical similarities without prior labeling, often using clustering techniques like k-means. Preprocessing steps such as atmospheric correction, cloud masking, and image normalization are critical to ensure accurate classification results.</p> <p>Explore Land Cover</p>"},{"location":"data/open-access/optical/","title":"Optical","text":""},{"location":"data/open-access/optical/#multispectral","title":"Multispectral","text":"<p>Multispectral satellite imagery refers to images captured by sensors that record data in multiple distinct spectral bands across the electromagnetic spectrum. These bands typically include visible light (red, green, blue) and extend into near-infrared (NIR) and sometimes shortwave infrared (SWIR) regions.</p> <p>INSERT IMAGE</p>"},{"location":"data/open-access/optical/#capabilities","title":"Capabilities","text":"<ul> <li> <p>Multiple Bands. Usually 4\u201313 bands, each sensitive to a specific wavelength range.</p> </li> <li> <p>Beyond Human Vision. Includes wavelengths invisible to the naked eye, enabling detection of features like vegetation health or soil moisture.</p> </li> <li> <p>Spatial Resolution. Varies by satellite (e.g., Landsat ~30m, Sentinel-2 ~10m).</p> </li> </ul> <p>Explore Multispectral</p>"},{"location":"data/open-access/radar/","title":"Radar","text":"<p>Radar satellite data refers to information collected by satellites equipped with Synthetic Aperture Radar (SAR) sensors. Unlike optical sensors, radar systems use microwave signals to actively illuminate the Earth's surface and measure the reflected signals.</p> <p>INSERT IMAGE</p>"},{"location":"data/open-access/radar/#capabilities","title":"Capabilities","text":"<p>This gives radar satellites unique capabilities:</p> <ul> <li>All-weather capability: Radar penetrates clouds, fog, and rain, making it useful in any weather.</li> <li>Day and night imaging: Since radar uses its own energy source, it doesn\u2019t rely on sunlight.</li> <li>Surface penetration: Certain radar wavelengths can penetrate vegetation, snow, and even soil to some depth.</li> <li>High spatial resolution: SAR can produce detailed images of terrain and structures.</li> </ul> <p>Explore Radar</p>"},{"location":"documentation/APIs/api-getting-started/","title":"APIs: Getting started","text":""},{"location":"documentation/APIs/api-getting-started/#hub-apis","title":"Hub APIs","text":"<p>A suite of user-facing APIs make up the core of the Hub\u2019s architecture. You can use these APIs to perform Hub actions programmatically. This includes:</p> <ul> <li>Browsing and downloading items from our catalogue</li> <li>Visualising data using TiTiler</li> <li>Uploading and running workflows in our architecture</li> <li>Purchasing commercial data</li> </ul> <p>These APIs are documented in a machine-readable definition language called OpenAPI (formerly Swagger), which allows developers to understand how the APIs work and generate client code and tests automatically.</p> <p>You can explore the Hub\u2019s APIs using the interactive documentation viewer we host here, or if you prefer you can just download the raw OpenApi documentation here. This documentation currently covers the Resource Catalogue, TiTiler and Workflow Runner, but is expected to cover APIs for the whole Hub in future.</p>"},{"location":"documentation/APIs/api-getting-started/#connecting","title":"Connecting","text":"<p>Connect to your EODH workspace from external interfaces via API tokens To obtain an API token, visit the Workspaces tab. Choose the desired workspace and then select Credentials on the left hand panel. Under DataHub API Key click the Request New Token button. Enter a name and expiry time.</p> <p>Warning</p> <p>The maximum duration of an API token is 30 days.</p> <p></p> <p>Warning</p> <p>Copy the Token ID and API Key now. These will not be available to view later.</p> <p>These credentials will be required when performing operations that require authorisation to your workspace.</p> <p></p>"},{"location":"documentation/APIs/api-getting-started/#explore-the-public-catalogue","title":"Explore the public catalogue","text":"<p>Our APIs can be used without authentication to perform actions such as browsing our publicly available data, an example of which is shown below. This uses pystac to perform a basic search for collections with data over the UK.</p> <p>More detailed examples for browsing our catalogue can be found here.</p> <pre><code>from pystac_client import Client\n\n# Set resource catalogue top-level url\nrc_url = f\"https://eodatahub.org.uk/api/catalogue/stac\"\n\n# Create STAC client\nstac_client = Client.open(rc_url)\n\n# Do some collection searching using bbox\nuk_bbox = [10.854492,49.823809,100.762709,60.860699]\ncollection_search = stac_client.collection_search(\n    bbox=uk_bbox,\n)\nprint(f\"{collection_search.matched()} collections found\")\nfor collection in collection_search.collections():\n    print(collection.id)\n</code></pre>"},{"location":"documentation/APIs/api-getting-started/#explore-your-private-catalogue","title":"Explore your private catalogue","text":"<p>Sections of the catalogue are private to your workspace and cannot be accessed without proper authorisation to the workspace. Authorisation for accessing this data is performed by using your API token obtained through the Hub UI as a Bearer token. An example of this is shown below.</p> <p>Authorisation such as this is required for actions such as accessing private workspace data, running workflows, and purchasing commercial data.</p> <pre><code>from pystac_client import Client\n\n# Set resource catalogue top-level url\nrc_url = f\"https://{platform_domain}/api/catalogue/stac/catalogs/user\"\n\n# Create STAC client with authentication\ntoken = \"\"  # API token should be stored and accessed securely\nstac_client = Client.open(rc_url, headers={\"Authorization\": f\"Bearer {token}\"})\n\n# See that all the available children in this catalogue includes your workspace catalogue\nchildren = stac_client.get_children()\nfor child in children:\n    print(child.id)\n</code></pre>"},{"location":"documentation/APIs/example-tutorials/","title":"Example API tutorials","text":"<p>Here are some examples highlighting the possibilities of working with Hub APIs are detailed here. These are designed to aid you in starting to use Hub functionality programmatically. You can explore the full list of the Hub\u2019s APIs using the interactive documentation viewer hosted here.</p>"},{"location":"documentation/APIs/example-tutorials/#browsing-the-catalogue","title":"Browsing the catalogue","text":"<p>Public and commercial catalogues are collated in the Hub under one STAC catalogue which can be navigated programmatically. Spatiotemporal and keyword searches are supported, allowing you to navigate through our extensive catalogue to find the data that matters to you. Examples using Python's <code>pystac</code> module follow.</p> <p>All examples first require you to initialise a stac client:</p> <pre><code>from pystac_client import Client\n\n# Set resource catalogue top-level url\nrc_url = f\"https://eodatahub.org.uk/api/catalogue/stac\"\n\n# Create STAC client\nstac_client = Client.open(rc_url)\n</code></pre>"},{"location":"documentation/APIs/example-tutorials/#keyword-search","title":"Keyword search","text":"<p>Where keywords have been set by catalogues harvested into the datahub, keywords can be used to search for both items and the collections containing them.</p> <pre><code>collection_search = stac_client.collection_search(\n    q=\"climate\",\n)\nprint(f\"{collection_search.matched()} collections found\")\nfor collection in collection_search.collections():\n    print(collection.id)\n</code></pre>"},{"location":"documentation/APIs/example-tutorials/#temporal-search","title":"Temporal search","text":"<p>Collections and items can be filtered by a date range, that may be open ended.</p> <pre><code># Looking into the future\ntime_to_search = \"2026-01-06T00:00:00Z/..\"\ncollection_search = stac_client.collection_search(\n    datetime=time_to_search,\n)\nprint(f\"{collection_search.matched()} collections found\")\nfor collection in collection_search.collections():\n    print(collection.id)\n</code></pre>"},{"location":"documentation/APIs/example-tutorials/#spatial-search","title":"Spatial search","text":"<p>You can perform a search for items that intersect a polygon of interest.</p> <pre><code># Geometry for the UK\ngeom = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [\n        [\n            [-4.41191386337143, 50.5437323318846],\n            [-2.86223024423239, 50.5522106222613],\n            [-2.86502949737798, 49.564676297854],\n            [-4.3832372671439, 49.5564878434316],\n            [-4.41191386337143, 50.5437323318846]\n        ]\n    ],\n}\nsearch = stac_client.search(\n    max_items=10,\n    collections=['cmip6'],\n    intersects=geom,\n)\nfor item in search.items():\n    print(item.id)\n</code></pre>"},{"location":"documentation/APIs/example-tutorials/#filter-search","title":"Filter search","text":"<p>Complex queries can be built up and stacked with a filter search.</p> <pre><code>filter = {\n        \"op\": \"and\",\n        \"args\": [\n          {\n            \"op\": \"between\",\n            \"args\": [\n              {\n                \"property\": \"properties.datetime\"\n              },\n              \"2020-01-12T00:00:00.000Z\",\n              \"2025-02-12T23:59:59.999Z\"\n            ]\n          },\n          {\n            \"op\": \"and\",\n            \"args\": [\n              {\n                \"op\": \"=\",\n                \"args\": [\n                  {\n                    \"property\": \"collection\"\n                  },\n                  \"sentinel2_ard\"\n                ]\n              },\n              {\n                \"op\": \"&lt;=\",\n                \"args\": [\n                  {\n                    \"property\": \"properties.eo:cloud_cover\"\n                  },\n                  5\n                ]\n              }\n            ]\n          }\n        ]\n      }\n\nsearch = stac_client.search(\n    max_items=10,\n    filter=filter,\n    fields={\"include\": [\"properties.datetime\", \"properties.eo:cloud_cover\"]},\n)\n\nfor item in search.items():\n    print(item.id)\n    print(f\"Datetime is {item.properties['datetime']}\")\n    print(f\"Cloud cover is {item.properties['eo:cloud_cover']}\")\n</code></pre>"},{"location":"documentation/APIs/example-tutorials/#finding-assets","title":"Finding assets","text":"<p>Once an item of interest has been obtained, follow the asset links to download relevant data. These links may be externally hosted by our trusted data providers.</p> <pre><code>assets = item.get_assets()\n\nfor asset in assets:\n    print(f\"Asset {asset} is available at {assets[asset].href}\")\n</code></pre>"},{"location":"documentation/APIs/example-tutorials/#commercial-data","title":"Commercial data","text":"<p>Commercial data from Airbus and Planet can be ordered directly through our APIs. Before following these instructions, please ensure that your Hub account is linked to an Airbus or Planet account as described here.</p> <p>Firstly, an item of interest from our catalogue should be obtained. For guidance on browsing the commercial catalogue, please see the section above, \"Browsing the catalogue\".</p> <pre><code># Search for airbus data that meets certain criteria\nsearch = stac_client.search(collections=[\"airbus_pneo_data\"], bbox=[10.854492,49.823809,100.762709,60.860699], max_items=10)\n\n# View all the items returned\nfor item in search.items():\n    print(item.id)\n\n# For demonstration purposes, pick one of these items\nchosen_item = item\n\n# with the user's chosen item then can extract the link for this item in preparation to order its associated data\nitem_links = chosen_item.links\n\nfor link in item_links:\n    if link.rel == \"self\":\n        item_href = link.href\n        print(f\"Chosen item href: {item_href}\")\n</code></pre> <p>You can obtain a quote for a commercial item via a POST request to /quote at the item's href. This must be done with Hub authorisation as it will use your linked API key. Airbus requires a licence as part of the request. Airbus optical and most Planet items can be clipped with an AOI to limit the area to be purchased</p> <pre><code>import requests\n\ntoken = \"\"  # Your Hub API should be stored and obtained securely\n\n# An example item for demonstration purposes\nitem_href = \"https://eodatahub.org.uk/api/catalogue/stac/catalogs/commercial/catalogs/airbus/collections/airbus_phr_data/items/DS_PHR1A_201203021558128_FR1_PX_W080S03_0221_01728\"\n\nurl = f\"{item_href}/quote\"\nheaders = {\n    'accept': 'application/json', \n    'Content-Type': 'application/json', \n    'Authorization': f'Bearer {token}'\n}\ndata =  {\n    \"licence\": \"Standard\",\n    \"coordinates\": [\n        [\n            [-79.8,-2.1], \n            [-79.8,-2.2], \n            [-79.95,-2.2], \n            [-79.95,-2.1], \n            [-79.8,-2.1]\n        ]\n    ]\n}\n\nresponse = requests.post(url, headers=headers, json=data)\n\nprint(\"Status Code\", response.status_code)\nprint(\"Response \", response.json())\n</code></pre> <p>Once you are happy with the item and pricing, place an order directly through the API. The response body will contain the STAC item ingested into your workspace catalogue, and there will be a location header to this item that can be used to track the status of your order and view assets when they are delivered.</p> <pre><code>import requests\n\nitem_href = \"https://eodatahub.org.uk/api/catalogue/stac/catalogs/commercial/catalogs/airbus/collections/airbus_phr_data/items/DS_PHR1A_201203021558128_FR1_PX_W080S03_0221_01728\"\nurl = f\"{item_href}/order\"\nheaders = {\n    'accept': 'application/json', \n    'Content-Type': 'application/json', \n    'Authorization': f'Bearer {token}'\n}\ndata =  {\n    \"licence\": \"Standard\",\n    \"endUserCountry\": \"GB\",\n    \"productBundle\": \"General use\", \n    \"coordinates\": [\n        [\n            [-79.8,-2.1], \n            [-79.8,-2.2], \n            [-79.95,-2.2], \n            [-79.95,-2.1], \n            [-79.8,-2.1]\n        ]\n    ]\n}\n\nresponse = requests.post(url, headers=headers, json=data)\n\nprint(\"Status Code\", response.status_code)\nprint(\"Response \", response.json())\n</code></pre>"},{"location":"documentation/APIs/example-tutorials/#workflows","title":"Workflows","text":"<p>Workflows can be uploaded, run, and managed through Hub APIs using your workspace token for authorisation. For details, please see the dedicated Workflow help page.</p>"},{"location":"documentation/APIs/example-tutorials/#workspace-data-management","title":"Workspace data management","text":"<p>Files stored in your workspace block and object stores can be viewed through authenticated GET requests in a browser or an API call, but they can also be managed through PUT or DELETE requests to the Hub API.</p> <pre><code>curl --request PUT --upload-file file.tif -H \"Authorization: Bearer &lt;HubAPItoken&gt; https://&lt;workspace&gt;.eodatahub-workspaces.org.uk/files/workspaces-eodhp/file.tif\n</code></pre>"},{"location":"documentation/APIs/example-tutorials/#data-visualisation","title":"Data Visualisation","text":"<p>Data can be visualised in the EO DataHub Platform, allowing users to render and explore geospatial datasets (such as satellite imagery or raster data) directly in their browser or GIS software, without needing to download entire files. See the Data Visualisation page for examples and information on the APIs used to accomplish this.</p>"},{"location":"documentation/APIs/pyeodh/","title":"API's: pyeodh","text":"<p>'pyeodh' is a Python client wrapping all EODH API endpoints. It aims to be lightweight and easy to use and has been developed especially for use with the EODH platform. If you are a data analyst or a developer, and you would like to interact fully with the Hub then using pyeodh is the recommended way to do so.</p> <p>Explore the API documentation </p>"},{"location":"documentation/Access%20Points/applications/","title":"Applications","text":""},{"location":"documentation/Access%20Points/applications/#climate-asset-risk-analysis-tool-clarat","title":"Climate Asset Risk Analysis Tool (CLARAT)","text":"<p>The Climate Asset Risk Analysis Tool (CLARAT), an application powered by the EODH, derives hazard indicator datasets from climate projections stored on EODH. When a user requests an asset portfolio risk assessment, the EODH provides cloud computing infrastructure to run workflows that determine exposure and vulnerability. CLARAT helps assess the future exposure and vulnerability of physical assets to climate hazards, such as chronic heat, based on climate projections for various scenarios. It also incorporates historic Earth Observation data, like land surface temperature, to enhance understanding of local variations such as urban heat islands. This helps decision-makers make informed financial, economic, and policy decisions. CLARAT\u2019s user-friendly, no-code interface is designed for entry-level to intermediate users with limited experience in Earth Observation data and workflows.</p> <ul> <li>Prerequisites: Asset shapefile (CSV, point data) comprising a unique ID with latitude and longitude fields</li> <li>Entrypoint: https://demo.eodh.sparkgeo.dev/</li> <li>Documentation: https://sparkgeo.github.io/uk_eodatahub/</li> <li>Quality-assured: Read more</li> </ul> <p></p>"},{"location":"documentation/Access%20Points/applications/#eopro","title":"EOPro","text":"<p>EOPro aims to provide a web based user-friendly platform for both new and advanced users of Earth Observation (EO) data, especially those already familiar with geospatial software. Powered by the EODH, its primary goal is to allow users to run predefined scenarios and obtain meaningful outputs without needing to understand the specifics of source data types. Predefined scenarios available at this stage are Land Cover Changes and Water Quality Analysis scenarios. EOPro also enables users to adjust inputs and learn how these changes affect results, serving as both a learning tool and a frontend for streamlined EO processing. Additionally, EOPro caters to intermediate users by offering a quick, code-free method for defining workflow inputs for specific tasks. The core feature of EOPro is the Action Creator, a powerful tool that helps users create customized workflows for processing EO data and providing clear visual and numeric results through built in visualization and analysis tools</p> <ul> <li>Prerequisites: No prior EO experience</li> <li>Entrypoint: https://eopro.eodatahub.org.uk/</li> <li>Documentation: Available in-app. Access the general Help guide from the main toolbar and find Action Creator specific help under the Help tab. The EOPro User &amp; Architecture Guide provides further reference material and documentation.</li> <li>Quality-assured: Read more</li> </ul> <p></p>"},{"location":"documentation/Access%20Points/integrations/","title":"Integrations","text":""},{"location":"documentation/Access%20Points/integrations/#pyeodh","title":"pyeodh","text":"<p>\u2018pyeodh\u2019 is a lightweight Python client for easy access to EODH APIs. It is a Python-based tool tailored to facilitate communication with the specific API endpoints exposed by the EODH platform. Using \u2018pyeodh\u2019, developers and scientists can programmatically access the API\u2019s features\u2014such as sending requests, retrieving data, or executing commands\u2014without needing to handle the underlying details such as crafting HTTP requests or managing authentication manually. By abstracting these complexities, \u2018pyeodh\u2019 makes it easier to integrate the API into Python applications, enabling developers to focus on building features rather than managing low-level networking tasks.</p> <ul> <li>Prerequisites: Python</li> <li>Entrypoint: https://pyeodh.readthedocs.io/en/latest/index.html</li> <li>Documentation: https://eo-datahub.github.io/eodh-training/api-client/1_ClientIntro.html</li> </ul>"},{"location":"documentation/Access%20Points/integrations/#qgis-plugin","title":"QGIS plugin","text":"<p>The QGIS plugin automates the connection to a user\u2019s workspace on the Hub, allowing a user to access pre-developed EO Application Packages (EOAPs) for processing data. Once a required EOAP has been found, the user can parameterise and execute the workflow. Upon completion the GIS user can then interact with the returned data assets and can view the processing logs, within their QGIS project environment.</p> <ul> <li>Prerequisites: CWL workflows in a workspace</li> <li>Entrypoint: https://github.com/EO-DataHub/eodh-qgis</li> <li>Documentation: https://eo-datahub.github.io/eodh-training/plugin/1_introduction.html</li> </ul> <p></p>"},{"location":"documentation/Access%20Points/integrations/#jupyter-hub","title":"Jupyter Hub","text":"<p>The AppHub is a JupyterHub instance made available to users to ensure simple access to information held within the Resource Catalogue. It also allows the creation of scientific analytical workflows and associated data processing. Once logged in to the AppHub a user is able to upload notebooks and data, or create new notebooks, from where they can interact with other EODH components using tools such as pyeodh.</p> <ul> <li>Prerequisites: Python, Jupyter Notebooks</li> <li>Entrypoint: https://prod.eodatahub-workspaces.org.uk/notebooks/hub/home and pyeodh</li> <li>Documentation: https://eo-datahub.github.io/eodh-training/platform/2_Components.html</li> </ul> <p></p>"},{"location":"documentation/Access%20Points/integrations/#eoap-gen","title":"eoap-gen","text":"<p>eoap-gen is a Python-based tool to help dev-ops specialists or specialist technicians to create compliant Earth Observation Application Packages (EOAPs) that will run on the EODH. It is a CLI tool for generating Earth Observation Application Packages, including CWL workflows and Dockerfiles, from user supplied Python scripts. The tool is intended to support specialist users to create custom workflows within their workspace that can be used within the wider suite of EODH interfaces to generate outputs. The Hub only supports workflows supplied in compliant Common Workflow Language (CWL) format, in an attempt to align with the move towards a standardised approach to Earth Observation analytics scripting across the sector. More information on \u201cWhy CWL?\u201d can be found here.</p> <ul> <li>Prerequisites: Python, CWL</li> <li>Entrypoint: https://github.com/EO-DataHub/eoap-gen</li> <li>Documentation: https://eo-datahub.github.io/eodh-training/workflows/1_Workflows.html</li> </ul>"},{"location":"documentation/Access%20Points/platform/","title":"Earth Observation DataHub Platform","text":"<p>The Earth Observation DataHub (EODH) is a centralised software infrastructure which provides a \u2018single point\u2019 of access for UK Earth Observation (EO) data obtained from distributed sources, to include public and commercial centres. The Web Presence, also referred to as the Hub Platform, is likely to be the route most new users take to interact with the EODH platform. This is a user interface (UI) hosted on the Hub website that allows users to interact with EODH and manage their user account. From here API keys can be generated and managed in the user Workspace, and the Notebook service and Resource Catalogue can be accessed. The Resource Catalogue (RC) is a searchable store of data links, workflows, and processed datasets, including open, commercial, and private resources, managed by a user authentication access system. It is a collection of STAC catalogues that can be searched visually via the Web Presence or programmatically via clients such as pyeodh. An updated UI for the Resource Catalogue is currently in development and will be released by the end of March 2025.</p> <ul> <li>Prerequisites: EODH account</li> </ul>"},{"location":"documentation/apps/apps/","title":"What is an App?","text":"<p>An app, in the context of the UK EO DataHub, is a 3rd party application that can access the Hub APIs and utilise them to provide bespoke functionality to the end user. This can involve querying STAC data from the resource catalogue and processing the data with custom workflows to output bespoke data products. If the app has a frontend, this data can also be displayed in useful ways to the user.</p>"},{"location":"documentation/apps/apps/#when-should-i-create-an-app","title":"When should I create an App?","text":"<p>You should create an app if you require any of the following:</p> <ul> <li>You have a data processing workflow you wish to expose to end users, either for them to run on public data, on their own data (in the EO DataHub) or from your own data (also in the EO DataHub).</li> <li>You wish to combine datasets available from the EO DataHub into bespoke data products and make them available to end users through your app.</li> <li>You have custom views in a web app you can present EO DataHub data to end users.</li> </ul>"},{"location":"documentation/apps/apps/#types-of-apps","title":"Types of Apps","text":"<p>Apps can be:</p> <ul> <li>Client-side web apps</li> <li>Server-side web apps</li> <li>Desktop app</li> <li>Mobile app</li> </ul> <p>As long as you can integrate with the Hub OIDC and retrieve an OIDC token on behalf of the end user then your app can utilise the Hub APIs on behalf of an end user.</p>"},{"location":"documentation/apps/billing/","title":"Billing","text":"<p>There are several aspects to billing for hub apps, although not all of them will apply to some types of app: charges made by the hub to be paid by the app developer, charges made by the hub paid by the app end user, and charges made by the app developer to be paid by the app user.</p>"},{"location":"documentation/apps/billing/#data-hub-charges-to-the-end-user","title":"Data Hub Charges to the End User","text":"<p>If an app seeks authorization from the end user to access the end user's own hub account then it may run workflows, store data and perform other operations that will result in charges. The hub will bill the end user directly for these just as if the end user had performed them directly. The app and app developer will not be able to access data about these charges or add any mark-up.</p> <p>An app developer who wishes to use pre-created workflows as part of the app's computation and wants that computation to be billed directly to the user must make the workflow public. This means that any user can run it even without using the app or being known to the app developer. If this is not acceptable then consider creating a user service.</p>"},{"location":"documentation/apps/billing/#data-hub-charges-to-the-app-developer","title":"Data Hub Charges to the App Developer","text":"<p>Note</p> <p>An app developer is only charged for an end user's use of an app when:</p> <ul> <li>the app uses an API key generated by the app developer for their own workspace (suitable only for server-side apps), or</li> <li>the app developer defines a user service in their own workspace and designs the app to call it on behalf of the end user, or</li> <li>the app downloads public data from the app developer's workspace store, incurring egress charges.</li> </ul> <p>Warning</p> <p>App developers cannot prevent charges being incurred for user service invocations or egress charges for end users who bypass the app and access them directly. </p> <p>An app developer can design the app to fetch a time-limited token from their own server, relying on the single-origin policy or on a login cookie to identify legitimate app users, and then validate that token inside the user service. Alternatively the user service could validate that the calling workspace is a legitimate app user. This can be found in the <code>calling_workspace</code> input.</p>"},{"location":"documentation/apps/billing/#app-developer-charges-to-the-end-user","title":"App Developer Charges to the End User","text":"<p>The Hub does not currently facilitate charging an app's end user for their use of an app, even where the end user is also a hub user. App developers must record access and bill their users directly.</p> <p>If your app is a server-side app then it can record accesses directly. Developers of client-side apps that depend on calling hub user services must rely on the user service workflow itself. These should be written to validate that the calling workspace is a legitimate user of the app who can be charged before returning any results. If necessary, they should also record any access data required for billing into files in the developer's workspace stores or call APIs on the developer's own servers.</p>"},{"location":"documentation/apps/creating-an-app/","title":"Creating an App","text":"<p>To provide bespoke functionality to end users app developers can integrate their apps with the UK EO DataHub. This allows for user single sign-on between your app and the DataHub, allowing you to call the Hub APIs on behalf of the user. You can access the resource catalogue, user workspaces, your app workspaces and execute workflows to process the data.</p>"},{"location":"documentation/apps/creating-an-app/#integration-with-the-hub-authentication","title":"Integration With the Hub Authentication","text":"<p>To integrate your app with the EO DataHub you must request an OIDC client be created for you in the Hub. For now, this is a manual request using our Help Scout beacon (bottom left).</p> <p>You can request either a Public or Confidential client. A public client is used for a front end application, whereas a confidential client is used for a backend app. One of the differences is that a frontend client will only require a client ID to authenticate, whereas a backend client requires both a client ID and client secret.</p> <p>The EO DataHub uses Keycloak as its OIDC Identity Provider. Keycloak clients exist for most major programming languages and will assist in your integration, although you are also free to write your own and implement the required OIDC flows.</p> <p>Once your OIDC client has been created, you will be provided with a client ID (and secret, if a confidential app), and you are ready for integration.</p>"},{"location":"documentation/apps/creating-an-app/#oidc-flows","title":"OIDC Flows","text":"<p>Your Hub client supports the following OIDC flows:</p> <ul> <li>Standard Flow - the default. This allows the user to authenticate with the Hub using a 3rd party Identity Provider. Currently GitHub and Google are both supported.</li> <li>Direct Access Grant - This allows the user to directly authenticate with a username and password. This is discouraged for the user, but may be necessary for some use cases, such as automated integration tests.</li> <li>Service Account Roles - This allows your app to authenticate as a machine user. Although not a common use case for EO DataHub apps, it is available as an option.</li> </ul> <p>Note</p> <p>Generally, Standard Flow will suffice.</p>"},{"location":"documentation/apps/workflows-and-user-services/","title":"App workflows and user services","text":"<p>A key way to provide applications to users is to define and deploy services via the Hub. These can be made available to other users via an API allowing them to carry out processing on their own datasets. This can be done via the Workflow Runner, in particular by deploying and publishing user services for other users.</p>"},{"location":"documentation/apps/workflows-and-user-services/#workflows","title":"Workflows","text":"<p>A workflow is an OGC Application Packaged defined using Common Workflow Language, or CWL. This is a process description defining a sequence of processing on given inputs. A single workflow can define a number of steps, with the outputs from one step being fed into another, in order to generate a final results output, structures in a STAC Catalog. An example for such a process might be the calculate of Normalised Difference Vegetation Index, or NDVI, to generate an output file identifying areas of significant vegetation, for use in climate change studies and the impact on greenery.</p> <p>Such a workflow might be written by a developer and published on the Hub for others to discover and use, using their own inputs either hosted on the Hub or uploaded by the user themselves. Such workflows will be provided as an API endpoint which can be shared with other users should they want to use these workflows.</p> <p>This functionality allows users to develop and share their processing workflows as applications, while also providing cloud processing resources on which to run the workflows. Using the Hub's infrastructure will enable easier execution for user's who are less experienced with CWL or cloud-processing.</p>"},{"location":"documentation/apps/workflows-and-user-services/#publishing-workflows","title":"Publishing Workflows","text":"<p>As an extension to Workflows, application developers can also publish their workflows for others to use, who do not have access to the workspace in which they were deployed. Setting a workflow public can be done via the Data Loader, by defining an access policy for your workflow and marking it as public.</p> <pre><code>{\n    \"block-store\": {},\n    \"object-store\": {},\n    \"catalogue\": {},\n    \"workflows\": {\n        \"&lt;workflow-id&gt;\": {\n            \"access\": \"public\"\n        }\n    }\n}\n</code></pre> <p>When this access policy is loaded into the Hub, it will update the specified workflow to be public, meaning other users can view and execute the workflow deployed into  using the API endpoint:  <pre><code>/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/processes/&lt;workflow-id&gt;/execution\n</code></pre>"},{"location":"documentation/apps/workflows-and-user-services/#accounts-and-billing","title":"Accounts and Billing","text":"<p>Note</p> <p>When a public workflow is executed by another workspace, it will be executed within that calling workspace, with any billing and charges being sent to that workspace, rather than the deploying workspace.</p> <p>While the workspace is deployed to one workspace and executed in another, access is still handle as if the calling workspace owned that workflow, no additional access is allowed to the deploying workspace.</p>"},{"location":"documentation/apps/workflows-and-user-services/#user-services","title":"User Services","text":"<p>An additional function when publishing workflows is to specify a workflow as a user service. This also allows other users to access the workflow, just as with setting it public, however this time the workflow will be executed within the deploying workspace, rather than the calling workspace.</p>"},{"location":"documentation/apps/workflows-and-user-services/#accounts-and-billing_1","title":"Accounts and Billing","text":"<p>As a user service is executed within the deploying workspace, any charges incurred will be sent to this workspace, rather than the calling workspace. It is then up to the deploying workspace account owner to settle these charges.</p> <p>A user-service can be deployed as normal via the Workflow Runner API and then the access-policy for this workflow can be updated by specifying the type for that workflow as as \"user-service\".</p> <pre><code>{\n    \"block-store\": {},\n    \"object-store\": {},\n    \"catalogue\": {},\n    \"workflows\": {\n        \"&lt;workflow-id&gt;\": {\n            \"access\": \"private\", \n            \"type\": \"user-service\"\n        }\n    }\n}\n</code></pre> <p>Here, we specify the workflow as private, but also a user-service. Setting the type as a \"user-service\" will overwrite the \"public\" setting ensuring the workflow is only available as a user-service, rather than as a normal public workflow.</p>"},{"location":"documentation/apps/workflows-and-user-services/#access-control","title":"Access Control","text":"<p>Note, when a workflow is executed in a workspace, it gains access to the object and block stores associated with that workspace, it can also use a workspace-scoped token for that workspace, meaning when a user service is executed, it can still make calls to the deploying workspace data stores. This allows the workflow to make use of workspace data that is private only to the deploying workspace. This is a key difference from simply publishing a workflow as public, as access there is restricted to only the calling workspace, just as when you execute a workflow deployed in your own workspace.</p>"},{"location":"documentation/apps/workflows-and-user-services/#example-use-case","title":"Example Use Case","text":"<p>An example use case might be a workflow that takes some input, applies a preconfigured mask, and returns a final image. This workflow is deployed as a user service by workspace A and it being invoked by workspace X. In this case, the mask details can be stored in A's object store, and then inputs can come from workspace X's object store. The user service will have the correct access to retrieve the data first from X's store, and then from the A's object store, before returning the final output file to X's object store and resource catalogue sub-catalog.</p>"},{"location":"documentation/apps/workflows-and-user-services/#example-workflow-applications","title":"Example Workflow Applications","text":"<p>Some example published workflows are available at the following API endpoints, these are public, and not user services, meaning the services will be executed within the calling workspace and any associated resource usage will be logged against that workspace. You will need a valid workspace-scoped token in order to access these endpoints, please pass your token in the authorization header of your requests as a Bearer token: \"Authorization: Bearer \". <ul> <li>Take an image specified by a URL and resize it by a given scale percentage</li> <li>Take an image specified by a STAC item and resize it by a given scale percentage</li> <li>Take STAC items, an area of interest, EPSG definition and a set of bands to identify water bodies based on NDWI and Otsu threshold</li> </ul> <p>See some example inputs for these services in this Notebook.</p>"},{"location":"documentation/data-publishing/data-publishing/","title":"Data publishing","text":"<p>The Hub allows users to upload and publish their data for others to access and use, for example in workflow processing or Jupyter Notebooks. This guide will take you through first how to upload metadata to your workspace, how to upload associated data files to your workspace and then how to publish this data so others can discover it via the Hub APIs.</p>"},{"location":"documentation/data-publishing/data-publishing/#what-is-metadata","title":"What is Metadata?","text":"<p>Metadata on the Hub takes the form of SpatioTemporal Asset Catalogs (STAC), which provides definitions for datasets formatted as JSON files, similar to GEOJSON, with agreed keys such as spatial and temporal data. STAC provides a standardized way to expose collections of spatial temporal data. STAC is used through the DataHub, including in our Resource Catalogue API, see this example here for a dataset hosted on the Hub. This example is for a STAC Collection with links to a number of Items, which define the actual data points in the dataset. Each Item may then contain a number of assets, which are files containing the data defined inside the dataset, for example these might be COG (Cloud-Optimised GeoTIFS) or NetCDF files.</p>"},{"location":"documentation/data-publishing/data-publishing/#uploading-metadata","title":"Uploading Metadata","text":"<p>The Hub provides a Metadata Loader which can be used to upload metadata into the your workspace object stores for this to then be harvested into the workspace Catalog within the Resource Catalogue on the Hub. The Data Loader takes STAC metadata files and loads these into your private workspace Catalog.</p> <p>First navigate to the Workspaces page on the Hub, logging in if necessary, and select the workspace you wish to load data into from the icons on the left. Then select STAC Metadata Loader from the menu on the left side of the page.</p> <p></p> <p>The first step is to select the Catalog in which you wish to load your new STAC metadata. The available catalogs in your workspace will be listed in the drop-down, in descending order of depth. Select the Catalog in which you wish to load your STAC metadata.</p> <p></p> <p>As STAC data is organised into Catalogs, Collections and Items, you must next select the Collection you wish to load any STAC Items into. Another drop-down list is provided containing any Collections within the selected Catalog, if instead you wish to load your data into a new Collection, you can specify a new Collection ID in the text box, to invoke the creation of a new template Collection.</p> <p></p> <p>Some CTA boxes are also provided to quickly view details of the Catalog, Collection and current Items before uploading your data to ensure you are happy with your selections.</p> <p></p> <p>Now you are ready to upload your STAC Item metadata files. Select the Choose Files box and select any STAC Items you wish to upload to your selected Catalog and Collection.</p> <p></p> <p>Once you have selected the Items you wish to upload to your selected Catalog and Collection, click the Submit button.</p> <p></p> <p>This will send your chosen Items for validation to ensure they are valid STAC JSON before you upload them to the Hub. Note, throughout the validation, uploading and loading steps process updates will be reported in the bottom left of the page to update you on the status of your loader.</p> <p></p> <p>The final loading step may take some time to complete, but once done so you will see the View Data button available which will take you directly to the Items you just uploaded inside the resource Catalogue.</p> <p>An example of a STAC Feature Collection is shown here.</p> <pre><code>{\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"stac_version\": \"1.0.0\",\n      \"stac_extensions\": [],\n      \"id\": \"test-item\",\n      \"collection\": \"test\",\n      ...\n    },\n    {\n      \"type\": \"Feature\",\n      \"stac_version\": \"1.0.0\",\n      \"stac_extensions\": [],\n      \"id\": \"test-item-2\",\n      \"collection\": \"test\",\n      ...\n    },\n    {\n      \"type\": \"Feature\",\n      \"stac_version\": \"1.0.0\",\n      \"stac_extensions\": [],\n      \"id\": \"test-item-3\",\n      \"collection\": \"test\",\n      ...\n    }\n  ]\n}\n</code></pre>"},{"location":"documentation/data-publishing/data-publishing/#uploading-data","title":"Uploading Data","text":"<p>STAC metadata is usually associated with some data which is of most use to scientists and app-developers. This data can take many forms but is often provided as COGs or GEOTIFS, and these are linked to in the assets sections of STAC Items.</p> <p>To upload data files to your workspace, you will need to use the S3 credentials generated from the Workspaces UI, or you can use the S3 directory in a Jupyter Notebook, launched in the correct workspace. These processes will upload your files to your Workspace object store, and can then be accessed either via an S3 client, inside your Jupyter Notebook or via HTTPS, just as with other object store data files.</p> <p>In order for this data to be discoverable within the Resource Catalogue, you need to make sure the STAC Items you load have the assets correctly linked to. The assets in your Item must follow the below structure.</p> <pre><code>\"assets\": {\n    \"asset-title\": {\n        \"href\": \"https://&lt;workspace&gt;.workspaces.eodhp.eco-ke-staging.com/files/workspaces-eodhp/path/to/data/asset\",\n        \"type\": \"image/png\",\n        \"roles\": [\n            \"data\"\n        ]\n    }\n}\n</code></pre> <p>Tip</p> <p>You need to construct the HREF link based on the location of the file inside your object store. </p> <p>For example, if you upload a file to your workspace, called <code>test-workspace</code>, with the object store at <code>/saved-data/assets/asset1.tif</code> you can construct your HTTP reference for that file as <code>https://test-workspace.workspaces.&lt;domain&gt;/files/workspaces-eodhp/saved-data/assets/asset1.tif</code>. </p> <p>Once you have added this HREF to your STAC Item you can upload and load it following the previous guidance and your asset will then be accessible directly from the asset links provided in the Resource Catalogue STAC entry for this Item.</p>"},{"location":"documentation/data-publishing/data-publishing/#publishing-datasets","title":"Publishing Datasets","text":"<p>Currently, all the metadata and data uploaded previously in this tutorial will be set as private in the workspace object store and Resource Catalogue, meaning it is only accessible to users who are a member of the workspace that owns the data in the Hub. However, you are also able to publish data so that others can find it publicly without needing to be a member of the owning workspace.</p> <p>Data publishing is handled via the Publisher, with inputs also being structured as JSON but with a unique format that must be followed to update the access policies for your data.</p> <p>Access policies allow you to specify directories (datasets) and data within your workspace that are marked as public or private. You can set these policies for the following workspace data:</p> <ul> <li>Workspace Object Store Data</li> <li>Workspace Block Store Data</li> <li>Resource Catalogue entries</li> <li>Workflows</li> </ul> <p>The Publisher allows you to configure the access policies by uploading a JSON file defining paths with associated access settings, as in the below example.</p> <pre><code>{\n    \"block-store\": {\n        \"/path/to/directory\": {\"access\": \"public\"},\n        \"/path/to/file/another.json\": {\"access\": \"private\"}\n    },\n    \"object-store\": {\n        \"/path/to/file/qa-check\": {\"access\": \"public\"}\n    },\n    \"catalogue\": {\n        \"/catalogs/user/catalogs/workspace/catalogs/test-catalog\": {\"access\": \"public\"},\n\"/catalogs/user/catalogs/workspace/catalogs/other-test-catalog/collections/test-collection\": {\"access\": \"public\"}\n    },\n    \"workflows\": {\n        \"my-workflow-name\": {\"access\": \"public\"}\n    }\n}\n</code></pre> <p>This example marks the following block store paths as public:</p> <pre><code>/path/to/directory\n/path/tofile/another.json\n</code></pre> <p>It also marks the following object store data as public:</p> <pre><code>/path/to/file/qa-check\n</code></pre> <p>As well as the following Catalog and Collection in the Resource Catalogue as public:</p> <pre><code>/catalogs/user/catalogs/workspace/catalogs/test-catalog\n/catalogs/user/catalogs/workspace/catalogs/other-test-catalog/collections/test-collection\n</code></pre> <p>And marks the following workflow as public:</p> <pre><code>my-workflow-name\n</code></pre> <p>Once you have configured such a file locally, you can then apply this access policy to your workspace. To do so, first open the Publisher page in the Workspaces UI</p> <p></p> <p>Select the file you wish to upload.</p> <p></p> <p>Then click Submit to validate the file and upload it to apply the changed to your workspace data stores.</p> <p>You can now verify the access has been applied by attempting to access any files you have set as public from an unauthenticated browser, via HTTPS, or for workflows you can try accessing the Workflow Runner API process definitions for any of your public workflows with another user account.</p> <p>For example for a workflow output located at the following URL in the Resource Catalogue <code>https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/catalogs/processing-results/catalogs/&lt;workflow-id&gt;</code> logout of the Hub using the Homepage and revisit the URL to confirm you can access the data without authentication.</p>"},{"location":"documentation/notebook-service/git-integration/","title":"Version control and notebooks","text":"<p>The Git extension for Jupyter Lab allows users to perform Git actions using the notebook web UI. This allows a user to clone example Notebooks or workflows directly into their notebook instance so that they can run such code inside their own workspace.</p> <p>Git integration also allows users to develop code and push changes directly from the notebook environment, enabling version control and allowing code to be shared with other developers.</p> <p>The extension is available through the left sidebar.</p> <p></p> <p>Note</p> <p>Make sure you are in the desired file system directory before switching to the Git extension panel.</p>"},{"location":"documentation/notebook-service/git-integration/#initialisation","title":"Initialisation","text":"<p>Before using the Git extension, you must tell Git who you are. Execute the following commands in the notebook terminal. To open the notebook terminal, follow this guide.</p> <pre><code>git config --global user.name \"My Name\"\ngit config --global user.email \"myemail@example.com\"\n</code></pre> <p>You can set this up with any name and email address you choose. These details will appear if you update any GitHub repositories. You may choose to use the same name and email address as is associated with your GitHub account, but this is not required.</p>"},{"location":"documentation/notebook-service/git-integration/#authorisation","title":"Authorisation","text":"<p>The Git extension can handle the authorisation for http(s) repositories by requesting your username and password, when required. Any entered credentials will be cached for one hour.</p> <p>If, instead, you wish to authenticate against the remote repository using SSH (Secure Shell), you will need to configure your SSH credentials within the notebook environment. To do this, first generate an SSH key pair, for example using ssh-keygen, and then add the public key to your Git provider account. This then allows you to clone a Git repository using the SSH link, rather than http(s).</p> <p>Tip</p> <p><code>ssh-keygen</code> will guide you through creating local SSH credentials, which you can then add to your remote Git provider.</p>"},{"location":"documentation/notebook-service/git-integration/#cloning-a-repository","title":"Cloning a repository","text":"<p>To clone your GitHub repository, select Clone a Repository in the left panel and paste your repository's URL into the pop up box. Once successfully cloned you should receive the confirmation as below.</p> <p></p> <p>Note</p> <p>For SSH authentication, you can instead use the terminal and SSH link provided by your Git provider.</p> <pre><code>    git clone git@github.com:&lt;owner&gt;/&lt;repository-name&gt;.git\n</code></pre> <p>For public repositories, no authentication is required when cloning them into your notebook. However, for private repositories, you will be prompted at this point to provide access credentials, including your username and personal access token.</p> <p>You can create a personal access key via GitHub following this documentation. Your username should reflect the username associated with your GitHub account. This does not need to be the same account associated with your DataHub account, even if you used GitHub authentication when creating your DataHub account.</p> <p>Warning</p> <p>If the repository fails to clone, your credentials may be incorrect, ensure that your access token has the correct permissions to access the selected repository.</p> <p>The repository should appear as a named folder in the left-hand pane.</p> <p>You can now make the desired edits to your file or code in the editor. These changes will log in the Changed tab under the name of the edited file.</p> <p></p>"},{"location":"documentation/notebook-service/git-integration/#committing-changes-to-your-repository","title":"Committing changes to your repository","text":"<p>The new changes can be staged by clicking the + icon. The edits should move to the Staged tab from the Changed tab.</p> <p>You will then be prompted to add a commit summary and description (optional). Once all the required fields are populated, the commit button will enable, by turning blue.</p> <p></p> <p>You can now click the Commit button to commit these changes to the branch. You should receive the following success message.</p> <p></p>"},{"location":"documentation/notebook-service/git-integration/#pushing-committed-changes-to-github","title":"Pushing committed changes to GitHub","text":"<p>Click the icon in the top-left banner, indicated by an orange dot, to push the changes to your GitHub repository.</p> <p></p> <p>Once clicked, you will be asked for the following information to connect to that repository.</p> <p></p> <p>The instructions to generate a personal access token can be found above, ensure your token has read/write access to your selected repository.</p> <p>Click Ok to push the committed changes. You should receive notification that the push was successful as below. Visit your GitHub repository to confirm the pushed changes are reflected.</p> <p></p> <p>If, instead, you see a warning saying the push failed, as below, please check your username is correct and that your personal access token has the correct permissions to make changes to your chosen repository.</p> <p></p>"},{"location":"documentation/notebook-service/installing-additional-packages/","title":"Installing additional packages","text":"<p>To install additional packages in Python notebooks you can use pip. You can install packages globally using <code>pip install &lt;package_name&gt;</code>, however these installs will not persist across notebook restarts. If you wish to install a package persistently, you must install it to your user directory with <code>pip install --user &lt;package_name&gt;</code>.</p> <p>Example</p> <p>If we were to install the <code>geopandas</code> package:</p> <pre><code>pip install geopandas # will not persist\npip install --user geopandas # will persist\n</code></pre>"},{"location":"documentation/notebook-service/notebooks-start/","title":"Starting a notebook","text":"<p>This guide will introduce the UK EO DataHub notebooks and provide a walk-through for getting started.</p>"},{"location":"documentation/notebook-service/notebooks-start/#navigating-to-the-notebooks","title":"Navigating to the notebooks","text":"<p>From the UK EO DataHub homepage, select notebooks from the navigation bar.</p> <p></p> <p>You will be requested to sign in, if you are not already.</p>"},{"location":"documentation/notebook-service/notebooks-start/#jupyter-hub","title":"Jupyter Hub","text":"<p>You will arrive at the Jupyter Hub. The Jupyter Hub allows management of notebook servers. From here, you can start, stop and create new notebooks.</p> <p></p> <p>Your default server is controlled using the button(s) in the top center. Let's start the default server by clicking \"Start My Server\".</p> <p></p> <p>Here you are presented with your options for your default server instance. These options are available each time you start any server.</p> <p>First you must select your workspace. The UK EO DataHub provides tenanted workspaces to keep data segregated. The notebook workspace selection controls what workspace stores will be mounted into your notebook. You need to have a workspace setup before you can start a notebook server. See here to create a workspace.</p> <p>Next, you must select a profile. The profile controls the image used to run the notebook server. The EO DataHub default image is recommended for most use cases, but vanilla Python and R images have also been provided. The EO DataHub image comes with some additional Linux and Python packages to help integrate with the EO DataHub, including pyeodh Python package, which includes a client for calling the EO DataHub APIs.</p> <p>Once you have selected your workspace and profile, click \"Start\".</p> <p></p> <p>You should see a loading screen as above. If any errors are reported, please contact a site administrator.</p> <p></p> <p>If notebook creation is successful, you should be forwarded to the browser interface for your notebook.</p>"},{"location":"documentation/notebook-service/notebooks-start/#creating-a-notebook","title":"Creating a notebook","text":"<p>To create a new notebook, you can either select the Python 3 (ipykernel) tile from the Launcher window or right click the file browser and click New Notebook.</p> <p></p> <p>Once you have a notebook open, the first thing to do is usually name the file. Right click on the untitled file in the file browser and rename as appropriate.</p> <p></p> <p>You can write your code in the cells and execute using the play button at the top to execute the highlighted cell (or press shift+enter when cursor is in target cell). The output will appear below the cell.</p> <p>To create a new cell, use the context menus that appear over cells when hovering. You can also create cells before the highlighted cell, or delete the highlighted cell.</p>"},{"location":"documentation/notebook-service/notebooks-start/#terminal-access","title":"Terminal access","text":"<p>For more general access to the notebook server, you may utilise the Terminal. You can open a terminal instance from the notebook launcher.</p> <p></p> <p>From the terminal you can interact with the server OS as you would with Linux, for instance creating or moving files, or editing them with Nano editor.</p>"},{"location":"documentation/notebook-service/notebooks-start/#data-persistence","title":"Data persistence","text":"<p>When a notebook is started for a workspace, the workspace block (file) store and object (s3) stores are automatically mounted to the server for convenient access to your workspace stores.</p> <p>The block store is mounted to the root of your user's system access (\"/\"). The object store is mounted to \"/s3\" directory as a POSIX file system where you can interact with the object store as you would a regular Linux file system.</p> <p>By default, auto-save for open files is turned on in the Hub. You can disable this in Settings &gt; Autosave Documents.</p>"},{"location":"documentation/notebook-service/notebooks-start/#data-upload-download","title":"Data upload / download","text":"<p>You can use the notebook file browser to upload to or download from the notebook server, and therefore your workspace stores.</p> <p></p> <p>Tip</p> <p>Download a file by right clicking it and selecting download.</p> <p>Upload a file by clicking the upload icon in the file browser.</p>"},{"location":"documentation/notebook-service/notebooks-start/#named-servers","title":"Named servers","text":"<p>By default, JupyterHub only allows users to run one notebook server at a time. The UK EO DataHub allows additional notebooks to be run simultaneously, if required.</p> <p>To create a named server, enter a name for the new server (this must be done first, or your default server will be assumed) and click \"Add New Server\".</p> <p></p> <p>You will be requested to select your workspace and profile again, however please note that these can be set on each start of default or named servers.</p> <p>A new (named) server instance will be created, which can be used independently of other server instances. However, please note that multiple notebook servers cannot be created for the same workspace for the same user. If you start a new notebook server for a workspace that you already have a notebook server running for, the previous server will be shutdown and restarted for the new server.</p> <p></p>"},{"location":"documentation/notebook-service/notebooks-start/#shutting-servers-down","title":"Shutting servers down","text":"<p>Once you are done with your server, please shut it down. You can do so from the Jupyter Hub. If you are in a notebook, you can navigate to the Hub by following File &gt; Hub Control Panel.</p> <p></p> <p>From the Hub, ensure that you stop any running servers, default or named, when you are done with them.</p> <p>Note</p> <p>Logging out will not automatically shut down your notebook server. However, your notebook server will be culled after one hour of inactivity, if left unattended.</p>"},{"location":"documentation/notebook-service/sensitive-data/","title":"Handling sensitive information","text":"<p>A guide to securely managing sensitive data in notebooks.</p>"},{"location":"documentation/notebook-service/sensitive-data/#using-secrets-in-notebooks","title":"Using secrets in notebooks","text":"<p>When writing notebooks you may need to use sensitive data, such as username and passwords or API keys, to authenticate against external services. Storing these secrets directly in a notebook is not recommended, as you may accidentally share this data with others (e.g. by committing to Git).</p> <p>Instead, it is recommended that you save any sensitive data in .env files, separate from your notebook. These can be read in by your notebook for use in your code.</p>"},{"location":"documentation/notebook-service/sensitive-data/#environment-files","title":"Environment files","text":"<p>Environment files (.env file) are are a simple text file used to store sensitive information in a secure and organized way. They are a standard way of separating sensitive data from your notebook code, making it easier to share or version-control your scripts without exposing private information. They generally take the form below.</p> <pre><code>USERNAME=myusername\nPASSWORD=mypassword\n</code></pre> <p>.env files contain key, value pairs separated by an equals sign. One key, value pair per line. They generally have the extension \".env\", which can be the entire filename or you can create multiple .env files with different name, e.g. test1.env, test2.env.</p> Working on Linux <p>In Linux, on which the notebook server is based, filenames starting with \".\" are hidden, so they will not appear in the file browser panel. This may be desired when dealing with environment files containing sensitive data. If you wish to edit a hidden file, you can do so from the terminal by executing <code>nano .env</code>, which will use the terminal editor Nano to edit the file.</p>"},{"location":"documentation/notebook-service/sensitive-data/#creating-an-environment-file","title":"Creating an environment file","text":"<p>In the same directory as the notebook you are working in, create a plain text file named .env. You don't need any special tools or scripts, just create a regular file and name it .env. Make sure you have generated an API key for your workspace. Follow this guide to create one if you haven't already, or copy an existing one. Ensure the key you are using hasn't expired. Populate the text file with the example code above, replacing myusername with the name of your workspace and mypassword with your actual API key, sourced from the corresponding workspace. These must match for the workspace to authenticate correctly. For example:</p> <pre><code>USERNAME=\"my-eodh-workspace-name\"\nPASSWORD=\"XXXXXXXXXXXXXXX\"\n</code></pre> <p>More guidance on how to create an environment file can be found here.</p>"},{"location":"documentation/notebook-service/sensitive-data/#reading-in-environment-files","title":"Reading in environment files","text":"<p>When an environment file is read in, the key, values within are loaded into Python environment variables, which are available through the os.environ package. The Python package Dotenv can be useful to read environment files into environment variables.</p>"},{"location":"documentation/notebook-service/sensitive-data/#dotenv-python-package","title":"Dotenv (Python package)","text":"<p>The Dotenv Python package is included in the default python notebook image. It allows you to easily read in environment files specified by filename into Python environment variables, which can then be assigned to local variables.</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv(\".env\")\n\nusername = os.environ[\"USERNAME\"]\npassword = os.environ[\"PASSWORD\"]\n</code></pre>"},{"location":"documentation/notebook-service/sensitive-data/#sensitive-data-usage","title":"Sensitive data usage","text":"<p>Once you have separated out the storage of your sensitive data, it is also important not to expose it in your notebooks. Avoid printing or logging sensitive data, as this may be preserved in cell outputs. If you are using Git to version control your notebook, ensure that \"*.env\" has been added to your .gitignore file so that environment files are never committed.</p>"},{"location":"documentation/notebook-service/sensitive-data/#summary","title":"Summary","text":"<p>Sensitive data should not be stored directly in notebooks. It should be decoupled from notebook code and stored in separate .env (environment) files.</p> <p>The Dotenv Python package is provided to facilitate the use of .env files. Sensitive data should be read in from .env files and assigned to local variables in your notebook, for use as required.</p> <p>Danger</p> <p>Take care not to expose sensitive data by printing it out in cell outputs, or by committing your .env files to Git.</p>"},{"location":"documentation/resource-catalogue/data-discovery/","title":"Finding data","text":"<p>The Resource Catalogue provides a user-friendly interface for discovering and accessing Earth Observation data. This guide walks through the basic features of the catalogue's browse page and how to use its filtering and search capabilities.</p>"},{"location":"documentation/resource-catalogue/data-discovery/#the-browse-page","title":"The Browse Page","text":"<p>When you first visit the Resource Catalogue, you'll see the browse page which displays all available datasets. Each dataset is presented as a card containing:</p> <ul> <li>A thumbnail image</li> <li>The dataset collection title</li> <li>The date range covered by the dataset</li> </ul>"},{"location":"documentation/resource-catalogue/data-discovery/#filtering-data","title":"Filtering Data","text":"<p>The catalogue provides several filtering options at the top of the page to help you narrow down your search:</p>"},{"location":"documentation/resource-catalogue/data-discovery/#data-category-filter","title":"Data Category Filter","text":"<p>You can filter datasets by their category. For example:</p> <ul> <li>Climate projection data</li> <li>Optical and multispectral data</li> <li>Other specialized data types</li> </ul> <p></p> <p>To use a filter:</p> <ul> <li>Click on the category dropdown</li> <li>Select your desired category</li> <li>To clear a selection, click the close button (X) on the pill icon that appears</li> </ul>"},{"location":"documentation/resource-catalogue/data-discovery/#multiple-filters","title":"Multiple Filters","text":"<p>You can combine multiple filters to refine your search. For example, you might want to find:</p> <ul> <li>Optical and multispectral data with an open data license</li> <li>Commercial data with specific characteristics</li> </ul> <p></p>"},{"location":"documentation/resource-catalogue/data-discovery/#grouping-options","title":"Grouping Options","text":"<p>The \"Group by\" feature allows you to organize the displayed data into categories. You can group by:</p> <ul> <li>Provider</li> <li>Data type</li> <li>Other available categories</li> </ul> <p>Note</p> <p>You can only use one grouping option at a time, but you can combine it with multiple filters.</p> <p></p>"},{"location":"documentation/resource-catalogue/data-discovery/#searching-for-data","title":"Searching for Data","text":"<p>The search function allows you to find specific datasets using keywords:</p> <ul> <li>Click the search button</li> <li>Enter your search terms (e.g., \"Sentinel\" or \"temperature\")</li> <li>The catalogue will filter to show only matching datasets</li> </ul> <p></p>"},{"location":"documentation/resource-catalogue/data-discovery/#viewing-dataset-details","title":"Viewing Dataset Details","text":"<p>When you find a dataset of interest:</p> <p>Click on its card. You'll see detailed information including:</p> <ul> <li>Full title</li> <li>Date range</li> <li>Description</li> <li>Relevant keywords</li> </ul> <p>Watch Tutorial  </p>"},{"location":"documentation/resource-catalogue/downloading-data/","title":"Downloading data","text":"<p>Open datasets can be downloaded from the catalogue to be used locally or imported into third-party software. Commercial data can be downloaded, once an order has been placed, from the My Data area. More information on this can be found here. Following the steps below for commercial data collections will allow you to download a thumbnail of the image, at a lower resolution and quality compared to the image product, to carry out a visual assessment before purchasing the data.</p>"},{"location":"documentation/resource-catalogue/downloading-data/#find-your-scene","title":"Find your scene","text":"<p>To download open data from the Resource Catalogue, first select the scene you want to download, by browsing and filtering the left hand panel to search.</p> <p></p> <p>Click on the (i) button on the bottom right of the card. This opens up the metadata panel.</p> <p></p>"},{"location":"documentation/resource-catalogue/downloading-data/#go-to-the-assets-panel","title":"Go to the Assets panel","text":"<p>Select Assets. This tab details the files that are associated with that scene. For example, Sentinel 2 scenes contain different files such as cloud, cloud probability, or topographic shadow.</p> <p></p>"},{"location":"documentation/resource-catalogue/downloading-data/#download-the-image-file","title":"Download the image file","text":"<p>Expand the asset you are interested in by clicking on it, and select Download. You can download any of the assets for a given scene. Depending on the collection you selected, different file types may be available.</p> <p></p>"},{"location":"documentation/resource-catalogue/downloading-data/#understanding-file-types","title":"Understanding file types","text":"<p>Most open data collections contain at least one common image file type, such as GeoTIFF or NetCDF files. Some collections come as Cloud Optimised GeoTIFF (cog) files when downloaded, which are another image file type containing the data for the scene itself.</p>"},{"location":"documentation/resource-catalogue/metadata/","title":"Metadata","text":"<p>Metadata, or \"data about data,\" is crucial for understanding and effectively using the datasets in the Resource Catalogue. This chapter explains the different types of metadata available and how to access and interpret them.</p>"},{"location":"documentation/resource-catalogue/metadata/#basic-collection-metadata","title":"Basic Collection Metadata","text":"<p>When you first view a collection card, you're already seeing some basic metadata:</p> <ul> <li>Collection title</li> <li>Date range</li> <li>Description</li> <li>Keywords</li> </ul>"},{"location":"documentation/resource-catalogue/metadata/#detailed-collection-metadata","title":"Detailed Collection Metadata","text":"<p>To access more detailed metadata about a collection:</p> <ol> <li>Open the collection</li> <li>Look for the information button in the left-hand panel</li> <li>Click the information button to view comprehensive metadata</li> </ol> <p></p>"},{"location":"documentation/resource-catalogue/metadata/#types-of-collection-metadata","title":"Types of Collection Metadata","text":"<p>The detailed metadata view includes:</p> STAC ExtensionsSpatial InformationRendering InformationData ContentScientific References <ul> <li>Information about the SpatioTemporal Asset Catalog (STAC) extensions used</li> <li>STAC is the standard protocol used for storing catalog data</li> </ul> <ul> <li>Geographic coverage of the data</li> <li>Spatial extent details</li> </ul> <ul> <li>Parameters for different visualization types</li> <li>Available variables and their display options</li> </ul> <ul> <li>Fields within the data cube</li> <li>Available variables (e.g., analysed sea surface temperature, standard deviation, sea ice fraction)</li> </ul> <ul> <li>Academic citations related to the dataset</li> <li>Links to relevant research</li> </ul>"},{"location":"documentation/resource-catalogue/metadata/#additional-collection-information-tabs","title":"Additional Collection Information Tabs","text":"<p>Beyond the basic metadata view, collections include several specialised tabs:</p> AssetsProvidersAdditional ResourcesQuality Information <ul> <li>Files associated with the collection</li> <li>Includes thumbnails and data files</li> <li>Links to actual data resources</li> </ul> <ul> <li>Information about data providers</li> <li>Includes hyperlinks to provider websites</li> <li>Specifies different roles (producer, host, etc.)</li> </ul> <ul> <li>Links to related references<ul> <li>Item geode</li> <li>JSON metadata (for developers)</li> <li>License information</li> <li>Collection items</li> </ul> </li> </ul> <ul> <li>Documented product validation</li> <li>Quality assurance details</li> <li>Data quality metrics</li> </ul>"},{"location":"documentation/resource-catalogue/metadata/#item-level-metadata","title":"Item-Level Metadata","text":"<p>Each individual item (scene) within a collection has its own metadata, accessible through the information button on the item card. </p>"},{"location":"documentation/resource-catalogue/metadata/#basic-scene-information","title":"Basic Scene Information","text":"<ul> <li>Platform information (e.g., which satellite captured the data)</li> <li>Orbit details</li> <li>Cloud cover information</li> </ul>"},{"location":"documentation/resource-catalogue/metadata/#scene-assets","title":"Scene Assets","text":"<p>For satellite imagery (e.g., Sentinel 2 ARD), assets might include:</p> <ul> <li>Cloud-optimized GeoTIFFs (COGs)</li> <li>Cloud cover masks</li> <li>Cloud probability files</li> <li>Topographic shadow data</li> </ul> <p>Each asset file includes metadata about:</p> <ul> <li>Available bands</li> <li>File formats</li> <li>Usage instructions</li> </ul> <p></p>"},{"location":"documentation/resource-catalogue/metadata/#accessing-machine-readable-metadata","title":"Accessing Machine-Readable Metadata","text":"<p>For developers or advanced users:</p> <p>Look for the \"self\" link in the additional resources. This provides the JSON representation of the metadata and can be used for programmatic access or integration with other tools</p> <p></p> <p>Watch video tutorial </p>"},{"location":"documentation/resource-catalogue/quality-assessment/","title":"Quality assessment","text":"<p>Quality Assurance (QA) is a crucial aspect of working with Earth Observation data. This guide explains how to find and interpret quality assessment information in the Resource Catalogue, including both documentation quality and product validation metrics.</p>"},{"location":"documentation/resource-catalogue/quality-assessment/#finding-collections-with-qa-information","title":"Finding Collections with QA Information","text":"<p>Not all collections have QA information available. To find collections that do:</p> <ol> <li>Go to the Catalogue homepage</li> <li>Look for the \"Quality Assessment\" dropdown in the top-right filters</li> <li>Select \"Available QA\" to see collections with QA data</li> </ol> <p></p>"},{"location":"documentation/resource-catalogue/quality-assessment/#accessing-qa-information","title":"Accessing QA Information","text":"<p>To view QA information for a collection:</p> <ul> <li>Open the collection</li> <li>Click the information (i) button in the top-right</li> <li>Navigate to the QA tabs</li> </ul>"},{"location":"documentation/resource-catalogue/quality-assessment/#documentation-quality-assessment","title":"Documentation Quality Assessment","text":"<p>The \"Document QA\" tab provides an assessment of the documentation quality for the collection. This assessment:</p> <ul> <li>Is provided by the National Physics Laboratory (NPL)</li> <li>Uses a standard developed by European Space Agency (ESA)</li> <li>Rates documentation quality from \"Ideal\" to \"Basic\" or \"Not Accessible\"</li> </ul>"},{"location":"documentation/resource-catalogue/quality-assessment/#documentation-quality-levels","title":"Documentation Quality Levels","text":"<p>The documentation quality level categories are defined below for reference.</p> IdealExcellentBasicNot AccessibleNot Assessed <p>Comprehensive, well-structured documentation</p> <p>Very good documentation with minor limitations</p> <p>Minimal but sufficient documentation</p> <p>Documentation exists but is not accessible</p> <p>Documentation has not been evaluated</p>"},{"location":"documentation/resource-catalogue/quality-assessment/#accessing-documentation","title":"Accessing Documentation","text":"<p>At the bottom of the Document QA page, you'll find:</p> <ul> <li>Hyperlinks to all referenced documents</li> <li>Direct access to specific documentation (e.g., geometric processing)</li> <li>Links to related technical specifications</li> </ul> <p></p>"},{"location":"documentation/resource-catalogue/quality-assessment/#product-validation","title":"Product Validation","text":"<p>The \"Product Validation\" tab provides metrics about the actual data quality. </p>"},{"location":"documentation/resource-catalogue/quality-assessment/#radiometric-uncertainty","title":"Radiometric Uncertainty","text":"<ul> <li>Assessment of radiometric accuracy</li> <li>Based on surveys and ground truth comparisons</li> <li>Shows accuracy criteria for each band (e.g., \u00b15%)</li> </ul>"},{"location":"documentation/resource-catalogue/quality-assessment/#validation-metrics","title":"Validation Metrics","text":"<p>For each band and time period:</p> <ul> <li>Accuracy criteria</li> <li>Compliance status (Pass/Fail)</li> <li>Overall collection assessment</li> </ul>"},{"location":"documentation/resource-catalogue/quality-assessment/#understanding-the-results","title":"Understanding the Results","text":"<ul> <li>Pass. Collection meets all quality metrics</li> <li>Fail. Collection does not meet quality metrics</li> <li>Partial Pass. Some metrics pass while others fail</li> </ul> <p>Note</p> <p>Different products may have varying metric values, different requirements for different bands, or time-specific validation results</p>"},{"location":"documentation/resource-catalogue/quality-assessment/#interpreting-qa-information","title":"Interpreting QA Information","text":"<p>When using QA information, it is useful to consider the following factors when interpreting the results of the assessment. You should:</p> <ul> <li>Check both documentation and product validation</li> <li>Consider the time period of validation</li> <li>Note any partial passes or specific band limitations</li> <li>Review the underlying documentation for detailed information</li> </ul>"},{"location":"documentation/resource-catalogue/quality-assessment/#future-developments","title":"Future Developments","text":"<p>The QA system is evolving. Currently, the scope of the QA assessment on the Hub focuses on quantifying radiometric uncertainty. Additional QA metrics will be added in the future, and new validation methods may be introduced.</p> <p>Info</p> <p>Best Practices:</p> <ul> <li>Always check QA information before using data</li> <li>Consider both documentation and product validation</li> <li>Review the specific time periods covered by validation</li> <li>Consult the linked documentation for detailed information</li> <li>Be aware of any partial passes or limitations</li> </ul> <p>Watch video tutorial </p>"},{"location":"documentation/resource-catalogue/search-and-browse/","title":"Finding data","text":"<p>Once you've discovered a collection of interest, this chapter will guide you through finding specific data within that collection, including how to navigate the interface, use the map view, and manage your selected data.</p>"},{"location":"documentation/resource-catalogue/search-and-browse/#opening-a-collection","title":"Opening a Collection","text":"<p>To begin working with a specific collection:</p> <ul> <li>Navigate to the collection's card on the browse page</li> <li>Click on the card</li> <li>Select \"Open Collection\"</li> </ul>"},{"location":"documentation/resource-catalogue/search-and-browse/#the-collection-interface","title":"The Collection Interface","text":"<p>When you open a collection, you'll see:</p> <ul> <li>A map display showing data coverage areas</li> <li>A list of scenes on the left side, including:</li> <li>Scene title</li> <li>Date</li> <li>Cloud cover percentage</li> </ul> <p></p>"},{"location":"documentation/resource-catalogue/search-and-browse/#navigating-the-scene-list","title":"Navigating the Scene List","text":"<p>The most recent scenes appear first. Hover over items in the list to highlight corresponding areas on the map. Use the scroll bar or pagination controls to view more scenes. As you scroll, additional data will load on the map.</p>"},{"location":"documentation/resource-catalogue/search-and-browse/#defining-your-area-of-interest","title":"Defining Your Area of Interest","text":"<p>You can narrow down your search to a specific geographic area in two ways:</p> <ul> <li>Map Zoom<ul> <li>Zoom in on the map. The scene list will automatically update to show only data for the visible area.</li> </ul> </li> <li>Drawing a Polygon<ul> <li>Click the \"Draw\" button (polygon tool)</li> <li>Draw your area of interest on the map</li> <li>The scene list will update to show only data within your defined area</li> </ul> </li> </ul> <p></p>"},{"location":"documentation/resource-catalogue/search-and-browse/#viewing-data-on-the-map","title":"Viewing Data on the Map","text":"<p>To view a specific scene:</p> <ul> <li>Click the display icon next to a scene in the list</li> <li>The data will be added to the map</li> <li>A pinned item will appear in the list</li> </ul> <p>You can add multiple scenes to the map, and they will be listed in the pinned items section.</p> <p></p>"},{"location":"documentation/resource-catalogue/search-and-browse/#display-styles","title":"Display Styles","text":"<p>Different collections offer various display styles. For example, the CEDA ARD collection includes:</p> <ul> <li>Natural Color</li> <li>Color Infrared</li> <li>Short-wave Infrared</li> <li>Normalized Difference Vegetation Index (NDVI)</li> </ul> <p>To change the display style:</p> <ol> <li>Select your preferred style from the visualization menu</li> <li>Add scenes to the map using the new style</li> <li>You can display the same scene in different styles</li> </ol> <p></p>"},{"location":"documentation/resource-catalogue/search-and-browse/#managing-pinned-items","title":"Managing Pinned Items","text":"<p>The pinned items list shows all scenes currently displayed on the map. You can:</p> <ul> <li>Remove Items<ul> <li>Click the close (X) icon to remove a scene. The scene will disappear from both the map and the list.</li> </ul> </li> <li>Toggle Visibility<ul> <li>Use the visibility toggle to temporarily hide/show scenes. This helps when comparing different data layers.</li> </ul> </li> <li>Adjust Opacity<ul> <li>Use the opacity slider to make scenes more or less transparent. This is useful for viewing underlying base maps or comparing multiple layers.</li> </ul> </li> </ul> <p></p>"},{"location":"documentation/resource-catalogue/search-and-browse/#filtering-by-cloud-cover","title":"Filtering by Cloud Cover","text":"<p>To find scenes with specific cloud cover characteristics:</p> <ol> <li>Click the filter icon above the scene list</li> <li>Adjust the cloud cover percentage slider</li> <li>Click \"Apply\" to update the list</li> </ol> <p>This helps you find the clearest scenes for your area of interest.</p> <p></p> <p>Watch video tutorial </p>"},{"location":"documentation/resource-catalogue/visualising-data/","title":"Visualising Data","text":"<p>This guide explains how to access and use data from the Resource Catalogue directly in QGIS, taking advantage of cloud-optimized formats for efficient data access.</p> <p>Prerequisites Before starting, ensure you have:</p> <ul> <li>QGIS installed on your computer</li> <li>A scene selected in the Resource Catalogue</li> <li>The URL for the data you want to access</li> </ul>"},{"location":"documentation/resource-catalogue/visualising-data/#accessing-cloud-optimized-geotiffs-cogs","title":"Accessing Cloud-Optimized GeoTIFFs (COGs)","text":""},{"location":"documentation/resource-catalogue/visualising-data/#finding-the-data-url","title":"Finding the Data URL","text":"<ol> <li>In the Resource Catalogue, open your scene of interest</li> <li>Click the information (i) button on the item card</li> <li>Navigate to the \"Assets\" tab</li> <li>Look for the \"cog\" (Cloud-Optimized GeoTIFF) asset</li> <li>Right-click the download link and select \"Copy link address\"</li> </ol>"},{"location":"documentation/resource-catalogue/visualising-data/#adding-the-layer-to-qgis","title":"Adding the Layer to QGIS","text":"<ol> <li>Open QGIS</li> <li>Go to Layer \u2192 Add Layer \u2192 Add Raster Layer</li> <li>In the \"Source\" section, select \"Protocol\" as the source type. Paste the COG URL you copied, and click \"Add\"</li> </ol>"},{"location":"documentation/resource-catalogue/visualising-data/#configuring-the-display","title":"Configuring the Display","text":""},{"location":"documentation/resource-catalogue/visualising-data/#band-assignment","title":"Band Assignment","text":"<p>After adding the layer, you may need to adjust the band assignments for proper visualization:</p> <ol> <li>Double-click the layer in the Layers panel</li> <li>Go to the \"Symbology\" tab</li> <li>Assign the correct bands</li> <li>Click \"Apply\" to see the changes</li> </ol> <p>For Sentinel 2, we select:</p> <ul> <li>Red band \u2192 Band 4</li> <li>Green band \u2192 Band 3</li> <li>Blue band \u2192 Band 2</li> </ul> <p></p>"},{"location":"documentation/resource-catalogue/visualising-data/#additional-display-options","title":"Additional Display Options","text":"<p>You can further enhance the visualization by:</p> <ul> <li>Adjusting the stretch type</li> <li>Modifying the contrast</li> <li>Changing the opacity</li> <li>Applying different colour ramps</li> </ul>"},{"location":"documentation/resource-catalogue/visualising-data/#working-with-additional-assets","title":"Working with Additional Assets","text":"<p>Many scenes include additional assets that can be useful for analysis:</p>"},{"location":"documentation/resource-catalogue/visualising-data/#cloud-masks","title":"Cloud Masks","text":"<p>To add a cloud mask:</p> <p>1, Copy the URL for the cloud asset 2. Add it as a new raster layer in QGIS 3. The mask will show completely cloud-covered areas in black, and areas potentially affected by cloud shadow in grey</p> <p></p>"},{"location":"documentation/resource-catalogue/visualising-data/#other-available-assets","title":"Other Available Assets","text":"<p>Depending on the scene, you might find:</p> <ul> <li>Topographic shadow masks</li> <li>Valid pixel masks</li> <li>Cloud probability layers</li> <li>Other specialised masks</li> </ul>"},{"location":"documentation/resource-catalogue/visualising-data/#best-practices","title":"Best Practices","text":"Use COGs if possibleLayer ManagementPerformance Tips <ul> <li>COGs are optimized for web access</li> <li>No need to download the entire file</li> <li>Efficient for large datasets</li> </ul> <ul> <li>Organize layers in groups</li> <li>Use meaningful names</li> <li>Save layer styles for reuse</li> </ul> <ul> <li>Use appropriate zoom levels</li> <li>Consider caching settings</li> <li>Monitor memory usage with large datasets</li> </ul> <p>Tip</p> <p>If the data is not available as a COG, download the file to your local machine, add it to QGIS using the standard file import, and process it as you would any other local file</p>"},{"location":"documentation/s3-credentials/s3-credentials/","title":"Using your temporary AWS S3 credentials","text":"<p>This page explains how to use the temporary S3 credentials provided by our platform. Whether you prefer the command line or a bit of Python, we\u2019ve included examples to help you get started quickly.</p>"},{"location":"documentation/s3-credentials/s3-credentials/#generating-your-temporary-credentials","title":"Generating Your Temporary Credentials","text":"<ol> <li>Log in to your account.</li> <li>Click on the Workspaces tab at the top.</li> <li>Select the Credentials tab.</li> <li>Click on the S3 Token sub-tab.</li> <li>Finally, click the Request Temporary AWS S3 Credentials button. A popup similar to the one below will appear:</li> </ol> <pre><code>Access Key ID: e.g. ASIATNVEVXXXXX\nSecret Access Key: e.g. eeLV8XXXXX\nSession Token: e.g. IQoJb3JpZXXXXX\nExpiration: e.g. 2025-02-18 16:17:54 +0000 UTC\n</code></pre> <p>Warning</p> <p>These credentials are temporary. Make sure you generate new ones if the current credentials expire.</p>"},{"location":"documentation/s3-credentials/s3-credentials/#understanding-your-credentials","title":"Understanding Your Credentials","text":"<ul> <li>Access Key ID &amp; Secret Access Key: These work together as your username and password for accessing your S3 bucket.</li> <li>Session Token: This is an extra security token needed to verify your session.</li> <li>Expiration: This tells you when the credentials will no longer be valid.</li> </ul> <p>Remember, you\u2019ll need all three items when connecting to S3.</p>"},{"location":"documentation/s3-credentials/s3-credentials/#using-the-aws-cli","title":"Using the AWS CLI","text":"<p>If you haven\u2019t already, please install the AWS CLI. Then, set your credentials in your shell session:</p> <pre><code>export AWS_ACCESS_KEY_ID=\"ASIATNVEVXXXXX\"\nexport AWS_SECRET_ACCESS_KEY=\"eeLV8XXXXX\"\nexport AWS_SESSION_TOKEN=\"IQoJb3JpZ2luX2VjEGgaCWV1LXdlc3XXX\"\n</code></pre> <p>You can then list the contents of your S3 bucket (replace your-bucket-name with the actual bucket name):</p> <pre><code>aws s3 ls s3://bucket-name/workspace-name/\n</code></pre> <p>The bucket name depends on the environment of EODH DataHub you are using. For production, it is workspaces-eodhp.</p> <pre><code>aws s3 ls s3://workspaces-eodhp/james-workspace/\n</code></pre>"},{"location":"documentation/s3-credentials/s3-credentials/#using-the-s3cmd-tool","title":"Using the s3cmd tool","text":"<p>If you prefer using <code>s3cmd</code>, you can configure it using your temporary credentials:</p> <p>Run the configuration command:</p> <pre><code>s3cmd --configure\n</code></pre> <p>When prompted, enter your Access Key, Secret Key, and Session Token as required. To list your bucket contents, use:</p> <pre><code>s3cmd ls s3://your-bucket-name/\n</code></pre> <p>Similar to the above, if I was using the EODH Production environment and my Workspace name was james-workspace the command would be</p> <pre><code>s3cmd ls s3://workspaces-eodhp/james-workspace/\n</code></pre>"},{"location":"documentation/s3-credentials/s3-credentials/#using-your-credentials-in-python","title":"Using Your Credentials in Python","text":"<p>For those who prefer to work in Python, the boto3 library is the easiest way to interact with S3. If you don\u2019t have it installed, you can install it with:</p> <pre><code>pip install boto3\n</code></pre> <p>Here\u2019s a short script that lists the objects in your bucket:</p> <pre><code>import boto3\n# Initialise the S3 client with your temporary credentials\ns3 = boto3.client(\n    's3',\n    aws_access_key_id='ASIATNVEVXXXXX',\n    aws_secret_access_key='eeLV8XXXXX',\n    aws_session_token='IQoJb3JpZ2luX2VjEGgaCWV1LXdlc3XXX'\n)\n\nbucket_name = 'workspaces-eodhp'\n\n# List objects in the bucket\nresponse = s3.list_objects_v2(Bucket=bucket_name, Prefix=\"YOUR-WORKSPACE-NAME\") # e.g. james-workspace\n\nif 'Contents' in response:\n    for obj in response['Contents']:\n        print(obj['Key'])\nelse:\n    print(\"No objects found in the bucket.\")\n</code></pre> <p>Tip</p> <p>For more detailed instructions and advanced use cases, please visit our Help Page on S3 Credentials. This page covers additional topics like copying files between buckets and troubleshooting common issues.</p>"},{"location":"documentation/training-materials/training-materials/","title":"Training materials","text":"<p>This repository aims to provide a live set of documents to demonstrate how to use the EODH and associated tools such as the pyeodh Python API client, eoap-gen workflow generator and QGIS plugin. Other training materials may be added to this repository in future.</p> <p>Whether you\u2019re a user looking to explore the project or a developer wanting to contribute, you\u2019ll find all the information you need here.</p>"},{"location":"documentation/training-materials/training-materials/#example-jupyter-notebooks","title":"Example Jupyter notebooks","text":"<p>The following example Jupyter notebooks hosted on GitHub are a great place to get started learning the range of EO DataHub capabilities. Explore the sample code below.</p> <ul> <li>Data search in pyeodh</li> <li>Order commercial data in Python</li> <li>Visualise imagery using TiTiler</li> <li>Deploy and run a workflow on the Workflow Runner</li> <li>Transfer data between workspaces in S3</li> </ul>"},{"location":"documentation/training-materials/training-materials/#example-workflows","title":"Example workflows","text":"<p>See our public GitHub repository to explore example workflows, defined as Common Workflow Language (CWL) scripts, available from the EODH. To get started, you can deploy an example workflow to your workspace via the Workflow Runner, and try out executing it on a set of defined inputs.</p> <p>Some of the example workflows in the repository are developed by the project consortium partners. Other examples are taken from other public workflow examples, and shared here.</p> <p>Go to the Training Materials GitHub repository </p>"},{"location":"documentation/training-materials/examples/Climate_Datasets/","title":"Introducing Climate Datasets","text":"<p>Description &amp; purpose: This notebook is designed to introduce the user to model data held within the CMIP6 STAC and to suggest simple ways to visualise the information held within the NetCDF files that are accessed through the STAC asset URLs and Kerchunk reference files. It is assumed that this notebook will be run within the Notebook Service on the Hub.</p> <p>The Coupled Model Intercomparison Project Phase 6 (CMIP6) is an international collaborative effort aimed at advancing our understanding of the Earth's climate system through standardised climate model experiments. It builds on the success of earlier phases of CMIP, providing a structured framework for comparing and improving climate models developed by research institutions worldwide. CMIP6 simulations include a wide range of scenarios, such as historical climate variations, future projections based on greenhouse gas emission trajectories, and experiments designed to isolate the effects of specific climate forcings (e.g. volcanic activity or solar variations). By providing consistent protocols and publicly available datasets, CMIP6 ensures that climate models can be systematically tested, benchmarked, and improved.</p> <p>CMIP6 has a critical role in assessing the impacts of climate change and guiding global policy decisions and forms the scientific backbone for major climate assessments, such as those by the Intergovernmental Panel on Climate Change (IPCC). The data generated by CMIP6 models underpin projections of global temperature rise, sea level changes, and the frequency and intensity of extreme weather events under different socioeconomic pathways. These insights are essential for governments, industries, and communities to make informed decisions about climate mitigation and adaptation strategies. CMIP6 also helps identify uncertainties in climate modeling, providing researchers with a roadmap for future model development. The EODH facilitates interdisciplinary research by providing open-access to the CMIP6 datasets. By fostering collaboration, transparency, and innovation, the EODH platform (and the CMIP6 data accessible through it) aspires to be a cornerstone of climate science.</p> <p>The data is provided in its original NetCDF (Network Common Data Form) file format, a widely used standard for storing, sharing, and analyzing scientific data, particularly in the geosciences and climate sciences. NetCDF is designed to handle large, multi-dimensional datasets, such as temperature, precipitation, and wind speed, across space and time. Its self-describing format allows metadata (e.g. variable names, units, and coordinate information) to be embedded directly within the file, making the data easily interpretable and portable across platforms. Climate scientists use NetCDF because it is highly efficient for storing gridded data, supports parallel I/O for high-performance computing, and is compatible with a range of software tools, including Python. Additionally, the ability to represent data on multiple dimensions (e.g., latitude, longitude, time, and height) and its compression capabilities make it indispensable for managing the massive datasets generated by climate models and observational systems.</p> <p>The CEDA STAC library also contains so-called reference files in Kerchunk format. Kerchunk is a lightweight reference file format that maps chunks of data in archival formats like NetCDF in a JSON file that can be lazily loaded to access only the required data chunks for any given request. Rather than converting the source NetCDF to a different format, Kerchunk contains pointers to individual chunks of data and uses the fsspec library to assemble those chunks into the required array in a tool like Xarray. Kerchunk files are also self-describing like NetCDF and include all the attributes and metadata from the files being referenced. CEDA's STAC catalog is continuously being updated with these reference assets, so the data can be accessed without downloading NetCDF files directly, as we will show here.</p> In\u00a0[\u00a0]: Copied! <pre># Notebook set-up\n\n# Import the Python API Client\nimport pyeodh\n\n# Import data handling and visualisation packages\n# import xarray as xr\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport geoviews as gv\nimport geoviews.feature as gf\nfrom cartopy import crs\n\n# Parameterise geoviews\ngv.extension(\"bokeh\", \"matplotlib\")\n</pre> # Notebook set-up  # Import the Python API Client import pyeodh  # Import data handling and visualisation packages # import xarray as xr import matplotlib.pyplot as plt import numpy as np  import geoviews as gv import geoviews.feature as gf from cartopy import crs  # Parameterise geoviews gv.extension(\"bokeh\", \"matplotlib\") <p>Information about the dataset we are interested in can be found here.</p> In\u00a0[\u00a0]: Copied! <pre># Connect to the Hub\nclient = pyeodh.Client().get_catalog_service()\ncmip6 = client.get_catalog(\"supported-datasets/ceda-stac-catalogue\").get_collection(\n    \"cmip6\"\n)\n\ncmip6.extent.to_dict()\n\n# Look for a specific item\ncmip6.get_item(\n    \"CMIP6.ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp126.r1i1p1f1.day.pr.gn.v20210317\"\n)\n\n# Another way to do the item search and return an object\nitem_search = client.search(\n    # collections=[\"cmip6\"],\n    ids=[\n        \"CMIP6.ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp126.r1i1p1f1.day.pr.gn.v20210317\"\n    ],\n    limit=10,\n)\n</pre> # Connect to the Hub client = pyeodh.Client().get_catalog_service() cmip6 = client.get_catalog(\"supported-datasets/ceda-stac-catalogue\").get_collection(     \"cmip6\" )  cmip6.extent.to_dict()  # Look for a specific item cmip6.get_item(     \"CMIP6.ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp126.r1i1p1f1.day.pr.gn.v20210317\" )  # Another way to do the item search and return an object item_search = client.search(     # collections=[\"cmip6\"],     ids=[         \"CMIP6.ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp126.r1i1p1f1.day.pr.gn.v20210317\"     ],     limit=10, ) <p>Once the item has been searched for (the item name was taken from a visual search on the Hub web presence) then it is possible to find the data access URL. In the following example we already know the asset name but also demonstrate how to find that using dictionary keys.</p> In\u00a0[4]: Copied! <pre>for item in item_search:\n    print(item)\n</pre> for item in item_search:     print(item) <pre>&lt;pyeodh.resource_catalog.Item object at 0x7f55c8b96450&gt;\n</pre> In\u00a0[5]: Copied! <pre>item = item_search[0]\nitem\n</pre> item = item_search[0] item Out[5]: <pre>&lt;pyeodh.resource_catalog.Item at 0x7f55c8b96450&gt;</pre> In\u00a0[6]: Copied! <pre>item.to_dict()\n</pre> item.to_dict() Out[6]: <pre>{'type': 'Feature',\n 'stac_version': '1.0.0',\n 'id': 'CMIP6.ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp126.r1i1p1f1.day.pr.gn.v20210317',\n 'properties': {'datetime': '2276-01-01T00:00:00Z'},\n 'geometry': {'type': 'Polygon',\n  'coordinates': [[[-179.0625, -89.375],\n    [179.0625, -89.375],\n    [179.0625, 89.375],\n    [-179.0625, 89.375],\n    [-179.0625, -89.375]]]},\n 'links': [{'rel': 'self',\n   'href': 'https://test.eodatahub.org.uk/api/catalogue/stac/catalogs/supported-datasets/ceda-stac-catalogue/collections/cmip6/items/CMIP6.ScenarioMIP.CSIRO-ARCCSS.ACCESS-CM2.ssp126.r1i1p1f1.day.pr.gn.v20210317',\n   'type': 'application/geo+json'},\n  {'rel': 'parent',\n   'href': 'https://test.eodatahub.org.uk/api/catalogue/stac/catalogs/supported-datasets/ceda-stac-catalogue/collections/cmip6',\n   'type': 'application/json'},\n  {'rel': 'collection',\n   'href': 'https://test.eodatahub.org.uk/api/catalogue/stac/catalogs/supported-datasets/ceda-stac-catalogue/collections/cmip6',\n   'type': 'application/json'},\n  {'rel': 'root',\n   'href': 'https://test.eodatahub.org.uk/api/catalogue/stac/catalogs/supported-datasets/ceda-stac-catalogue',\n   'type': 'application/json',\n   'title': 'CEDA STAC API'}],\n 'assets': {'reference_file': {'href': 'https://dap.ceda.ac.uk/badc/cmip6/metadata/kerchunk/pipeline1/ScenarioMIP/CSIRO-ARCCSS/ACCESS-CM2/kr1.0/CMIP6_ScenarioMIP_CSIRO-ARCCSS_ACCESS-CM2_ssp126_r1i1p1f1_day_pr_gn_v20210317_kr1.0.json',\n   'type': 'application/zstd',\n   'checksum_type': None,\n   'checksum': None,\n   'open_zarr_kwargs': {'decode_times': True},\n   'size': None,\n   'roles': ['reference', 'data']},\n  'data0001': {'href': 'https://dap.ceda.ac.uk/badc/cmip6/data/CMIP6/ScenarioMIP/CSIRO-ARCCSS/ACCESS-CM2/ssp126/r1i1p1f1/day/pr/gn/v20210317/pr_day_ACCESS-CM2_ssp126_r1i1p1f1_gn_22510101-23001231.nc',\n   'type': 'application/netcdf',\n   'area': [-179.0625, -89.375, 179.0625, 89.375],\n   'checksum_type': None,\n   'checksum': None,\n   'time': '2251-01-01T00:00:00/2300-12-31T23:59:59',\n   'roles': ['data']}},\n 'bbox': [-179.0625, -89.375, 179.0625, 89.375],\n 'stac_extensions': [],\n 'collection': 'cmip6'}</pre> <p>To enable us to view the information held in an individual CMIP6 NetCDf file we are going to use Geoviews. The GeoViews Python package is a high-level library designed for interactive visualization of geospatial data, and built on top of the HoloViews library. GeoViews simplifies the creation of dynamic and rich visualisations that integrate geographic elements such as maps, satellite imagery, and geospatial datasets. It supports seamless interaction with popular geospatial libraries such as <code>cartopy</code>, GeoPandas and Shapely. GeoViews enables the overlaying of data on maps, customisation of visual styles, and interactivity such as zooming and panning.</p> <p>First we need to access the data file and open it using xarray.</p> In\u00a0[7]: Copied! <pre>product = item.get_cloud_products()\n</pre> product = item.get_cloud_products() In\u00a0[8]: Copied! <pre>data = product.open_dataset(local_only=True)\ndata\n</pre> data = product.open_dataset(local_only=True) data <pre>WARNING [ceda_datapoint.core.cloud]: Property \"mapper_kwargs\" for None is undefined.\n/home/users/dwest77/cedadev/eodh-training/science/.venv/lib/python3.11/site-packages/xarray/coding/times.py:987: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n/home/users/dwest77/cedadev/eodh-training/science/.venv/lib/python3.11/site-packages/xarray/coding/times.py:987: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n/home/users/dwest77/cedadev/eodh-training/science/.venv/lib/python3.11/site-packages/xarray/core/indexing.py:525: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n  return np.asarray(self.get_duck_array(), dtype=dtype)\n</pre> Out[8]: <pre>&lt;xarray.Dataset&gt; Size: 2GB\nDimensions:    (lat: 144, bnds: 2, lon: 192, time: 18262)\nCoordinates:\n  * lat        (lat) float64 1kB -89.38 -88.12 -86.88 ... 86.88 88.12 89.38\n  * lon        (lon) float64 2kB 0.9375 2.812 4.688 6.562 ... 355.3 357.2 359.1\n  * time       (time) object 146kB 2251-01-01 12:00:00 ... 2300-12-31 12:00:00\nDimensions without coordinates: bnds\nData variables:\n    lat_bnds   (lat, bnds) float64 2kB dask.array&lt;chunksize=(144, 2), meta=np.ndarray&gt;\n    lon_bnds   (lon, bnds) float64 3kB dask.array&lt;chunksize=(192, 2), meta=np.ndarray&gt;\n    pr         (time, lat, lon) float32 2GB dask.array&lt;chunksize=(1, 144, 192), meta=np.ndarray&gt;\n    time_bnds  (time, bnds) object 292kB dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\nAttributes: (12/47)\n    Conventions:            CF-1.7 CMIP-6.2\n    activity_id:            ScenarioMIP\n    branch_method:          standard\n    branch_time_in_child:   60265.0\n    branch_time_in_parent:  60265.0\n    cmor_version:           3.4.0\n    ...                     ...\n    table_info:             Creation Date:(30 April 2019) MD5:9328082e1e6d1da...\n    title:                  ACCESS-CM2 output prepared for CMIP6\n    tracking_id:            hdl:21.14100/d0f83bdc-dad3-41e5-9bcc-84c8c1cd099c\n    variable_id:            pr\n    variant_label:          r1i1p1f1\n    version:                v20210317</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 144</li><li>bnds: 2</li><li>lon: 192</li><li>time: 18262</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float64-89.38 -88.12 ... 88.12 89.38axis :Ybounds :lat_bndslong_name :Latitudestandard_name :latitudeunits :degrees_north<pre>array([-89.375, -88.125, -86.875, -85.625, -84.375, -83.125, -81.875, -80.625,\n       -79.375, -78.125, -76.875, -75.625, -74.375, -73.125, -71.875, -70.625,\n       -69.375, -68.125, -66.875, -65.625, -64.375, -63.125, -61.875, -60.625,\n       -59.375, -58.125, -56.875, -55.625, -54.375, -53.125, -51.875, -50.625,\n       -49.375, -48.125, -46.875, -45.625, -44.375, -43.125, -41.875, -40.625,\n       -39.375, -38.125, -36.875, -35.625, -34.375, -33.125, -31.875, -30.625,\n       -29.375, -28.125, -26.875, -25.625, -24.375, -23.125, -21.875, -20.625,\n       -19.375, -18.125, -16.875, -15.625, -14.375, -13.125, -11.875, -10.625,\n        -9.375,  -8.125,  -6.875,  -5.625,  -4.375,  -3.125,  -1.875,  -0.625,\n         0.625,   1.875,   3.125,   4.375,   5.625,   6.875,   8.125,   9.375,\n        10.625,  11.875,  13.125,  14.375,  15.625,  16.875,  18.125,  19.375,\n        20.625,  21.875,  23.125,  24.375,  25.625,  26.875,  28.125,  29.375,\n        30.625,  31.875,  33.125,  34.375,  35.625,  36.875,  38.125,  39.375,\n        40.625,  41.875,  43.125,  44.375,  45.625,  46.875,  48.125,  49.375,\n        50.625,  51.875,  53.125,  54.375,  55.625,  56.875,  58.125,  59.375,\n        60.625,  61.875,  63.125,  64.375,  65.625,  66.875,  68.125,  69.375,\n        70.625,  71.875,  73.125,  74.375,  75.625,  76.875,  78.125,  79.375,\n        80.625,  81.875,  83.125,  84.375,  85.625,  86.875,  88.125,  89.375])</pre></li><li>lon(lon)float640.9375 2.812 4.688 ... 357.2 359.1axis :Xbounds :lon_bndslong_name :Longitudestandard_name :longitudeunits :degrees_east<pre>array([  0.9375,   2.8125,   4.6875,   6.5625,   8.4375,  10.3125,  12.1875,\n        14.0625,  15.9375,  17.8125,  19.6875,  21.5625,  23.4375,  25.3125,\n        27.1875,  29.0625,  30.9375,  32.8125,  34.6875,  36.5625,  38.4375,\n        40.3125,  42.1875,  44.0625,  45.9375,  47.8125,  49.6875,  51.5625,\n        53.4375,  55.3125,  57.1875,  59.0625,  60.9375,  62.8125,  64.6875,\n        66.5625,  68.4375,  70.3125,  72.1875,  74.0625,  75.9375,  77.8125,\n        79.6875,  81.5625,  83.4375,  85.3125,  87.1875,  89.0625,  90.9375,\n        92.8125,  94.6875,  96.5625,  98.4375, 100.3125, 102.1875, 104.0625,\n       105.9375, 107.8125, 109.6875, 111.5625, 113.4375, 115.3125, 117.1875,\n       119.0625, 120.9375, 122.8125, 124.6875, 126.5625, 128.4375, 130.3125,\n       132.1875, 134.0625, 135.9375, 137.8125, 139.6875, 141.5625, 143.4375,\n       145.3125, 147.1875, 149.0625, 150.9375, 152.8125, 154.6875, 156.5625,\n       158.4375, 160.3125, 162.1875, 164.0625, 165.9375, 167.8125, 169.6875,\n       171.5625, 173.4375, 175.3125, 177.1875, 179.0625, 180.9375, 182.8125,\n       184.6875, 186.5625, 188.4375, 190.3125, 192.1875, 194.0625, 195.9375,\n       197.8125, 199.6875, 201.5625, 203.4375, 205.3125, 207.1875, 209.0625,\n       210.9375, 212.8125, 214.6875, 216.5625, 218.4375, 220.3125, 222.1875,\n       224.0625, 225.9375, 227.8125, 229.6875, 231.5625, 233.4375, 235.3125,\n       237.1875, 239.0625, 240.9375, 242.8125, 244.6875, 246.5625, 248.4375,\n       250.3125, 252.1875, 254.0625, 255.9375, 257.8125, 259.6875, 261.5625,\n       263.4375, 265.3125, 267.1875, 269.0625, 270.9375, 272.8125, 274.6875,\n       276.5625, 278.4375, 280.3125, 282.1875, 284.0625, 285.9375, 287.8125,\n       289.6875, 291.5625, 293.4375, 295.3125, 297.1875, 299.0625, 300.9375,\n       302.8125, 304.6875, 306.5625, 308.4375, 310.3125, 312.1875, 314.0625,\n       315.9375, 317.8125, 319.6875, 321.5625, 323.4375, 325.3125, 327.1875,\n       329.0625, 330.9375, 332.8125, 334.6875, 336.5625, 338.4375, 340.3125,\n       342.1875, 344.0625, 345.9375, 347.8125, 349.6875, 351.5625, 353.4375,\n       355.3125, 357.1875, 359.0625])</pre></li><li>time(time)object2251-01-01 12:00:00 ... 2300-12-...axis :Tbounds :time_bndslong_name :timestandard_name :time<pre>array([cftime.DatetimeProlepticGregorian(2251, 1, 1, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeProlepticGregorian(2251, 1, 2, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeProlepticGregorian(2251, 1, 3, 12, 0, 0, 0, has_year_zero=True),\n       ...,\n       cftime.DatetimeProlepticGregorian(2300, 12, 29, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeProlepticGregorian(2300, 12, 30, 12, 0, 0, 0, has_year_zero=True),\n       cftime.DatetimeProlepticGregorian(2300, 12, 31, 12, 0, 0, 0, has_year_zero=True)],\n      dtype=object)</pre></li></ul></li><li>Data variables: (4)<ul><li>lat_bnds(lat, bnds)float64dask.array&lt;chunksize=(144, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   2.25 kiB   2.25 kiB   Shape   (144, 2)   (144, 2)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 144 </li><li>lon_bnds(lon, bnds)float64dask.array&lt;chunksize=(192, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   3.00 kiB   3.00 kiB   Shape   (192, 2)   (192, 2)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 192 </li><li>pr(time, lat, lon)float32dask.array&lt;chunksize=(1, 144, 192), meta=np.ndarray&gt;cell_measures :area: areacellacell_methods :area: time: meancomment :includes both liquid and solid phaseslong_name :Precipitationstandard_name :precipitation_fluxunits :kg m-2 s-1  Array   Chunk   Bytes   1.88 GiB   108.00 kiB   Shape   (18262, 144, 192)   (1, 144, 192)   Dask graph   18262 chunks in 2 graph layers   Data type   float32 numpy.ndarray  192 144 18262 </li><li>time_bnds(time, bnds)objectdask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   285.34 kiB   16 B   Shape   (18262, 2)   (1, 2)   Dask graph   18262 chunks in 2 graph layers   Data type   object numpy.ndarray  2 18262 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([-89.375, -88.125, -86.875, -85.625, -84.375, -83.125, -81.875, -80.625,\n       -79.375, -78.125,\n       ...\n        78.125,  79.375,  80.625,  81.875,  83.125,  84.375,  85.625,  86.875,\n        88.125,  89.375],\n      dtype='float64', name='lat', length=144))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([  0.9375,   2.8125,   4.6875,   6.5625,   8.4375,  10.3125,  12.1875,\n        14.0625,  15.9375,  17.8125,\n       ...\n       342.1875, 344.0625, 345.9375, 347.8125, 349.6875, 351.5625, 353.4375,\n       355.3125, 357.1875, 359.0625],\n      dtype='float64', name='lon', length=192))</pre></li><li>timePandasIndex<pre>PandasIndex(CFTimeIndex([2251-01-01 12:00:00, 2251-01-02 12:00:00, 2251-01-03 12:00:00,\n             2251-01-04 12:00:00, 2251-01-05 12:00:00, 2251-01-06 12:00:00,\n             2251-01-07 12:00:00, 2251-01-08 12:00:00, 2251-01-09 12:00:00,\n             2251-01-10 12:00:00,\n             ...\n             2300-12-22 12:00:00, 2300-12-23 12:00:00, 2300-12-24 12:00:00,\n             2300-12-25 12:00:00, 2300-12-26 12:00:00, 2300-12-27 12:00:00,\n             2300-12-28 12:00:00, 2300-12-29 12:00:00, 2300-12-30 12:00:00,\n             2300-12-31 12:00:00],\n            dtype='object',\n            length=18262,\n            calendar='proleptic_gregorian',\n            freq='D'))</pre></li></ul></li><li>Attributes: (47)Conventions :CF-1.7 CMIP-6.2activity_id :ScenarioMIPbranch_method :standardbranch_time_in_child :60265.0branch_time_in_parent :60265.0cmor_version :3.4.0creation_date :2021-03-17T01:55:47Zdata_specs_version :01.00.30experiment :update of RCP2.6 based on SSP1experiment_id :ssp126external_variables :areacellaforcing_index :1frequency :dayfurther_info_url :https://furtherinfo.es-doc.org/CMIP6.CSIRO-ARCCSS.ACCESS-CM2.ssp126.none.r1i1p1f1grid :native atmosphere N96 grid (144x192 latxlon)grid_label :gnhistory :2021-03-17T01:55:47Z ; CMOR rewrote data to be consistent with CMIP6, CF-1.7 CMIP-6.2 and CF standards.initialization_index :1institution :CSIRO (Commonwealth Scientific and Industrial Research Organisation, Aspendale, Victoria 3195, Australia), ARCCSS (Australian Research Council Centre of Excellence for Climate System Science)institution_id :CSIRO-ARCCSSlicense :CMIP6 model data produced by CSIRO is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License (https://creativecommons.org/licenses/). Consult https://pcmdi.llnl.gov/CMIP6/TermsOfUse for terms of use governing CMIP6 output, including citation requirements and proper acknowledgment.  Further information about this data, including some limitations, can be found via the further_info_url (recorded as a global attribute in this file). The data producers and data providers make no warranty, either express or implied, including, but not limited to, warranties of merchantability and fitness for a particular purpose. All liabilities arising from the supply of the information (including any liability arising in negligence) are excluded to the fullest extent permitted by law.mip_era :CMIP6nominal_resolution :250 kmnotes :Exp: CM2-ssp126; Local ID: bz683; Variable: pr (['fld_s05i216'])parent_activity_id :CMIPparent_experiment_id :historicalparent_mip_era :CMIP6parent_source_id :ACCESS-CM2parent_time_units :days since 1850-01-01parent_variant_label :r1i1p1f1physics_index :1product :model-outputrealization_index :1realm :atmosrun_variant :forcing: GHG, Oz, SA, Sl, Vl, BC, OC, (GHG = CO2, N2O, CH4, CFC11, CFC12, CFC113, HCFC22, HFC125, HFC134a)source :ACCESS-CM2 (2019):  aerosol: UKCA-GLOMAP-mode atmos: MetUM-HadGEM3-GA7.1 (N96; 192 x 144 longitude/latitude; 85 levels; top level 85 km) atmosChem: none land: CABLE2.5 landIce: none ocean: ACCESS-OM2 (GFDL-MOM5, tripolar primarily 1deg; 360 x 300 longitude/latitude; 50 levels; top grid cell 0-10 m) ocnBgchem: none seaIce: CICE5.1.2 (same grid as ocean)source_id :ACCESS-CM2source_type :AOGCMsub_experiment :nonesub_experiment_id :nonetable_id :daytable_info :Creation Date:(30 April 2019) MD5:9328082e1e6d1da0a63ed585f05cc105title :ACCESS-CM2 output prepared for CMIP6tracking_id :hdl:21.14100/d0f83bdc-dad3-41e5-9bcc-84c8c1cd099cvariable_id :prvariant_label :r1i1p1f1version :v20210317</li></ul> In\u00a0[9]: Copied! <pre>f'{144*192*18262*32/1000000000:.2f} GB'\n</pre> f'{144*192*18262*32/1000000000:.2f} GB' Out[9]: <pre>'16.16 GB'</pre> <p>We can see from the above, taking the precipitation flux (pr) variable as an example, that the data represented by this Dataset is well over 16 GB uncompressed. Thus in order to access any of the data without using Kerchunk, we would need to download at least 10 GB just for this one dataset. Clearly there is a better way, especially since it is unlikely we will need to access the entire array for any of our analyses.</p> <p>In the example below we take only the first time step, which most likely relates to just the first chunk of data using the Kerchunk file.</p> In\u00a0[10]: Copied! <pre>%matplotlib inline\n# select the first time step\noneframe = data.pr.isel(time=0)\n\n# generate the image to show\nfigdata = gv.Dataset(oneframe, [\"lon\", \"lat\"], crs=crs.PlateCarree())\nimages = figdata.to(gv.Image)\n\n# Plot\nimages.opts(cmap=\"viridis\", colorbar=True, width=600, height=500) * gf.coastline\n</pre> %matplotlib inline # select the first time step oneframe = data.pr.isel(time=0)  # generate the image to show figdata = gv.Dataset(oneframe, [\"lon\", \"lat\"], crs=crs.PlateCarree()) images = figdata.to(gv.Image)  # Plot images.opts(cmap=\"viridis\", colorbar=True, width=600, height=500) * gf.coastline Out[10]: In\u00a0[11]: Copied! <pre># Another way to do the item search and return an object\nitem_search = client.search(\n    # collections=[\"cmip6\"],\n    ids=[\n        \"CMIP6.ScenarioMIP.MRI.MRI-ESM2-0.ssp119.r1i1p1f1.Amon.clt.gn.v20190222\"\n    ],\n    limit=10,\n)\nitem=item_search[0]\n</pre> # Another way to do the item search and return an object item_search = client.search(     # collections=[\"cmip6\"],     ids=[         \"CMIP6.ScenarioMIP.MRI.MRI-ESM2-0.ssp119.r1i1p1f1.Amon.clt.gn.v20190222\"     ],     limit=10, ) item=item_search[0] <p>We can very quickly retrieve the cloud-format product to use for accessing the data.</p> In\u00a0[12]: Copied! <pre>product = item.get_cloud_products()\nproduct\n</pre> product = item.get_cloud_products() product Out[12]: <pre>&lt;DataPointCloudProduct: CMIP6.ScenarioMIP.MRI.MRI-ESM2-0.ssp119.r1i1p1f1.Amon.clt.gn.v20190222-reference_file (Format: kerchunk)&gt;\n - bbox: [-180.0, -89.14152, 178.875, 89.14152]\n - asset_id: CMIP6.ScenarioMIP.MRI.MRI-ESM2-0.ssp119.r1i1p1f1.Amon.clt.gn.v20190222-reference_file\n - cloud_format: kerchunk\nAttributes:\n - datetime: 2057-12-31T12:00:00+00:00</pre> In\u00a0[13]: Copied! <pre>ds = product.open_dataset()\nds\n</pre> ds = product.open_dataset() ds <pre>WARNING [ceda_datapoint.core.cloud]: Property \"mapper_kwargs\" for None is undefined.\n</pre> Out[13]: <pre>&lt;xarray.Dataset&gt; Size: 211MB\nDimensions:    (time: 1032, lat: 160, lon: 320, bnds: 2)\nCoordinates:\n  * lat        (lat) float64 1kB -89.14 -88.03 -86.91 ... 86.91 88.03 89.14\n  * lon        (lon) float64 3kB 0.0 1.125 2.25 3.375 ... 356.6 357.8 358.9\n  * time       (time) datetime64[ns] 8kB 2015-01-16T12:00:00 ... 2100-12-16T1...\nDimensions without coordinates: bnds\nData variables:\n    clt        (time, lat, lon) float32 211MB dask.array&lt;chunksize=(1, 160, 320), meta=np.ndarray&gt;\n    lat_bnds   (lat, bnds) float64 3kB dask.array&lt;chunksize=(160, 2), meta=np.ndarray&gt;\n    lon_bnds   (lon, bnds) float64 5kB dask.array&lt;chunksize=(320, 2), meta=np.ndarray&gt;\n    time_bnds  (time, bnds) datetime64[ns] 17kB dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\nAttributes: (12/44)\n    Conventions:            CF-1.7 CMIP-6.2\n    activity_id:            ScenarioMIP\n    branch_method:          standard\n    branch_time_in_child:   60265.0\n    branch_time_in_parent:  60265.0\n    cmor_version:           3.4.0\n    ...                     ...\n    table_id:               Amon\n    table_info:             Creation Date:(14 December 2018) MD5:b2d32d1a0d9b...\n    title:                  MRI-ESM2-0 output prepared for CMIP6\n    tracking_id:            hdl:21.14100/3fef816c-0095-4950-a45a-ba080ec2b7eb\n    variable_id:            clt\n    variant_label:          r1i1p1f1</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 1032</li><li>lat: 160</li><li>lon: 320</li><li>bnds: 2</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float64-89.14 -88.03 ... 88.03 89.14axis :Ybounds :lat_bndslong_name :Latitudestandard_name :latitudeunits :degrees_north<pre>array([-89.14152, -88.02943, -86.91077, -85.79063, -84.66992, -83.54895,\n       -82.42782, -81.30659, -80.18531, -79.06398, -77.94262, -76.82124,\n       -75.69984, -74.57843, -73.45701, -72.33558, -71.21414, -70.09269,\n       -68.97124, -67.84978, -66.72833, -65.60686, -64.4854 , -63.36393,\n       -62.24246, -61.12099, -59.99952, -58.87804, -57.75657, -56.63509,\n       -55.51361, -54.39214, -53.27066, -52.14917, -51.02769, -49.90621,\n       -48.78473, -47.66325, -46.54176, -45.42028, -44.29879, -43.17731,\n       -42.05582, -40.93434, -39.81285, -38.69137, -37.56988, -36.44839,\n       -35.32691, -34.20542, -33.08393, -31.96244, -30.84096, -29.71947,\n       -28.59798, -27.47649, -26.355  , -25.23351, -24.11203, -22.99054,\n       -21.86905, -20.74756, -19.62607, -18.50458, -17.38309, -16.2616 ,\n       -15.14011, -14.01862, -12.89713, -11.77564, -10.65415,  -9.53266,\n        -8.41117,  -7.28968,  -6.16819,  -5.0467 ,  -3.92521,  -2.80372,\n        -1.68223,  -0.56074,   0.56074,   1.68223,   2.80372,   3.92521,\n         5.0467 ,   6.16819,   7.28968,   8.41117,   9.53266,  10.65415,\n        11.77564,  12.89713,  14.01862,  15.14011,  16.2616 ,  17.38309,\n        18.50458,  19.62607,  20.74756,  21.86905,  22.99054,  24.11203,\n        25.23351,  26.355  ,  27.47649,  28.59798,  29.71947,  30.84096,\n        31.96244,  33.08393,  34.20542,  35.32691,  36.44839,  37.56988,\n        38.69137,  39.81285,  40.93434,  42.05582,  43.17731,  44.29879,\n        45.42028,  46.54176,  47.66325,  48.78473,  49.90621,  51.02769,\n        52.14917,  53.27066,  54.39214,  55.51361,  56.63509,  57.75657,\n        58.87804,  59.99952,  61.12099,  62.24246,  63.36393,  64.4854 ,\n        65.60686,  66.72833,  67.84978,  68.97124,  70.09269,  71.21414,\n        72.33558,  73.45701,  74.57843,  75.69984,  76.82124,  77.94262,\n        79.06398,  80.18531,  81.30659,  82.42782,  83.54895,  84.66992,\n        85.79063,  86.91077,  88.02943,  89.14152])</pre></li><li>lon(lon)float640.0 1.125 2.25 ... 357.8 358.9axis :Xbounds :lon_bndslong_name :Longitudestandard_name :longitudeunits :degrees_east<pre>array([  0.   ,   1.125,   2.25 , ..., 356.625, 357.75 , 358.875])</pre></li><li>time(time)datetime64[ns]2015-01-16T12:00:00 ... 2100-12-...axis :Tbounds :time_bndslong_name :timestandard_name :time<pre>array(['2015-01-16T12:00:00.000000000', '2015-02-15T00:00:00.000000000',\n       '2015-03-16T12:00:00.000000000', ..., '2100-10-16T12:00:00.000000000',\n       '2100-11-16T00:00:00.000000000', '2100-12-16T12:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (4)<ul><li>clt(time, lat, lon)float32dask.array&lt;chunksize=(1, 160, 320), meta=np.ndarray&gt;cell_measures :area: areacellacell_methods :area: time: meancomment :Total cloud area fraction for the whole atmospheric column, as seen from the surface or the top of the atmosphere. Includes both large-scale and convective cloud.history :2019-02-20T14:25:47Z altered by CMOR: replaced missing value flag (-9.99e+33) with standard missing value (1e+20).long_name :Total Cloud Cover Percentageoriginal_name :TCLOUDstandard_name :cloud_area_fractionunits :%  Array   Chunk   Bytes   201.56 MiB   200.00 kiB   Shape   (1032, 160, 320)   (1, 160, 320)   Dask graph   1032 chunks in 2 graph layers   Data type   float32 numpy.ndarray  320 160 1032 </li><li>lat_bnds(lat, bnds)float64dask.array&lt;chunksize=(160, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   2.50 kiB   2.50 kiB   Shape   (160, 2)   (160, 2)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 160 </li><li>lon_bnds(lon, bnds)float64dask.array&lt;chunksize=(320, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   5.00 kiB   5.00 kiB   Shape   (320, 2)   (320, 2)   Dask graph   1 chunks in 2 graph layers   Data type   float64 numpy.ndarray  2 320 </li><li>time_bnds(time, bnds)datetime64[ns]dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;  Array   Chunk   Bytes   16.12 kiB   16 B   Shape   (1032, 2)   (1, 2)   Dask graph   1032 chunks in 2 graph layers   Data type   datetime64[ns] numpy.ndarray  2 1032 </li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([-89.14152, -88.02943, -86.91077, -85.79063, -84.66992, -83.54895,\n       -82.42782, -81.30659, -80.18531, -79.06398,\n       ...\n        79.06398,  80.18531,  81.30659,  82.42782,  83.54895,  84.66992,\n        85.79063,  86.91077,  88.02943,  89.14152],\n      dtype='float64', name='lat', length=160))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([    0.0,   1.125,    2.25,   3.375,     4.5,   5.625,    6.75,   7.875,\n           9.0,  10.125,\n       ...\n        348.75, 349.875,   351.0, 352.125,  353.25, 354.375,   355.5, 356.625,\n        357.75, 358.875],\n      dtype='float64', name='lon', length=320))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2015-01-16 12:00:00', '2015-02-15 00:00:00',\n               '2015-03-16 12:00:00', '2015-04-16 00:00:00',\n               '2015-05-16 12:00:00', '2015-06-16 00:00:00',\n               '2015-07-16 12:00:00', '2015-08-16 12:00:00',\n               '2015-09-16 00:00:00', '2015-10-16 12:00:00',\n               ...\n               '2100-03-16 12:00:00', '2100-04-16 00:00:00',\n               '2100-05-16 12:00:00', '2100-06-16 00:00:00',\n               '2100-07-16 12:00:00', '2100-08-16 12:00:00',\n               '2100-09-16 00:00:00', '2100-10-16 12:00:00',\n               '2100-11-16 00:00:00', '2100-12-16 12:00:00'],\n              dtype='datetime64[ns]', name='time', length=1032, freq=None))</pre></li></ul></li><li>Attributes: (44)Conventions :CF-1.7 CMIP-6.2activity_id :ScenarioMIPbranch_method :standardbranch_time_in_child :60265.0branch_time_in_parent :60265.0cmor_version :3.4.0creation_date :2019-02-20T14:25:47Zdata_specs_version :01.00.29experiment :low-end scenario reaching 1.9 W m-2, based on SSP1experiment_id :ssp119external_variables :areacellaforcing_index :1frequency :monfurther_info_url :https://furtherinfo.es-doc.org/CMIP6.MRI.MRI-ESM2-0.ssp119.none.r1i1p1f1grid :native atmosphere TL159 gaussian grid (160x320 latxlon)grid_label :gnhistory :2019-02-20T14:25:47Z ; CMOR rewrote data to be consistent with CMIP6, CF-1.7 CMIP-6.2 and CF standards.; Output from run-Dr064_ssp119_145 (sfc_avr_mon.ctl)initialization_index :1institution :Meteorological Research Institute, Tsukuba, Ibaraki 305-0052, Japaninstitution_id :MRIlicense :CMIP6 model data produced by MRI is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License (https://creativecommons.org/licenses/). Consult https://pcmdi.llnl.gov/CMIP6/TermsOfUse for terms of use governing CMIP6 output, including citation requirements and proper acknowledgment. Further information about this data, including some limitations, can be found via the further_info_url (recorded as a global attribute in this file). The data producers and data providers make no warranty, either express or implied, including, but not limited to, warranties of merchantability and fitness for a particular purpose. All liabilities arising from the supply of the information (including any liability arising in negligence) are excluded to the fullest extent permitted by law.mip_era :CMIP6nominal_resolution :100 kmparent_activity_id :CMIPparent_experiment_id :historicalparent_mip_era :CMIP6parent_source_id :MRI-ESM2-0parent_time_units :days since 1850-01-01parent_variant_label :r1i1p1f1physics_index :1product :model-outputrealization_index :1realm :atmossource :MRI-ESM2.0 (2017):  aerosol: MASINGAR mk2r4 (TL95; 192 x 96 longitude/latitude; 80 levels; top level 0.01 hPa) atmos: MRI-AGCM3.5 (TL159; 320 x 160 longitude/latitude; 80 levels; top level 0.01 hPa) atmosChem: MRI-CCM2.1 (T42; 128 x 64 longitude/latitude; 80 levels; top level 0.01 hPa) land: HAL 1.0 landIce: none ocean: MRI.COM4.4 (tripolar primarily 0.5 deg latitude/1 deg longitude with meridional refinement down to 0.3 deg within 10 degrees north and south of the equator; 360 x 364 longitude/latitude; 61 levels; top grid cell 0-2 m) ocnBgchem: MRI.COM4.4 seaIce: MRI.COM4.4source_id :MRI-ESM2-0source_type :AOGCM AER CHEMsub_experiment :nonesub_experiment_id :nonetable_id :Amontable_info :Creation Date:(14 December 2018) MD5:b2d32d1a0d9b196411429c8895329d8ftitle :MRI-ESM2-0 output prepared for CMIP6tracking_id :hdl:21.14100/3fef816c-0095-4950-a45a-ba080ec2b7ebvariable_id :cltvariant_label :r1i1p1f1</li></ul> <p>Applying our data slices and analyses can be performed in a few short steps that takes very little time thanks to the lazily loaded dataset.</p> In\u00a0[14]: Copied! <pre>ds.clt\n</pre> ds.clt Out[14]: <pre>&lt;xarray.DataArray 'clt' (time: 1032, lat: 160, lon: 320)&gt; Size: 211MB\ndask.array&lt;open_dataset-clt, shape=(1032, 160, 320), dtype=float32, chunksize=(1, 160, 320), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * lat      (lat) float64 1kB -89.14 -88.03 -86.91 -85.79 ... 86.91 88.03 89.14\n  * lon      (lon) float64 3kB 0.0 1.125 2.25 3.375 ... 355.5 356.6 357.8 358.9\n  * time     (time) datetime64[ns] 8kB 2015-01-16T12:00:00 ... 2100-12-16T12:...\nAttributes:\n    cell_measures:  area: areacella\n    cell_methods:   area: time: mean\n    comment:        Total cloud area fraction for the whole atmospheric colum...\n    history:        2019-02-20T14:25:47Z altered by CMOR: replaced missing va...\n    long_name:      Total Cloud Cover Percentage\n    original_name:  TCLOUD\n    standard_name:  cloud_area_fraction\n    units:          %</pre>xarray.DataArray'clt'<ul><li>time: 1032</li><li>lat: 160</li><li>lon: 320</li></ul><ul><li>dask.array&lt;chunksize=(1, 160, 320), meta=np.ndarray&gt;  Array   Chunk   Bytes   201.56 MiB   200.00 kiB   Shape   (1032, 160, 320)   (1, 160, 320)   Dask graph   1032 chunks in 2 graph layers   Data type   float32 numpy.ndarray  320 160 1032 </li><li>Coordinates: (3)<ul><li>lat(lat)float64-89.14 -88.03 ... 88.03 89.14axis :Ybounds :lat_bndslong_name :Latitudestandard_name :latitudeunits :degrees_north<pre>array([-89.14152, -88.02943, -86.91077, -85.79063, -84.66992, -83.54895,\n       -82.42782, -81.30659, -80.18531, -79.06398, -77.94262, -76.82124,\n       -75.69984, -74.57843, -73.45701, -72.33558, -71.21414, -70.09269,\n       -68.97124, -67.84978, -66.72833, -65.60686, -64.4854 , -63.36393,\n       -62.24246, -61.12099, -59.99952, -58.87804, -57.75657, -56.63509,\n       -55.51361, -54.39214, -53.27066, -52.14917, -51.02769, -49.90621,\n       -48.78473, -47.66325, -46.54176, -45.42028, -44.29879, -43.17731,\n       -42.05582, -40.93434, -39.81285, -38.69137, -37.56988, -36.44839,\n       -35.32691, -34.20542, -33.08393, -31.96244, -30.84096, -29.71947,\n       -28.59798, -27.47649, -26.355  , -25.23351, -24.11203, -22.99054,\n       -21.86905, -20.74756, -19.62607, -18.50458, -17.38309, -16.2616 ,\n       -15.14011, -14.01862, -12.89713, -11.77564, -10.65415,  -9.53266,\n        -8.41117,  -7.28968,  -6.16819,  -5.0467 ,  -3.92521,  -2.80372,\n        -1.68223,  -0.56074,   0.56074,   1.68223,   2.80372,   3.92521,\n         5.0467 ,   6.16819,   7.28968,   8.41117,   9.53266,  10.65415,\n        11.77564,  12.89713,  14.01862,  15.14011,  16.2616 ,  17.38309,\n        18.50458,  19.62607,  20.74756,  21.86905,  22.99054,  24.11203,\n        25.23351,  26.355  ,  27.47649,  28.59798,  29.71947,  30.84096,\n        31.96244,  33.08393,  34.20542,  35.32691,  36.44839,  37.56988,\n        38.69137,  39.81285,  40.93434,  42.05582,  43.17731,  44.29879,\n        45.42028,  46.54176,  47.66325,  48.78473,  49.90621,  51.02769,\n        52.14917,  53.27066,  54.39214,  55.51361,  56.63509,  57.75657,\n        58.87804,  59.99952,  61.12099,  62.24246,  63.36393,  64.4854 ,\n        65.60686,  66.72833,  67.84978,  68.97124,  70.09269,  71.21414,\n        72.33558,  73.45701,  74.57843,  75.69984,  76.82124,  77.94262,\n        79.06398,  80.18531,  81.30659,  82.42782,  83.54895,  84.66992,\n        85.79063,  86.91077,  88.02943,  89.14152])</pre></li><li>lon(lon)float640.0 1.125 2.25 ... 357.8 358.9axis :Xbounds :lon_bndslong_name :Longitudestandard_name :longitudeunits :degrees_east<pre>array([  0.   ,   1.125,   2.25 , ..., 356.625, 357.75 , 358.875])</pre></li><li>time(time)datetime64[ns]2015-01-16T12:00:00 ... 2100-12-...axis :Tbounds :time_bndslong_name :timestandard_name :time<pre>array(['2015-01-16T12:00:00.000000000', '2015-02-15T00:00:00.000000000',\n       '2015-03-16T12:00:00.000000000', ..., '2100-10-16T12:00:00.000000000',\n       '2100-11-16T00:00:00.000000000', '2100-12-16T12:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([-89.14152, -88.02943, -86.91077, -85.79063, -84.66992, -83.54895,\n       -82.42782, -81.30659, -80.18531, -79.06398,\n       ...\n        79.06398,  80.18531,  81.30659,  82.42782,  83.54895,  84.66992,\n        85.79063,  86.91077,  88.02943,  89.14152],\n      dtype='float64', name='lat', length=160))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([    0.0,   1.125,    2.25,   3.375,     4.5,   5.625,    6.75,   7.875,\n           9.0,  10.125,\n       ...\n        348.75, 349.875,   351.0, 352.125,  353.25, 354.375,   355.5, 356.625,\n        357.75, 358.875],\n      dtype='float64', name='lon', length=320))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2015-01-16 12:00:00', '2015-02-15 00:00:00',\n               '2015-03-16 12:00:00', '2015-04-16 00:00:00',\n               '2015-05-16 12:00:00', '2015-06-16 00:00:00',\n               '2015-07-16 12:00:00', '2015-08-16 12:00:00',\n               '2015-09-16 00:00:00', '2015-10-16 12:00:00',\n               ...\n               '2100-03-16 12:00:00', '2100-04-16 00:00:00',\n               '2100-05-16 12:00:00', '2100-06-16 00:00:00',\n               '2100-07-16 12:00:00', '2100-08-16 12:00:00',\n               '2100-09-16 00:00:00', '2100-10-16 12:00:00',\n               '2100-11-16 00:00:00', '2100-12-16 12:00:00'],\n              dtype='datetime64[ns]', name='time', length=1032, freq=None))</pre></li></ul></li><li>Attributes: (8)cell_measures :area: areacellacell_methods :area: time: meancomment :Total cloud area fraction for the whole atmospheric column, as seen from the surface or the top of the atmosphere. Includes both large-scale and convective cloud.history :2019-02-20T14:25:47Z altered by CMOR: replaced missing value flag (-9.99e+33) with standard missing value (1e+20).long_name :Total Cloud Cover Percentageoriginal_name :TCLOUDstandard_name :cloud_area_fractionunits :%</li></ul> In\u00a0[15]: Copied! <pre>%%time\ntime_series = ds.time.dt.year.isin([2015+i for i in range(5)])\noneframe = ds.clt.isel(time=time_series).sel(lon=slice(220,340), lat=slice(-60,60)).mean(dim='time')\n</pre> %%time time_series = ds.time.dt.year.isin([2015+i for i in range(5)]) oneframe = ds.clt.isel(time=time_series).sel(lon=slice(220,340), lat=slice(-60,60)).mean(dim='time') <pre>CPU times: user 15.4 ms, sys: 2.15 ms, total: 17.5 ms\nWall time: 18.8 ms\n</pre> <p>Basic plot of the Americas - average cloud cover percentage over a 5 year period (2015-2020)</p> In\u00a0[16]: Copied! <pre># Generate a plain image to show\nplt.imshow(oneframe.compute(), origin='lower',cmap='Blues') # We need the 'lower' option to flip the image because the latitude coordinates for this dataset start from -90.\nplt.colorbar(label=f'{ds.clt.long_name} ({ds.clt.units})')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\n</pre> # Generate a plain image to show plt.imshow(oneframe.compute(), origin='lower',cmap='Blues') # We need the 'lower' option to flip the image because the latitude coordinates for this dataset start from -90. plt.colorbar(label=f'{ds.clt.long_name} ({ds.clt.units})') plt.xlabel('Longitude') plt.ylabel('Latitude') Out[16]: <pre>Text(0, 0.5, 'Latitude')</pre> <p>The above image well represents the underlying data in terms of arrangement and resolution, and we can even see the outlines of continents where the land-sea border has an effect on the cloud cover percentage (clt).</p> <p>If we want to have the coastlines added, plus a different projection method for the map, we can use the Geoviews package to arrange the data in this new way.</p> <p>Note: Using different projections may add some apparent artifacts to the data, as we can see below. This is probably due to the data conversion to Geoviews and is a good example of why using multiple plots is a valuable filter for unexpected results.</p> In\u00a0[17]: Copied! <pre># generate the image to show\nfigdata = gv.Dataset(oneframe.compute(), [\"lon\", \"lat\"], crs=crs.PlateCarree())\nimages = figdata.to(gv.Image)\n\n# Plot\nimages.opts(cmap=\"viridis\", colorbar=True, width=600, height=500) * gf.coastline\n</pre> # generate the image to show figdata = gv.Dataset(oneframe.compute(), [\"lon\", \"lat\"], crs=crs.PlateCarree()) images = figdata.to(gv.Image)  # Plot images.opts(cmap=\"viridis\", colorbar=True, width=600, height=500) * gf.coastline Out[17]: In\u00a0[18]: Copied! <pre># Define our filter based on the desired months.\nsummer = data.time.dt.month.isin([6,7,8])\n\nyears  = []\nvalues = []\nfor i in range(60): # Arbitrary 60-year time-series.\n\n    # Define a filter for each year and use to calculate the mean value.\n    yr_filter = data.time.dt.year.isin([2251+i])\n    value = data.pr.isel(time=summer&amp;yr_filter).mean().compute()\n\n    # Concatenate values\n    values.append(value)\n    years.append(2251+i)\n</pre> # Define our filter based on the desired months. summer = data.time.dt.month.isin([6,7,8])  years  = [] values = [] for i in range(60): # Arbitrary 60-year time-series.      # Define a filter for each year and use to calculate the mean value.     yr_filter = data.time.dt.year.isin([2251+i])     value = data.pr.isel(time=summer&amp;yr_filter).mean().compute()      # Concatenate values     values.append(value)     years.append(2251+i) <p>Note: When plotting data in this fashion, it is best to retrieve all the data you're definitely going to need, using the <code>.compute()</code> function. This should only be done at the point where all the data will be required so you can take advantage of the lazy loading as far as possible, but by fetching the data slightly sooner, you are avoiding the potential for duplicate retrievals. In our example above, if the data chunks each contain multiple years, then our data requests in each loop will likely try to fetch the same data multiple times over the network.</p> <p>We can already guess the expected results but we can see it clearly from the <code>values</code> parameter: There isn't a lot of variation in Precipitation for this set of time periods expected within this data, the average is fairly constant.</p> In\u00a0[19]: Copied! <pre>np.array(values)\n</pre> np.array(values) Out[19]: <pre>array([3.2855816e-05, 3.2685162e-05, 3.3101434e-05, 3.2580436e-05,\n       3.3071603e-05, 3.2749525e-05, 3.2923213e-05, 3.2849730e-05,\n       3.2942939e-05, 3.2836724e-05, 3.2885007e-05, 3.3062988e-05,\n       3.2747223e-05, 3.2915803e-05, 3.2871350e-05, 3.2934004e-05,\n       3.2730630e-05, 3.2895732e-05, 3.2411441e-05, 3.2801654e-05,\n       3.2587293e-05, 3.3185788e-05, 3.2788066e-05, 3.3151795e-05,\n       3.3164430e-05, 3.2921696e-05, 3.3123262e-05, 3.2765842e-05,\n       3.2568005e-05, 3.2715379e-05, 3.3246262e-05, 3.2860520e-05,\n       3.2898555e-05, 3.3144406e-05, 3.2595581e-05, 3.2835953e-05,\n       3.2745345e-05, 3.2944168e-05, 3.2687018e-05, 3.2905038e-05,\n       3.2768705e-05, 3.2982960e-05, 3.2923665e-05, 3.2810131e-05,\n       3.2835127e-05, 3.3216886e-05, 3.2803429e-05, 3.2939348e-05,\n       3.2507061e-05, 3.2745895e-05,           nan,           nan,\n                 nan,           nan,           nan,           nan,\n                 nan,           nan,           nan,           nan],\n      dtype=float32)</pre> <p>Finally we can plot this data as a simple scatter plot.</p> In\u00a0[20]: Copied! <pre>%matplotlib inline\n\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.ylim(0,4e-5)\nplt.ylabel(f'{data.pr.long_name} JJA Avg ({data.pr.units})')\nplt.xlabel('Years')\nplt.scatter(np.array(years),np.array(values))\n</pre> %matplotlib inline  import matplotlib import numpy as np import matplotlib.pyplot as plt  plt.ylim(0,4e-5) plt.ylabel(f'{data.pr.long_name} JJA Avg ({data.pr.units})') plt.xlabel('Years') plt.scatter(np.array(years),np.array(values)) Out[20]: <pre>&lt;matplotlib.collections.PathCollection at 0x7f5540cf7d50&gt;</pre> In\u00a0[21]: Copied! <pre>%%time\n# Load the data selection in Xarray\ndata2 = data.isel(time=slice(0, 50)).sel(lat=slice(-30,30)).compute()\n</pre> %%time # Load the data selection in Xarray data2 = data.isel(time=slice(0, 50)).sel(lat=slice(-30,30)).compute() <pre>CPU times: user 552 ms, sys: 89.7 ms, total: 642 ms\nWall time: 499 ms\n</pre> <p>Note: It is best to load the data into geoviews at this point (losing the lazy loading) because the output generation takes considerably longer than the data loading, and this causes more frequent connection issues when awaiting http-requested chunks.</p> <p>In this case, because we are using a Cloud-based dataset which retrieves data by request, it is best to load our data into an array at this stage, rather than pass the unloaded dataset to Geoviews. This notebook has been written in this way because it was noted that Geoviews is more prone to request-based failures to fetch data, most likely because in converting the data it attempts to access data selections multiple times, which triggers multiple requests for the same portion of data.</p> In\u00a0[22]: Copied! <pre>%%time\n# Create a Geoviews dataset and convert to the required format for visualisation\ndataset = gv.Dataset(data2)\nensemble = dataset.to(gv.Image, [\"lon\", \"lat\"], \"pr\")\n</pre> %%time # Create a Geoviews dataset and convert to the required format for visualisation dataset = gv.Dataset(data2) ensemble = dataset.to(gv.Image, [\"lon\", \"lat\"], \"pr\") <pre>CPU times: user 221 ms, sys: 2.56 ms, total: 223 ms\nWall time: 222 ms\n</pre> <p>We can see the above two sections - where we have retrieved the data and assembled a Geoviews dataset - have short durations in terms of computation time. The cell below generates the interactive animation, which even for 50 selections of pre-loaded data takes significantly longer to assemble.</p> In\u00a0[23]: Copied! <pre>%%time\n# Generate the interactive annimation\ngv.output(\n    ensemble.opts(cmap=\"viridis\", colorbar=True, fig_size=120, backend=\"matplotlib\")\n    * gf.coastline(),\n    backend=\"matplotlib\",\n    widget_location=\"top\",\n    max_frames=200,\n)\n</pre> %%time # Generate the interactive annimation gv.output(     ensemble.opts(cmap=\"viridis\", colorbar=True, fig_size=120, backend=\"matplotlib\")     * gf.coastline(),     backend=\"matplotlib\",     widget_location=\"top\",     max_frames=200, ) <pre>CPU times: user 3min 2s, sys: 53.1 s, total: 3min 55s\nWall time: 2min 30s\n</pre> <p>Use the time slider above the figure to step through the spatial dataset held within this dataset. The Geoviews package and Holoviews suite of tools are very powerful when building interactive components to help scientists and users understand these complex datasets. If the colormap doesn't work for you alternatibves can be found here.</p> In\u00a0[24]: Copied! <pre># # Close the dataset to free resources\ndata.close()\n</pre> # # Close the dataset to free resources data.close()"},{"location":"documentation/training-materials/examples/Climate_Datasets/#introducing-climate-datasets","title":"Introducing Climate Datasets\u00b6","text":""},{"location":"documentation/training-materials/examples/Climate_Datasets/#introduction","title":"Introduction\u00b6","text":""},{"location":"documentation/training-materials/examples/Climate_Datasets/#finding-data","title":"Finding data\u00b6","text":""},{"location":"documentation/training-materials/examples/Climate_Datasets/#visualising-data","title":"Visualising data\u00b6","text":""},{"location":"documentation/training-materials/examples/Climate_Datasets/#1-plot-a-single-time-step","title":"1. Plot a single time-step\u00b6","text":"<p>From the above information (and the file name) we can see that the dataset contained in the NetCDF file is precipitation flux (pr). There are a large number of timeperiods that have been included in the file. We can plot a single slice of the data using GeoViews as laid out in the following code.</p>"},{"location":"documentation/training-materials/examples/Climate_Datasets/#2-plot-a-regional-average","title":"2. Plot a regional average\u00b6","text":"<p>We can perform multiple selections and operations on the data lazily, where those operations are not computed until the data is plotted at the end. Here we take an example of selecting some cloud height data and plotting the average over a period of 5 years.</p>"},{"location":"documentation/training-materials/examples/Climate_Datasets/#3-plot-some-assembled-data","title":"3. Plot some assembled data\u00b6","text":"<p>Here we plot an example set of data for the years 2251 to 2311 within this model forecase for the Precipitation Flux (pr) variable. We are specifically looking at the mean rainfall in the summer months June-August across the globe. We can select these data from the Xarray dataset, combine them into an array of mean values and plot</p>"},{"location":"documentation/training-materials/examples/Climate_Datasets/#4-animations-across-a-time-series","title":"4. Animations across a time-series\u00b6","text":"<p>It would be useful if a user was able to move through the data and understand the changes in the supplied data as the time periods change. This is where Geoviews really helps.</p>"},{"location":"documentation/training-materials/examples/Climate_Datasets/#see-also","title":"See also\u00b6","text":"<p>More information about handling CMIP6 data can be found through this notebook from CEDA.</p>"},{"location":"documentation/training-materials/examples/Code_Snippets/","title":"Code Snippets","text":"In\u00a0[\u00a0]: Copied! <pre># Note that this uses pystac and pystac_client\n\n# import os\n\nimport dask.distributed\nimport odc.stac\nimport rasterio\nfrom pystac.extensions.raster import RasterBand\nfrom pystac_client import Client\n</pre> # Note that this uses pystac and pystac_client  # import os  import dask.distributed import odc.stac import rasterio from pystac.extensions.raster import RasterBand from pystac_client import Client In\u00a0[2]: Copied! <pre>client = dask.distributed.Client()\n</pre> client = dask.distributed.Client() In\u00a0[3]: Copied! <pre>client = Client.open(\"https://api.stac.ceda.ac.uk/\")\nitem_collection = client.search(\n    collections=[\"sentinel2_ard\"],\n    intersects={\"type\": \"Point\", \"coordinates\": [-1.3144, 51.5755]},\n    sortby=\"-properties.datetime\",\n    max_items=10,\n).item_collection()\n\n# Commented out for clarity when submitted to training materials website.\n# Uncomment the next line to view the Feature Collection\n\n#item_collection\n</pre> client = Client.open(\"https://api.stac.ceda.ac.uk/\") item_collection = client.search(     collections=[\"sentinel2_ard\"],     intersects={\"type\": \"Point\", \"coordinates\": [-1.3144, 51.5755]},     sortby=\"-properties.datetime\",     max_items=10, ).item_collection()  # Commented out for clarity when submitted to training materials website. # Uncomment the next line to view the Feature Collection  #item_collection <p>As of November 2024, the STAC items in <code>sentinel2_ard</code> are missing information because the projection and raster extensions are not installed. This means that <code>odc-stac</code> will refuse to compute the \"geobox\" and hence the subsequent components needed to generate and view the data cube. The code in the following cell adds those extensions.</p> In\u00a0[\u00a0]: Copied! <pre>for item in item_collection.items:\n    asset = item.assets[\"cog\"]\n    cog = rasterio.open(asset.href)\n    epsg = cog.crs.to_epsg()\n    dtypes = cog.dtypes\n    shape = cog.shape\n    transform = list(cog.transform)\n\n    item.ext.add(\"proj\")\n    item.ext.add(\"raster\")\n\n    item.ext.proj.epsg = epsg\n\n    cog = item.assets[\"cog\"]\n    cog.ext.raster.bands = [RasterBand.create(data_type=dtype) for dtype in dtypes]\n    cog.ext.proj.shape = shape\n    cog.ext.proj.transform = transform\n</pre> for item in item_collection.items:     asset = item.assets[\"cog\"]     cog = rasterio.open(asset.href)     epsg = cog.crs.to_epsg()     dtypes = cog.dtypes     shape = cog.shape     transform = list(cog.transform)      item.ext.add(\"proj\")     item.ext.add(\"raster\")      item.ext.proj.epsg = epsg      cog = item.assets[\"cog\"]     cog.ext.raster.bands = [RasterBand.create(data_type=dtype) for dtype in dtypes]     cog.ext.proj.shape = shape     cog.ext.proj.transform = transform  In\u00a0[8]: Copied! <pre>bbox = [-1.2, 51.6, -1.1, 51.7]\ndataset = odc.stac.load(item_collection, bands=(\"B03\", \"B04\", \"B08\"), chunks={}, bbox=bbox)\ndataset\n</pre> bbox = [-1.2, 51.6, -1.1, 51.7] dataset = odc.stac.load(item_collection, bands=(\"B03\", \"B04\", \"B08\"), chunks={}, bbox=bbox) dataset Out[8]: <pre>&lt;xarray.Dataset&gt; Size: 48MB\nDimensions:      (y: 1121, x: 706, time: 10)\nCoordinates:\n  * y            (y) float64 9kB 2.005e+05 2.005e+05 ... 1.893e+05 1.893e+05\n  * x            (x) float64 6kB 4.554e+05 4.554e+05 ... 4.624e+05 4.624e+05\n    spatial_ref  int32 4B 27700\n  * time         (time) datetime64[ns] 80B 2023-10-03T11:08:09 ... 2023-11-20...\nData variables:\n    B03          (time, y, x) uint16 16MB dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;\n    B04          (time, y, x) uint16 16MB dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;\n    B08          (time, y, x) uint16 16MB dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>y: 1121</li><li>x: 706</li><li>time: 10</li></ul></li><li>Coordinates: (4)<ul><li>y(y)float642.005e+05 2.005e+05 ... 1.893e+05units :metreresolution :-10.0crs :EPSG:27700<pre>array([200525., 200515., 200505., ..., 189345., 189335., 189325.])</pre></li><li>x(x)float644.554e+05 4.554e+05 ... 4.624e+05units :metreresolution :10.0crs :EPSG:27700<pre>array([455385., 455395., 455405., ..., 462415., 462425., 462435.])</pre></li><li>spatial_ref()int3227700spatial_ref :PROJCRS[\"OSGB36 / British National Grid\",BASEGEOGCRS[\"OSGB36\",DATUM[\"Ordnance Survey of Great Britain 1936\",ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,LENGTHUNIT[\"metre\",1]]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433]],ID[\"EPSG\",4277]],CONVERSION[\"British National Grid\",METHOD[\"Transverse Mercator\",ID[\"EPSG\",9807]],PARAMETER[\"Latitude of natural origin\",49,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8801]],PARAMETER[\"Longitude of natural origin\",-2,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8802]],PARAMETER[\"Scale factor at natural origin\",0.9996012717,SCALEUNIT[\"unity\",1],ID[\"EPSG\",8805]],PARAMETER[\"False easting\",400000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",-100000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]]],CS[Cartesian,2],AXIS[\"(E)\",east,ORDER[1],LENGTHUNIT[\"metre\",1]],AXIS[\"(N)\",north,ORDER[2],LENGTHUNIT[\"metre\",1]],USAGE[SCOPE[\"Engineering survey, topographic mapping.\"],AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49\u00b045'N to 61\u00b0N and 9\u00b0W to 2\u00b0E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],BBOX[49.75,-9.01,61.01,2.01]],ID[\"EPSG\",27700]]crs_wkt :PROJCRS[\"OSGB36 / British National Grid\",BASEGEOGCRS[\"OSGB36\",DATUM[\"Ordnance Survey of Great Britain 1936\",ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,LENGTHUNIT[\"metre\",1]]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433]],ID[\"EPSG\",4277]],CONVERSION[\"British National Grid\",METHOD[\"Transverse Mercator\",ID[\"EPSG\",9807]],PARAMETER[\"Latitude of natural origin\",49,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8801]],PARAMETER[\"Longitude of natural origin\",-2,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8802]],PARAMETER[\"Scale factor at natural origin\",0.9996012717,SCALEUNIT[\"unity\",1],ID[\"EPSG\",8805]],PARAMETER[\"False easting\",400000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",-100000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]]],CS[Cartesian,2],AXIS[\"(E)\",east,ORDER[1],LENGTHUNIT[\"metre\",1]],AXIS[\"(N)\",north,ORDER[2],LENGTHUNIT[\"metre\",1]],USAGE[SCOPE[\"Engineering survey, topographic mapping.\"],AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49\u00b045'N to 61\u00b0N and 9\u00b0W to 2\u00b0E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],BBOX[49.75,-9.01,61.01,2.01]],ID[\"EPSG\",27700]]semi_major_axis :6377563.396semi_minor_axis :6356256.909237285inverse_flattening :299.3249646reference_ellipsoid_name :Airy 1830longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :OSGB36horizontal_datum_name :Ordnance Survey of Great Britain 1936projected_crs_name :OSGB36 / British National Gridgrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :49.0longitude_of_central_meridian :-2.0false_easting :400000.0false_northing :-100000.0scale_factor_at_central_meridian :0.9996012717GeoTransform :455380 10 0 200530 0 -10<pre>array(27700, dtype=int32)</pre></li><li>time(time)datetime64[ns]2023-10-03T11:08:09 ... 2023-11-...<pre>array(['2023-10-03T11:08:09.000000000', '2023-10-06T11:21:19.000000000',\n       '2023-10-08T11:09:41.000000000', '2023-10-13T11:09:19.000000000',\n       '2023-10-26T11:21:19.000000000', '2023-10-28T11:11:51.000000000',\n       '2023-11-10T11:23:11.000000000', '2023-11-15T11:22:39.000000000',\n       '2023-11-17T11:13:31.000000000', '2023-11-20T11:23:51.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (3)<ul><li>B03(time, y, x)uint16dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;  Array   Chunk   Bytes   15.10 MiB   1.51 MiB   Shape   (10, 1121, 706)   (1, 1121, 706)   Dask graph   10 chunks in 3 graph layers   Data type   uint16 numpy.ndarray  706 1121 10 </li><li>B04(time, y, x)uint16dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;  Array   Chunk   Bytes   15.10 MiB   1.51 MiB   Shape   (10, 1121, 706)   (1, 1121, 706)   Dask graph   10 chunks in 3 graph layers   Data type   uint16 numpy.ndarray  706 1121 10 </li><li>B08(time, y, x)uint16dask.array&lt;chunksize=(1, 1121, 706), meta=np.ndarray&gt;  Array   Chunk   Bytes   15.10 MiB   1.51 MiB   Shape   (10, 1121, 706)   (1, 1121, 706)   Dask graph   10 chunks in 3 graph layers   Data type   uint16 numpy.ndarray  706 1121 10 </li></ul></li><li>Indexes: (3)<ul><li>yPandasIndex<pre>PandasIndex(Index([200525.0, 200515.0, 200505.0, 200495.0, 200485.0, 200475.0, 200465.0,\n       200455.0, 200445.0, 200435.0,\n       ...\n       189415.0, 189405.0, 189395.0, 189385.0, 189375.0, 189365.0, 189355.0,\n       189345.0, 189335.0, 189325.0],\n      dtype='float64', name='y', length=1121))</pre></li><li>xPandasIndex<pre>PandasIndex(Index([455385.0, 455395.0, 455405.0, 455415.0, 455425.0, 455435.0, 455445.0,\n       455455.0, 455465.0, 455475.0,\n       ...\n       462345.0, 462355.0, 462365.0, 462375.0, 462385.0, 462395.0, 462405.0,\n       462415.0, 462425.0, 462435.0],\n      dtype='float64', name='x', length=706))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2023-10-03 11:08:09', '2023-10-06 11:21:19',\n               '2023-10-08 11:09:41', '2023-10-13 11:09:19',\n               '2023-10-26 11:21:19', '2023-10-28 11:11:51',\n               '2023-11-10 11:23:11', '2023-11-15 11:22:39',\n               '2023-11-17 11:13:31', '2023-11-20 11:23:51'],\n              dtype='datetime64[ns]', name='time', freq=None))</pre></li></ul></li><li>Attributes: (0)</li></ul> In\u00a0[10]: Copied! <pre>dataset.odc.geobox\ndataset = dataset.compute()\n</pre> dataset.odc.geobox dataset = dataset.compute() In\u00a0[11]: Copied! <pre>_ = dataset.isel(time=0).to_array(\"band\").plot.imshow(vmin=0, vmax=256)\n</pre> _ = dataset.isel(time=0).to_array(\"band\").plot.imshow(vmin=0, vmax=256)"},{"location":"documentation/training-materials/examples/Code_Snippets/#code-snippets","title":"Code Snippets\u00b6","text":"<p>Description &amp; purpose: This Notebook is designed to showcase different tools and functions that are of scientific interest/use and implement the Earth Observation Data Hub. It provides a snapshot of the Hub, the <code>pyeodh</code> API client and the various datasets as of November 2024.   It is assumed that this notebook will be run within the Notebook Service on the Hub.</p>"},{"location":"documentation/training-materials/examples/Code_Snippets/#create-data-cube-using-sentinel2_ard-stac-records","title":"Create data cube using sentinel2_ard STAC records\u00b6","text":"<p>Users may have a need to generate a data cube directly from the CEDA Sentinel 2 Analysis Ready Data (ARD) (<code>sentinel2_ard</code>). As the STAC catalogue for that dataset has been created in a specific way, users will need to be aware of this and make alterations from the default. The following code was supplied by Pete Gadomski of Development Seed and enables the generation of a data cube from the ARD catalogue (as structured in November 2024). The STAC catalogue may be reprocessed in future to allow the default settings for <code>odc-stac</code> to be implemented.</p>"},{"location":"documentation/training-materials/examples/Polar_Data/","title":"Polar Data","text":"<p>Description &amp; purpose: This Notebook is designed to demonstrate how to find and plot EOCIS related data held on the Earth Observation Data Hub.   It is assumed that this notebook will be running within the Notebook Service on the Hub.</p> <p>The first thing to do is ensure that the most recent version of <code>pyeodh</code> is installed on your system. It is good practice to run the following cell if you have not installed <code>pyeodh</code> or have not used it in a while. The cell will also install the plotting and spatial libraries required.</p> In\u00a0[\u00a0]: Copied! <pre># Run this cell if pyeodh is not installed, or needs updating\n%pip install --upgrade pyeodh\n%pip install folium shapely xarray fsspec rioxarray cartopy pyproj\n</pre> # Run this cell if pyeodh is not installed, or needs updating %pip install --upgrade pyeodh %pip install folium shapely xarray fsspec rioxarray cartopy pyproj In\u00a0[\u00a0]: Copied! <pre>import pyeodh\n\nimport cartopy.crs as ccrs\nimport folium\nimport fsspec\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport shapely as sh\nimport xarray as xr\nfrom pyproj import Transformer\nfrom shapely.geometry import Point\n</pre> import pyeodh  import cartopy.crs as ccrs import folium import fsspec import geopandas as gpd import matplotlib.pyplot as plt import pandas as pd  import shapely as sh import xarray as xr from pyproj import Transformer from shapely.geometry import Point In\u00a0[3]: Copied! <pre># Point of interest - ice/land (Nuuk, Greenland)\nipnt = sh.Point(-51.7216, 64.1835)\n\n# Point of interest - ocean (Baffin Bay, Greenland)\nolon, olat = [-54.22884, 83.11055]\nopnt = sh.Point(olon, olat)\n</pre> # Point of interest - ice/land (Nuuk, Greenland) ipnt = sh.Point(-51.7216, 64.1835)  # Point of interest - ocean (Baffin Bay, Greenland) olon, olat = [-54.22884, 83.11055] opnt = sh.Point(olon, olat) In\u00a0[\u00a0]: Copied! <pre># Create a folium map centred between the points\nm = folium.Map(location=[(ipnt.y + opnt.y)/2, (ipnt.x + opnt.x)/2], zoom_start=3)\n\n# Add points to the map\nfor pt, name in [(ipnt, \"Nuuk\"), (opnt, \"Arctic Ocean\")]:\n    folium.Marker([pt.y, pt.x], popup=name).add_to(m)\n\nm\n</pre> # Create a folium map centred between the points m = folium.Map(location=[(ipnt.y + opnt.y)/2, (ipnt.x + opnt.x)/2], zoom_start=3)  # Add points to the map for pt, name in [(ipnt, \"Nuuk\"), (opnt, \"Arctic Ocean\")]:     folium.Marker([pt.y, pt.x], popup=name).add_to(m)  m <p>We need to create an instance of the <code>Client</code>, which is our entrypoint to EODH APIs. From there we can start to search the collections held within the platform. First we'll look at the paths to the data.</p> In\u00a0[5]: Copied! <pre># Connect to the Hub\nclient = pyeodh.Client(base_url=\"https://eodatahub.org.uk\").get_catalog_service()\n\nfor c in client.get_catalogs():\n    print(c._pystac_object.self_href.removeprefix(\"https://eodatahub.org.uk/api/catalogue/stac/catalogs/\"))\n</pre> # Connect to the Hub client = pyeodh.Client(base_url=\"https://eodatahub.org.uk\").get_catalog_service()  for c in client.get_catalogs():     print(c._pystac_object.self_href.removeprefix(\"https://eodatahub.org.uk/api/catalogue/stac/catalogs/\"))  <pre>commercial/catalogs/airbus\nuser/catalogs/npl/catalogs/processing-results/catalogs/qa-workflow/catalogs/airbus_phr_qa\nuser/catalogs/tjellicoetpzuk/catalogs/processing-results/catalogs/snuggs/catalogs/catalog\npublic/catalogs/ceda-stac-catalogue\ncommercial\nuser/catalogs/npl\ncommercial/catalogs/planet\nuser/catalogs/npl/catalogs/processing-results/catalogs/qa-workflow/catalogs/planet_psscene_qa\nuser/catalogs/npl/catalogs/processing-results\nuser/catalogs/tjellicoetpzuk/catalogs/processing-results\n</pre> <p>Now we'll look explicitly for the EOCIS climate data.</p> In\u00a0[6]: Copied! <pre>for index, collect in enumerate(\n        [c for c in client.get_collections() if \"eocis\" in c.id],\n        start=1):\n    print(f\"{index} -- {collect.id}\")\n</pre> for index, collect in enumerate(         [c for c in client.get_collections() if \"eocis\" in c.id],         start=1):     print(f\"{index} -- {collect.id}\")  <pre>1 -- eocis-sst-cdrv3\n2 -- eocis-soil-moisture-africa\n3 -- eocis-lst-s3b-night\n4 -- eocis-lst-s3b-day\n5 -- eocis-lst-s3a-night\n6 -- eocis-lst-s3a-day\n7 -- eocis-chuk-land-vegetation-lai\n8 -- eocis-chuk-land-vegetation-fapar\n9 -- eocis-chuk-geospatial-landcover\n10 -- eocis-chuk-geospatial-elevation\n11 -- eocis-chuk-geospatial-builtarea\n12 -- eocis-chuk-geospatial\n13 -- eocis-arctic-sea-ice-thickness-monthly\n</pre> <p>Note: For additional details see this link.</p> <p>We need to connect to the Sea Ice collection and get some simple information about it</p> In\u00a0[7]: Copied! <pre>datasets = client.get_catalog(\n    \"public/catalogs/ceda-stac-catalogue\").get_collection(\n    'eocis-arctic-sea-ice-thickness-monthly')\n\nprint(\"id: \", datasets.id)\nprint(\"title: \", datasets.title)\nprint(\"description: \", datasets.description)\nprint(\"\")\nprint(\n    \"DATASET TEMPORAL EXTENT: \",\n    [str(d) for d in datasets.extent.temporal.intervals[0]],\n)\n</pre> datasets = client.get_catalog(     \"public/catalogs/ceda-stac-catalogue\").get_collection(     'eocis-arctic-sea-ice-thickness-monthly')  print(\"id: \", datasets.id) print(\"title: \", datasets.title) print(\"description: \", datasets.description) print(\"\") print(     \"DATASET TEMPORAL EXTENT: \",     [str(d) for d in datasets.extent.temporal.intervals[0]], ) <pre>id:  eocis-arctic-sea-ice-thickness-monthly\ntitle:  EOCIS Arctic Monthly Gridded Sea Ice Thickness Product from CryoSat-2\ndescription:  The sea ice products provide a 5x5 km grid of Arctic sea ice thickness (for the whole Arctic region and of 17 sub-regions), delivered as NetCDF files.  EOCIS sea ice thickness NetCDF products are generated monthly by the Centre for Polar Observation and Modelling (CPOM) from radar altimetry measurements taken from the ESA CryoSAT-2 satellite during the winter months (Oct-Apr). Sea ice thickness is only reliably measured from satellite radar altimetry during the winter months. During summer, melt ponds can form on the sea ice floes making it difficult for the satellite to differentiate between floes and leads, and hence calculate sea ice freeboard (and subsequently thickness). Measurement during summer months using radar altimetry is an area of active research (Landy et al, 2022) but is not yet operationally processed.\n\nDATASET TEMPORAL EXTENT:  ['2010-11-15 00:00:00+00:00', '2024-11-15 00:00:00+00:00']\n</pre> <p>It would be useful to understand more about the items that are held within the catalogue. The attributes of a catalogue are mapped to a series of properties. For instance, we can find out the properties such as<code>id</code>, <code>title</code> and <code>description</code>.</p> In\u00a0[8]: Copied! <pre># Now we want to access the first item and see what properties are available. Need to convert an iterator to a list first. \nitems = list(datasets.get_items())\nprint(\"Number of items available: \", len(items))\nprint (\"-----\")\n\nitem = items[1]   # second item\nprint(f\"\\n{item.id}\")\nfor k, v in item.properties.items():\n    print(f\"  {k}: {v}\")\n</pre> # Now we want to access the first item and see what properties are available. Need to convert an iterator to a list first.  items = list(datasets.get_items()) print(\"Number of items available: \", len(items)) print (\"-----\")  item = items[1]   # second item print(f\"\\n{item.id}\") for k, v in item.properties.items():     print(f\"  {k}: {v}\")  <pre>Number of items available:  198\n-----\n\n0a2cdd11-d4e1-4806-ab4f-1a66849cf30a\n  title: EOCIS Arctic Monthly Gridded Sea Ice Thickness 2024-11-15\n  description: The sea ice products provide a 5x5 km grid of Arctic sea ice thickness (for the whole Arctic region and of 17 sub-regions), delivered as NetCDF files.  EOCIS sea ice thickness NetCDF products are generated monthly by the Centre for Polar Observation and Modelling (CPOM) from radar altimetry measurements taken from the ESA CryoSAT-2 satellite during the winter months (Oct-Apr). Sea ice thickness is only reliably measured from satellite radar altimetry during the winter months. During summer, melt ponds can form on the sea ice floes making it difficult for the satellite to differentiate between floes and leads, and hence calculate sea ice freeboard (and subsequently thickness). Measurement during summer months using radar altimetry is an area of active research (Landy et al, 2022) but is not yet operationally processed.\n  datetime: 2024-11-15T00:00:00Z\n  created: 2025-10-09T10:39:36.790911Z\n  updated: 2025-10-09T10:39:36.790911Z\n  license: cc-by-4.0\n  platform: CryoSat-2\n  instruments: ['SIRAL']\n  sci:doi: https://dx.doi.org/10.5285/4c246cb11aa04651a0182b2d329a84f9\n  sci:citation: Ridout, A.; Palmer, B.; Muir, A.; Swiggs, A.; McMillan, M.; Maddalena, J. (2025): EOCIS: Arctic Sea Ice Thickness Grids, v1.00. NERC EDS Centre for Environmental Data Analysis, 20 March 2025. doi:10.5285/4c246cb11aa04651a0182b2d329a84f9.\n  sci:publications: []\n  cube:dimensions: {'xc': {'type': 'spatial', 'axis': 'xc', 'extent': [-3850000, 3749999], 'reference_system': 3413, 'unit': 'm'}, 'yc': {'type': 'spatial', 'axis': 'yc', 'extent': [-5350000, 5849999], 'reference_system': 3413, 'unit': 'm'}, 'time': {'type': 'temporal', 'step': 'P1M', 'values': ['2024-11-15T00:00:00Z']}}\n  cube:variables: {'sea_ice_thickness': {'description': 'sea ice thickness', 'dimensions': ['time', 'yc', 'xc'], 'type': 'data', 'unit': 'm'}, 'sea_ice_thickness_stdev': {'description': 'standard deviation of sea ice thickness, defined as the standard deviation of the thickness in the grid cell', 'dimensions': ['time', 'yc', 'xc'], 'type': 'data', 'unit': 'm'}, 'n_thickness_measurements': {'description': 'number of contributing thickness measurements', 'dimensions': ['time', 'yc', 'xc'], 'type': 'auxiliary'}}\n  renders: {'sea_ice_thickness': {'title': 'Sea Ice Thickness', 'assets': ['reference_file'], 'reference': True, 'variable': 'sea_ice_thickness', 'colormap_name': 'blues_r', 'rescale': [0, 3.5]}, 'sea_ice_thickness_stdev': {'title': 'Standard deviation of Sea Ice Thickness', 'assets': ['reference_file'], 'reference': True, 'variable': 'sea_ice_thickness_stdev', 'colormap_name': 'viridis', 'rescale': [0, 2]}}\n  project: UK Earth Observation Climate Information Service (EOCIS)\n  institution: EOCIS UK\n  resolution: 5 km\n</pre> <p>We can then get the item id, date and name for all items. It is recommended to only look at the first few as the number of items could be large.</p> In\u00a0[9]: Copied! <pre># Warning: without the limit to 10 items this will take a long time for large catalogues\nfor item in datasets.get_items()[:10]:\n    print(item.id, \"---\", item.properties[\"datetime\"], \"---\", item.properties[\"title\"])\n</pre> # Warning: without the limit to 10 items this will take a long time for large catalogues for item in datasets.get_items()[:10]:     print(item.id, \"---\", item.properties[\"datetime\"], \"---\", item.properties[\"title\"]) <pre>ea60feff-3a21-49d1-b79f-9179c052cfc0 --- 2024-11-15T00:00:00Z --- EOCIS Sea-Surface Temperatures V3 2024-11-15\n0a2cdd11-d4e1-4806-ab4f-1a66849cf30a --- 2024-11-15T00:00:00Z --- EOCIS Arctic Monthly Gridded Sea Ice Thickness 2024-11-15\nf2a5764b-5f1e-4011-a8c2-d1eb75995d3b --- 2024-10-15T00:00:00Z --- EOCIS Sea-Surface Temperatures V3 2024-10-15\nc2a3829e-ea0c-4379-be8c-da4f7e0e6c07 --- 2024-10-15T00:00:00Z --- EOCIS Arctic Monthly Gridded Sea Ice Thickness 2024-10-15\nf476f3a5-14ca-4055-9b4d-d735946338ff --- 2024-04-15T00:00:00Z --- EOCIS Arctic Monthly Gridded Sea Ice Thickness 2024-04-15\nd3ee4814-f522-4bf9-99d6-45bff8018cec --- 2024-04-15T00:00:00Z --- EOCIS Sea-Surface Temperatures V3 2024-04-15\naa5d9ec6-d7f3-4a23-b55b-ee6751bb7170 --- 2024-03-15T00:00:00Z --- EOCIS Arctic Monthly Gridded Sea Ice Thickness 2024-03-15\n113583bf-a0d3-499d-9252-ea6ec096abe7 --- 2024-03-15T00:00:00Z --- EOCIS Sea-Surface Temperatures V3 2024-03-15\nd9caf30d-ca73-4673-a079-17342019c598 --- 2024-02-14T00:00:00Z --- EOCIS Arctic Monthly Gridded Sea Ice Thickness 2024-02-14\n8d23432c-9f46-4ae9-9854-27b880d675f3 --- 2024-02-14T00:00:00Z --- EOCIS Sea-Surface Temperatures V3 2024-02-14\n</pre> <p>The next thing to do is find out what assets are held within each item. The asset is the actual data that we are interested in. We will look at the asset information for the first item returned in the list.</p> In\u00a0[10]: Copied! <pre>for item in items[:1]:  # Process only the second item\n    print(f\"Item ID: {item.id}\")\n    print(f\"Item name: {item.properties[\"title\"]}\")\n    print(\"Assets:\")\n    \n    if not item.assets:\n        print(\"  No assets available.\")\n    else:\n        for asset_key, asset in item.assets.items():\n            print(f\"  - {asset_key}: {asset.to_dict()}\")  # Convert asset to dict for readable output\n            print(\"-\" * 40)  # Separator for better readability\n</pre> for item in items[:1]:  # Process only the second item     print(f\"Item ID: {item.id}\")     print(f\"Item name: {item.properties[\"title\"]}\")     print(\"Assets:\")          if not item.assets:         print(\"  No assets available.\")     else:         for asset_key, asset in item.assets.items():             print(f\"  - {asset_key}: {asset.to_dict()}\")  # Convert asset to dict for readable output             print(\"-\" * 40)  # Separator for better readability <pre>Item ID: ea60feff-3a21-49d1-b79f-9179c052cfc0\nItem name: EOCIS Sea-Surface Temperatures V3 2024-11-15\nAssets:\n  - reference_file: {'href': 'https://gws-access.jasmin.ac.uk/public/nceo_uor/eocis-stac/eocis-arctic-sea-ice-thickness-monthly-aux/items/2024/11/EOCIS-SEAICE-L3C-SITHICK-CS2-5KM-202411-fv1.0-kerchunk.json', 'type': 'application/zstd', 'cloud_format': 'kerchunk', 'checksum_type': None, 'open_kwargs': {'open_mapper_kwargs': {}, 'open_zarr_kwargs': {}, 'open_xarray_kwargs': {}}, 'size': None, 'checksum': None, 'roles': ['reference', 'data']}\n----------------------------------------\n  - EOCIS-SEAICE-L3C-SITHICK-CS2-5KM-202411-fv1.0: {'href': 'https://dap.ceda.ac.uk/neodc/eocis/data/global_and_regional/arctic_sea_ice/arctic_sea_ice_thickness_grids/L3C/monthly/v1.0/EOCIS-SEAICE-L3C-SITHICK-CS2-5KM-202411-fv1.0.nc', 'type': 'application/netcdf', 'roles': ['data']}\n----------------------------------------\n</pre> <p>Now we are ready to load some data and visualise it. To do this we will need to use <code>xarray</code>.</p> <p>-- Notes --</p> <p>The data are provided in <code>NetCDF</code> (Network Common Data Form) file format, a widely used standard for storing, sharing, and analyzing scientific data. NetCDF is designed to handle large, multi-dimensional datasets, such as temperature, precipitation, and wind speed, across space and time. The CEDA STAC library also contains reference files in <code>Kerchunk</code> format. Kerchunk is a lightweight reference file format that maps chunks of data in archival formats such as NetCDF that can be lazily loaded to access only the required data chunks for any given request. Rather than converting the source NetCDF to a different format, Kerchunk contains pointers to individual chunks of data and uses the <code>fsspec</code> library to assemble those chunks into the required array in a tool like <code>xarray</code>.</p> In\u00a0[18]: Copied! <pre>item = items[1]\n\n# --- Identify the kerchunk reference asset if it exists ---\nasset = None\n\nif \"reference_file\" in item.assets:\n    asset = item.assets[\"reference_file\"]\n    href = asset.href\n    is_kerchunk = True\nelse:\n    # Fallback: use the first 'data' asset\n    for k, a in item.assets.items():\n        if \"data\" in a.roles or a.type == \"application/netcdf\":\n            asset = a\n            href = asset.href\n            is_kerchunk = False\n            break\n\nprint(\"Selected asset:\", asset)\n\n# Load the dataset depending on asset type\nif is_kerchunk: # kerchunk: open via fsspec + zarr\n    fs = fsspec.filesystem(\"reference\", fo=href)\n    mapper = fs.get_mapper(\"\")\n    ds = xr.open_dataset(mapper, engine=\"zarr\", consolidated=False)\nelse:\n    # netcdf: direct open file\n    ds = xr.open_dataset(href)\n\nprint(\"\\n=== DATASET SUMMARY ===\")\nprint(ds)\n\nprint(\"\\n=== VARIABLES ===\")\nprint(list(ds.data_vars))\n\nprint(\"\\n=== COORDINATES ===\")\nprint(list(ds.coords))\n\nprint(\"\\n=== GLOBAL ATTRIBUTES ===\")\nfor k, v in ds.attrs.items():\n    print(f\"{k}: {v}\")\n</pre> item = items[1]  # --- Identify the kerchunk reference asset if it exists --- asset = None  if \"reference_file\" in item.assets:     asset = item.assets[\"reference_file\"]     href = asset.href     is_kerchunk = True else:     # Fallback: use the first 'data' asset     for k, a in item.assets.items():         if \"data\" in a.roles or a.type == \"application/netcdf\":             asset = a             href = asset.href             is_kerchunk = False             break  print(\"Selected asset:\", asset)  # Load the dataset depending on asset type if is_kerchunk: # kerchunk: open via fsspec + zarr     fs = fsspec.filesystem(\"reference\", fo=href)     mapper = fs.get_mapper(\"\")     ds = xr.open_dataset(mapper, engine=\"zarr\", consolidated=False) else:     # netcdf: direct open file     ds = xr.open_dataset(href)  print(\"\\n=== DATASET SUMMARY ===\") print(ds)  print(\"\\n=== VARIABLES ===\") print(list(ds.data_vars))  print(\"\\n=== COORDINATES ===\") print(list(ds.coords))  print(\"\\n=== GLOBAL ATTRIBUTES ===\") for k, v in ds.attrs.items():     print(f\"{k}: {v}\")  <pre>Selected asset: &lt;Asset href=https://gws-access.jasmin.ac.uk/public/nceo_uor/eocis-stac/eocis-arctic-sea-ice-thickness-monthly-aux/items/2024/11/EOCIS-SEAICE-L3C-SITHICK-CS2-5KM-202411-fv1.0-kerchunk.json&gt;\n\n=== DATASET SUMMARY ===\n&lt;xarray.Dataset&gt; Size: 68MB\nDimensions:                   (yc: 2240, xc: 1520, time: 1, nv: 2)\nCoordinates:\n    lat                       (yc, xc) float32 14MB ...\n    lon                       (yc, xc) float32 14MB ...\n  * time                      (time) datetime64[ns] 8B 2024-11-15\n  * xc                        (xc) float32 6kB -3.85e+06 -3.845e+06 ... 3.75e+06\n  * yc                        (yc) float32 9kB -5.35e+06 -5.345e+06 ... 5.85e+06\nDimensions without coordinates: nv\nData variables:\n    n_thickness_measurements  (time, yc, xc) int32 14MB ...\n    polar_stereographic       float32 4B ...\n    sea_ice_thickness         (time, yc, xc) float32 14MB ...\n    sea_ice_thickness_stdev   (time, yc, xc) float32 14MB ...\n    time_bnds                 (time, nv) datetime64[ns] 16B ...\nAttributes: (12/32)\n    Conventions:               CF-1.10\n    conventions:               CF-1.10\n    creator_name:              Andy Ridout, Alan Muir (CPOM, University Colle...\n    creator_url:               http://www.cpom.org.uk\n    date_created:              20250114T095946Z\n    end_day:                   30\n    ...                        ...\n    start_year:                2024\n    time_coverage_duration:    P1M\n    time_coverage_end:         20241130T235959Z\n    time_coverage_resolution:  P1M\n    time_coverage_start:       20241101T000000Z\n    title:                     Arctic Monthly Gridded Sea Ice Thickness Produ...\n\n=== VARIABLES ===\n['n_thickness_measurements', 'polar_stereographic', 'sea_ice_thickness', 'sea_ice_thickness_stdev', 'time_bnds']\n\n=== COORDINATES ===\n['lat', 'lon', 'time', 'xc', 'yc']\n\n=== GLOBAL ATTRIBUTES ===\nConventions: CF-1.10\nconventions: CF-1.10\ncreator_name: Andy Ridout, Alan Muir (CPOM, University College London)\ncreator_url: http://www.cpom.org.uk\ndate_created: 20250114T095946Z\nend_day: 30\nend_month: 11\nend_year: 2024\ngeospatial_lat_max: 88.2284\ngeospatial_lat_min: 67.1128\ngeospatial_lon_max: 359.9999\ngeospatial_lon_min: 0.0002\ngeospatial_vertical_max: 0.0\ngeospatial_vertical_min: 0.0\ninstitution: Centre for Polar Observation and Modelling (CPOM), U.K\nkeywords: Sea Ice, Thickness, Radar Altimeters\nlicense: Creative Commons Attribution 4.0 International (CC BY 4.0)\nndays: 30\nplatform: CryoSat-2\nprocessing_level: Level-3C\nproject: EOCIS\nsensor: SIRAL\nspatial_resolution: 5 km\nstandard_name_vocabulary: CF\nstart_day: 1\nstart_month: 11\nstart_year: 2024\ntime_coverage_duration: P1M\ntime_coverage_end: 20241130T235959Z\ntime_coverage_resolution: P1M\ntime_coverage_start: 20241101T000000Z\ntitle: Arctic Monthly Gridded Sea Ice Thickness Product from CryoSat-2\n</pre> <p>Now we need to reproject and plot</p> In\u00a0[12]: Copied! <pre># set the projection\nds3413 = ds.rio.write_crs(\"EPSG:3413\", inplace=False)\nsit = ds3413[\"sea_ice_thickness\"].squeeze()\n\nproj = ccrs.Stereographic(central_latitude=90, central_longitude=-45)  # NSIDC default\n\nplt.figure(figsize=(10,10))\nax = plt.axes(projection=proj)\n\n# Transform from EPSG:3413 to map projection\nsit.plot(\n    ax=ax,\n    transform=ccrs.epsg(3413),\n    cmap=\"viridis\",\n    cbar_kwargs={\"label\": \"Sea Ice Thickness (m)\"}\n)\n\nax.coastlines()\nax.set_title(\"Sea Ice Thickness (EPSG:3413)\")\nplt.show()\n</pre>  # set the projection ds3413 = ds.rio.write_crs(\"EPSG:3413\", inplace=False) sit = ds3413[\"sea_ice_thickness\"].squeeze()  proj = ccrs.Stereographic(central_latitude=90, central_longitude=-45)  # NSIDC default  plt.figure(figsize=(10,10)) ax = plt.axes(projection=proj)  # Transform from EPSG:3413 to map projection sit.plot(     ax=ax,     transform=ccrs.epsg(3413),     cmap=\"viridis\",     cbar_kwargs={\"label\": \"Sea Ice Thickness (m)\"} )  ax.coastlines() ax.set_title(\"Sea Ice Thickness (EPSG:3413)\") plt.show()  In\u00a0[13]: Copied! <pre># Transform lon/lat for ocean point to EPSG:3413\nto_3413 = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3413\", always_xy=True)\nx0, y0 = to_3413.transform(olon, olat)\n\n# 50 km buffer in metres\naoi_buffer = Point(x0, y0).buffer(50_000)   # 50 km radius\n</pre> # Transform lon/lat for ocean point to EPSG:3413 to_3413 = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3413\", always_xy=True) x0, y0 = to_3413.transform(olon, olat)  # 50 km buffer in metres aoi_buffer = Point(x0, y0).buffer(50_000)   # 50 km radius In\u00a0[14]: Copied! <pre># Build GeoDataFrame in EPSG:3413\ngdf = gpd.GeoDataFrame(\n    {\"geometry\": [aoi_buffer]},\n    crs=\"EPSG:3413\"\n)\n\n# Clip to buffer polygon\nsit_clip = sit.rio.clip(gdf.geometry, gdf.crs)\n</pre> # Build GeoDataFrame in EPSG:3413 gdf = gpd.GeoDataFrame(     {\"geometry\": [aoi_buffer]},     crs=\"EPSG:3413\" )  # Clip to buffer polygon sit_clip = sit.rio.clip(gdf.geometry, gdf.crs) In\u00a0[15]: Copied! <pre>plt.figure(figsize=(8,8))\nax = plt.axes(projection=proj)\n\nsit_clip.plot(\n    ax=ax,\n    transform=ccrs.epsg(3413),\n    cmap=\"viridis\",\n    cbar_kwargs={\"label\": \"Sea Ice Thickness (m)\"}\n)\n\n# Draw AOI point\nax.plot(x0, y0, \"ro\", markersize=6, transform=ccrs.epsg(3413))\n\nax.coastlines()\nax.set_title(\"Sea Ice Thickness \u2013 50 km AOI Buffer (EPSG:3413)\")\nplt.show()\n</pre> plt.figure(figsize=(8,8)) ax = plt.axes(projection=proj)  sit_clip.plot(     ax=ax,     transform=ccrs.epsg(3413),     cmap=\"viridis\",     cbar_kwargs={\"label\": \"Sea Ice Thickness (m)\"} )  # Draw AOI point ax.plot(x0, y0, \"ro\", markersize=6, transform=ccrs.epsg(3413))  ax.coastlines() ax.set_title(\"Sea Ice Thickness \u2013 50 km AOI Buffer (EPSG:3413)\") plt.show()  <p>We can do something similar for other datasets. We would usually follow the data discovery steps above for each dataset we are interested in. For brevity we can access the <code>href</code> that we require by completing an online search and copying the reference file <code>href</code> into the cell below.</p> In\u00a0[16]: Copied! <pre>nhref = \"https://gws-access.jasmin.ac.uk/public/nceo_uor/eocis-stac/sst-cdrv3-aux/items/2024/06/20240621120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_ICDR3.0-v02.0-fv01.0-kerchunk.json\"\n\n# Load the dataset \nfsst = fsspec.filesystem(\"reference\", fo=nhref)\nmapper1 = fsst.get_mapper(\"\")\ndsst = xr.open_dataset(mapper1, engine=\"zarr\", consolidated=False)\n\nprint(\"\\n=== DATASET SUMMARY ===\")\nprint(dsst)\n\nprint(\"\\n=== VARIABLES ===\")\nprint(list(dsst.data_vars))\n\nprint(\"\\n=== COORDINATES ===\")\nprint(list(dsst.coords))\n\nprint(\"\\n=== GLOBAL ATTRIBUTES ===\")\nfor k, v in dsst.attrs.items():\n    print(f\"{k}: {v}\")\n</pre>  nhref = \"https://gws-access.jasmin.ac.uk/public/nceo_uor/eocis-stac/sst-cdrv3-aux/items/2024/06/20240621120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_ICDR3.0-v02.0-fv01.0-kerchunk.json\"  # Load the dataset  fsst = fsspec.filesystem(\"reference\", fo=nhref) mapper1 = fsst.get_mapper(\"\") dsst = xr.open_dataset(mapper1, engine=\"zarr\", consolidated=False)  print(\"\\n=== DATASET SUMMARY ===\") print(dsst)  print(\"\\n=== VARIABLES ===\") print(list(dsst.data_vars))  print(\"\\n=== COORDINATES ===\") print(list(dsst.coords))  print(\"\\n=== GLOBAL ATTRIBUTES ===\") for k, v in dsst.attrs.items():     print(f\"{k}: {v}\") <pre>\n=== DATASET SUMMARY ===\n&lt;xarray.Dataset&gt; Size: 726MB\nDimensions:                   (time: 1, lat: 3600, lon: 7200, bnds: 2)\nCoordinates:\n  * lat                       (lat) float32 14kB -89.97 -89.93 ... 89.93 89.97\n  * lon                       (lon) float32 29kB -180.0 -179.9 ... 179.9 180.0\n  * time                      (time) datetime64[ns] 8B 2024-06-21T12:00:00\nDimensions without coordinates: bnds\nData variables:\n    analysed_sst              (time, lat, lon) float64 207MB ...\n    analysed_sst_uncertainty  (time, lat, lon) float64 207MB ...\n    lat_bnds                  (lat, bnds) float32 29kB ...\n    lon_bnds                  (lon, bnds) float32 58kB ...\n    mask                      (time, lat, lon) float32 104MB ...\n    sea_ice_fraction          (time, lat, lon) float64 207MB ...\n    time_bnds                 (time, bnds) datetime64[ns] 16B ...\nAttributes: (12/66)\n    Conventions:                    CF-1.5, Unidata Observation Dataset v1.0\n    Metadata_Conventions:           Unidata Dataset Discovery v1.0\n    acknowledgment:                 The European Space Agency (ESA) funded th...\n    cdm_data_type:                  grid\n    comment:                        These data were produced by the Met Offic...\n    contact:                        https://climate.esa.int/en/projects/sea-s...\n    ...                             ...\n    time_coverage_resolution:       P1D\n    time_coverage_start:            20240621T000000Z\n    title:                          ESA SST CCI Analysis v3.0\n    tracking_id:                    ef4779ea-3d9a-42b5-8423-9da05b7b7d7d\n    uuid:                           ef4779ea-3d9a-42b5-8423-9da05b7b7d7d\n    westernmost_longitude:          -180.0\n\n=== VARIABLES ===\n['analysed_sst', 'analysed_sst_uncertainty', 'lat_bnds', 'lon_bnds', 'mask', 'sea_ice_fraction', 'time_bnds']\n\n=== COORDINATES ===\n['lat', 'lon', 'time']\n\n=== GLOBAL ATTRIBUTES ===\nConventions: CF-1.5, Unidata Observation Dataset v1.0\nMetadata_Conventions: Unidata Dataset Discovery v1.0\nacknowledgment: The European Space Agency (ESA) funded the research and development of software to generate these data (grant reference ESA/AO/1-9322/18/I-NB), in addition to funding the production of the Climate Data Record (CDR) for 1980 to 2021. The Copernicus Climate Change Service (C3S) funded the development of the Interim-CDR (ICDR) extension and production of ICDR during 2022. From 2023 onwards the production of the ICDR is funded by the UK Natural Environment Research Council (NERC grant reference number NE/X019071/1, Earth Observation Climate Information Service) and the UK Marine and Climate Advisory Service (UKMCAS), benefitting from the Earth Observation Investment Package of the Department of Science, Innovation and Technology.\ncdm_data_type: grid\ncomment: These data were produced by the Met Office as part of the MCAS project. WARNING Some applications are unable to properly handle signed byte values. If values are encountered &gt; 127, please subtract 256 from this reported value\ncontact: https://climate.esa.int/en/projects/sea-surface-temperature\ncontributor_name: JASMIN\ncontributor_role: This work used JASMIN, the UK's collaborative data analysis environment (https://jasmin.ac.uk)\ncreation_date: 2024-07-04T15:56:27Z\ncreator_email: ml-ostia@metoffice.gov.uk\ncreator_institution: Met Office\ncreator_name: Met Office\ncreator_type: institution\ncreator_url: https://www.metoffice.gov.uk\ndate_created: 20240704T155627Z\ndoi: 10.5285/4a9654136a7148e39b7feb56f8bb02d2\neasternmost_longitude: 180.00001525878906\nfile_quality_level: 3\ngds_version_id: 2.0\ngeospatial_lat_max: 90.0\ngeospatial_lat_min: -90.0\ngeospatial_lat_resolution: 0.05000000074505806\ngeospatial_lat_units: degrees_north\ngeospatial_lon_max: 180.0\ngeospatial_lon_min: -180.0\ngeospatial_lon_resolution: 0.05000000074505806\ngeospatial_lon_units: degrees_east\ngeospatial_vertical_max: -0.20000000298023224\ngeospatial_vertical_min: -0.20000000298023224\nhistory: Created using OSTIA reanalysis system ICDR3.0\nid: OSTIA-ESACCI-L4-GLOB_ICDR-v3.0\ninstitution: ESACCI\nkey_variables: analysed_sst,sea_ice_fraction\nkeywords: Oceans &gt; Ocean Temperature &gt; Sea Surface Temperature\nkeywords_vocabulary: NASA Global Change Master Directory (GCMD) Science Keywords\nlicense: Creative Commons Attribution 4.0 https://creativecommons.org/licenses/by/4.0/ \nUsers of these data should cite the dataset along with the dataset paper: Embury, O. et al. Satellite-based time-series of sea-surface temperature since 1980 for climate applications. Scientific Data (2024). https://doi.org/10.1038/s41597-024-03147-w\nmetadata_link: https://doi.org/10.5285/4a9654136a7148e39b7feb56f8bb02d2\nnaming_authority: org.ghrsst\nnetcdf_version_id: 4.3.2\nnorthernmost_latitude: 90.0\nplatform: MetOpB, Sentinel-3A, Sentinel-3B\nprocessing_level: L4\nproduct_specification_version: SST_CCI-PSD-UKMO-201-Issue-2\nproduct_version: 3.0.1\nproject: UK Marine and Climate Advisory Service\npublisher_email: support@ceda.ac.uk\npublisher_name: NERC EDS Centre for Environmental Data Analysis\npublisher_type: institution\npublisher_url: https://www.ceda.ac.uk\nreferences: Embury, O. et al. Satellite-based time-series of sea-surface temperature since 1980 for climate applications. Scientific Data (2024). https://doi.org/10.1038/s41597-024-03147-w\nsensor: AVHRR, SLSTR\nsource: AVHRRMTB-UKEOCIS-L3U-ICDR-v3.0, SLSTRA-UKEOCIS-L3U-ICDR-v3.0, SLSTRB-UKEOCIS-L3U-ICDR-v3.0, EUMETSAT_OSI-SAF-ICE-OSI-430-a\nsouthernmost_latitude: -90.0\nspatial_resolution: 0.05 degree\nstandard_name_vocabulary: NetCDF Climate and Forecast (CF) Metadata Convention\nstart_time: 20240621T000000Z\nstop_time: 20240622T000000Z\nsummary: European Space Agency Sea Surface Temperature Climate Change Initiative: Analysis product version 3.0\ntime_coverage_duration: P1D\ntime_coverage_end: 20240622T000000Z\ntime_coverage_resolution: P1D\ntime_coverage_start: 20240621T000000Z\ntitle: ESA SST CCI Analysis v3.0\ntracking_id: ef4779ea-3d9a-42b5-8423-9da05b7b7d7d\nuuid: ef4779ea-3d9a-42b5-8423-9da05b7b7d7d\nwesternmost_longitude: -180.0\n</pre> In\u00a0[17]: Copied! <pre># Extract SST variable\nsst = dsst[\"analysed_sst\"].squeeze()   # already in EPSG:4326\n\nproj = ccrs.PlateCarree()  # map projection = EPSG:4326\n\nplt.figure(figsize=(10,10))\nax = plt.axes(projection=proj)\n\nsst.plot(\n    ax=ax,\n    transform=ccrs.PlateCarree(),   # IMPORTANT: matches data CRS\n    cmap=\"viridis\",\n    cbar_kwargs={\"label\": \"Sea Surface Temperature (\u00b0C)\"}\n)\n\nax.coastlines()\nax.set_title(\"Sea Surface Temperature\")\nplt.show()\n</pre>  # Extract SST variable sst = dsst[\"analysed_sst\"].squeeze()   # already in EPSG:4326  proj = ccrs.PlateCarree()  # map projection = EPSG:4326  plt.figure(figsize=(10,10)) ax = plt.axes(projection=proj)  sst.plot(     ax=ax,     transform=ccrs.PlateCarree(),   # IMPORTANT: matches data CRS     cmap=\"viridis\",     cbar_kwargs={\"label\": \"Sea Surface Temperature (\u00b0C)\"} )  ax.coastlines() ax.set_title(\"Sea Surface Temperature\") plt.show()   In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"documentation/training-materials/examples/Polar_Data/#polar-data","title":"Polar Data\u00b6","text":""},{"location":"documentation/training-materials/examples/Polar_Data/#imports","title":"Imports\u00b6","text":""},{"location":"documentation/training-materials/examples/Polar_Data/#set-geographic-locations","title":"Set geographic locations\u00b6","text":""},{"location":"documentation/training-materials/examples/Polar_Data/#exploring-the-resource-catalogue","title":"Exploring the Resource Catalogue\u00b6","text":"<p>Now we are ready to investigate the Resource Catalogue.</p>"},{"location":"documentation/training-materials/examples/Polar_Data/#sea-ice","title":"Sea Ice\u00b6","text":""},{"location":"documentation/training-materials/examples/Polar_Data/#sea-surface-temperature","title":"Sea Surface Temperature\u00b6","text":""},{"location":"documentation/training-materials/examples/Timeseries/","title":"Timeseries Plotting","text":"<p>Description &amp; purpose: This Notebook is designed to demonstrate how to plot a timeseries using data held on the Earth Observation Data Hub. This notebook plots data for a point in Greater London. It is assumed that the notebook will be running within the Notebook Service on the Hub.</p> <p>The first thing to do is ensure that the most recent version of <code>pyeodh</code> is installed on your system. It is good practice to run the following cell if you have not installed <code>pyeodh</code> or have not used it in a while. The cell will also install the plotting and spatial libraries required.</p> In\u00a0[\u00a0]: Copied! <pre># Run this cell if pyeodh is not installed, or needs updating\n%pip install --upgrade pyeodh\n%pip install hvplot shapely ipywidgets rasterio pyproj\n</pre> # Run this cell if pyeodh is not installed, or needs updating %pip install --upgrade pyeodh %pip install hvplot shapely ipywidgets rasterio pyproj In\u00a0[\u00a0]: Copied! <pre># Import the Python API Client\nimport pyeodh\n\n# Import all other packages\nimport math\nimport shapely \nimport rasterio\n\nimport pandas as pd\n# import hvplot.pandas\nimport matplotlib.pyplot as plt\nfrom pyproj import Transformer\n\nimport urllib.request\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom IPython.display import display\n# from ipywidgets import interact\n</pre> # Import the Python API Client import pyeodh  # Import all other packages import math import shapely  import rasterio  import pandas as pd # import hvplot.pandas import matplotlib.pyplot as plt from pyproj import Transformer  import urllib.request from PIL import Image from io import BytesIO  from IPython.display import display # from ipywidgets import interact <p>Next we need to create an instance of the <code>Client</code>, which is our entrypoint to EODH APIs. From there we can start to search the collections held within the platform.</p> In\u00a0[\u00a0]: Copied! <pre># Connect to the Hub\n# base_url can be changed to optionally specify a different server, but the default is the production server\n\nclient = pyeodh.Client(\n    base_url=\"https://eodatahub.org.uk\"\n).get_catalog_service()\n\n\n# Print a list of the collections held in the Resource Catalogue (their id and description).\n# As the Resource Catalogue fills and development continues, the number of collections and the richness of their descriptions will increase\nfor index, collect in enumerate(client.get_collections(), start=1):\n    print(f\"{index} -- {collect.id}\")\n    print(f\"{collect.description}\")\n</pre> # Connect to the Hub # base_url can be changed to optionally specify a different server, but the default is the production server  client = pyeodh.Client(     base_url=\"https://eodatahub.org.uk\" ).get_catalog_service()   # Print a list of the collections held in the Resource Catalogue (their id and description). # As the Resource Catalogue fills and development continues, the number of collections and the richness of their descriptions will increase for index, collect in enumerate(client.get_collections(), start=1):     print(f\"{index} -- {collect.id}\")     print(f\"{collect.description}\") <p>Note: for clarity the output from this command is not being shown here. Expect a long scrollable list.</p> <p>The attributes of a catalogue are mapped to a series of properties. For instance, in the following cell we print the <code>id</code>, <code>title</code> and <code>description</code>.</p> In\u00a0[4]: Copied! <pre># The next thing to do is find some open data\n\n# Let's use the CEDA Sentinel 2 ARD\ns2ard_cat = client.get_catalog(\"public/catalogs/ceda-stac-catalogue\").get_collection('sentinel2_ard')\n\nprint(\"id: \", s2ard_cat.id)\nprint(\"title: \", s2ard_cat.title)\nprint(\"description: \", s2ard_cat.description)\n</pre> # The next thing to do is find some open data  # Let's use the CEDA Sentinel 2 ARD s2ard_cat = client.get_catalog(\"public/catalogs/ceda-stac-catalogue\").get_collection('sentinel2_ard')  print(\"id: \", s2ard_cat.id) print(\"title: \", s2ard_cat.title) print(\"description: \", s2ard_cat.description) <pre>id:  sentinel2_ard\ntitle:  Sentinel 2 ARD\ndescription:  These data have been created by the Department for Environment, Food and Rural Affairs (Defra) and Joint Nature Conservation Committee (JNCC) in order to cost-effectively provide high quality, Analysis Ready Data (ARD) for a wide range of applications. The dataset contains modified Copernicus Sentinel-2 (Level 1C data processed into a surface reflectance product using ARCSI software (Level 2)).\n</pre> <p>The Hub API endpoints are wrapped in methods inside <code>pyeodh</code> and are structured into classes, following the same logic as the underlying APIs. This means that, for example, to fetch a collection item we first need to get the collection from the resource catalogue. The following cell provedes a code example to do this.</p> In\u00a0[5]: Copied! <pre>s2ard = client.get_catalog(\"public/catalogs/ceda-stac-catalogue\").get_collection('sentinel2_ard')\ns2ard\n</pre> s2ard = client.get_catalog(\"public/catalogs/ceda-stac-catalogue\").get_collection('sentinel2_ard') s2ard Out[5]: <pre>&lt;pyeodh.resource_catalog.Collection at 0x7f72a64f3830&gt;</pre> <p>Some API responses are paginated (e.g. collection items), and you can simply iterate over them.</p> In\u00a0[6]: Copied! <pre>items = s2ard.get_items()\n\n# Warning: without the limit to 10 items this will take a long time for large catalogues such as s2ard\nfor item in items[:10]:\n    print(item.id)\n</pre> items = s2ard.get_items()  # Warning: without the limit to 10 items this will take a long time for large catalogues such as s2ard for item in items[:10]:     print(item.id) <pre>neodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn590lonw0063_T29VPF_ORB023_20251130142106_utm29n_osgb\nneodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn590lonw0055_T30VUL_ORB023_20251130142106_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn590lonw0038_T30VVL_ORB023_20251130142106_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn581lonw0064_T29VPE_ORB023_20251130142106_utm29n_osgb\nneodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn581lonw0055_T30VUK_ORB023_20251130142106_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn581lonw0038_T30VVK_ORB023_20251130142106_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn572lonw0064_T29VPD_ORB023_20251130142106_utm29n_osgb\nneodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn572lonw0054_T30VUJ_ORB023_20251130142106_utm30n_osgb\nneodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn563lonw0065_T29VPC_ORB023_20251130142106_utm29n_osgb\nneodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn563lonw0053_T30VUH_ORB023_20251130142106_utm30n_osgb\n</pre> <p>Now we want to access the first few items and see what they are called and how much cloud there is.</p> In\u00a0[7]: Copied! <pre>for i, item in enumerate(items[:10]):\n    print(i, \" \", item.id)\n    print(\"Cloud cover: \", item.properties['eo:cloud_cover']) \n</pre> for i, item in enumerate(items[:10]):     print(i, \" \", item.id)     print(\"Cloud cover: \", item.properties['eo:cloud_cover'])  <pre>0   neodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn590lonw0063_T29VPF_ORB023_20251130142106_utm29n_osgb\nCloud cover:  42.67\n1   neodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn590lonw0055_T30VUL_ORB023_20251130142106_utm30n_osgb\nCloud cover:  55.48\n2   neodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn590lonw0038_T30VVL_ORB023_20251130142106_utm30n_osgb\nCloud cover:  31.54\n3   neodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn581lonw0064_T29VPE_ORB023_20251130142106_utm29n_osgb\nCloud cover:  26.12\n4   neodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn581lonw0055_T30VUK_ORB023_20251130142106_utm30n_osgb\nCloud cover:  16.69\n5   neodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn581lonw0038_T30VVK_ORB023_20251130142106_utm30n_osgb\nCloud cover:  13.24\n6   neodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn572lonw0064_T29VPD_ORB023_20251130142106_utm29n_osgb\nCloud cover:  50.0\n7   neodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn572lonw0054_T30VUJ_ORB023_20251130142106_utm30n_osgb\nCloud cover:  15.11\n8   neodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn563lonw0065_T29VPC_ORB023_20251130142106_utm29n_osgb\nCloud cover:  78.08\n9   neodc.sentinel_ard.data.sentinel_2.2025.11.30.S2A_20251130_latn563lonw0053_T30VUH_ORB023_20251130142106_utm30n_osgb\nCloud cover:  24.78\n</pre> <p>To find specific imagery for a given date range we can set up a search with a query. That query needs to have a start and end date.</p> <p>If we wanted a specific location, we could add in an <code>intersects</code> parameter.</p> In\u00a0[8]: Copied! <pre>items = client.search(\n    collections=['sentinel2_ard'],\n    catalog_paths=[\"public/catalogs/ceda-stac-catalogue\"],\n    query=[\n        'start_datetime&gt;=2024-04-01',\n        'end_datetime&lt;=2024-09-30', \n    ],\n)\n\n# We can then count the number of items returned by the search \n\ntotal_items = sum(1 for _ in items)\nprint(f\"Total items: {total_items}\")\n</pre>  items = client.search(     collections=['sentinel2_ard'],     catalog_paths=[\"public/catalogs/ceda-stac-catalogue\"],     query=[         'start_datetime&gt;=2024-04-01',         'end_datetime&lt;=2024-09-30',      ], )  # We can then count the number of items returned by the search   total_items = sum(1 for _ in items) print(f\"Total items: {total_items}\")  <pre>Total items: 3412\n</pre> <p>For this example we will use a location that relates to the WWT London Wetlands Centre: https://www.wwt.org.uk/wetland-centres/london</p> In\u00a0[9]: Copied! <pre># Define the WWT site\nlon, lat = (-0.2346089199955088, 51.478446832015834)\nwwt_pnt = shapely.Point(lon, lat) \n\n\nitems = client.search(\n    collections=['sentinel2_ard'],\n    catalog_paths=[\"public/catalogs/ceda-stac-catalogue\"],\n    intersects = wwt_pnt,\n    query=[\n        'start_datetime&gt;=2024-04-01',\n        'end_datetime&lt;=2024-09-30',\n        'eo:cloud_cover&lt;=30',\n    ],\n)\n\n# We can then count the number of items returned by the search \n\ntotal_items = sum(1 for _ in items)\nprint(f\"Total items: {total_items}\")\n</pre> # Define the WWT site lon, lat = (-0.2346089199955088, 51.478446832015834) wwt_pnt = shapely.Point(lon, lat)    items = client.search(     collections=['sentinel2_ard'],     catalog_paths=[\"public/catalogs/ceda-stac-catalogue\"],     intersects = wwt_pnt,     query=[         'start_datetime&gt;=2024-04-01',         'end_datetime&lt;=2024-09-30',         'eo:cloud_cover&lt;=30',     ], )  # We can then count the number of items returned by the search   total_items = sum(1 for _ in items) print(f\"Total items: {total_items}\") <pre>Total items: 13\n</pre> <p>A useful thing to do now is find the asset information for one of those items. We shall use the first item in the list.</p> In\u00a0[10]: Copied! <pre>for item in items[:1]:  # Process only the first item\n    print(f\"Item ID: {item.id}\")\n    print(\"Assets:\")\n    \n    if not item.assets:\n        print(\"  No assets available.\")\n    else:\n        for asset_key, asset in item.assets.items():\n            print(f\"  - {asset_key}: {asset.to_dict()}\")  # Convert asset to dict for readable output\n            print(\"-\" * 40)  # Separator for better readability\n</pre> for item in items[:1]:  # Process only the first item     print(f\"Item ID: {item.id}\")     print(\"Assets:\")          if not item.assets:         print(\"  No assets available.\")     else:         for asset_key, asset in item.assets.items():             print(f\"  - {asset_key}: {asset.to_dict()}\")  # Convert asset to dict for readable output             print(\"-\" * 40)  # Separator for better readability <pre>Item ID: neodc.sentinel_ard.data.sentinel_2.2024.09.19.S2A_20240919_latn518lonw0008_T30UXC_ORB094_20240919144023_utm30n_osgb\nAssets:\n  - cloud: {'href': 'https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2024/09/19/S2A_20240919_latn518lonw0008_T30UXC_ORB094_20240919144023_utm30n_osgb_clouds.tif', 'type': 'image/tiff; application=geotiff', 'size': 2116177, 'location': 'on_disk', 'roles': ['data']}\n----------------------------------------\n  - cloud_probability: {'href': 'https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2024/09/19/S2A_20240919_latn518lonw0008_T30UXC_ORB094_20240919144023_utm30n_osgb_clouds_prob.tif', 'type': 'image/tiff; application=geotiff', 'size': 31613463, 'location': 'on_disk', 'roles': ['data']}\n----------------------------------------\n  - cog: {'href': 'https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2024/09/19/S2A_20240919_latn518lonw0008_T30UXC_ORB094_20240919144023_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.tif', 'type': 'image/tiff; application=geotiff', 'size': 571656423, 'location': 'on_disk', 'eo:bands': [{'eo:central_wavelength': 496.6, 'eo:common_name': 'blue', 'description': 'Blue', 'eo: full_width_half_max': 0.07, 'name': 'B02'}, {'eo:central_wavelength': 560, 'eo:common_name': 'green', 'description': 'Green', 'eo: full_width_half_max': 0.04, 'name': 'B03'}, {'eo:central_wavelength': 664.5, 'eo:common_name': 'red', 'description': 'Red', 'eo: full_width_half_max': 0.03, 'name': 'B04'}, {'eo:central_wavelength': 703.9, 'eo:common_name': 'rededge', 'description': 'Visible and Near Infrared', 'eo: full_width_half_max': 0.02, 'name': 'B05'}, {'eo:central_wavelength': 740.2, 'eo:common_name': 'rededge', 'description': 'Visible and Near Infrared', 'eo: full_width_half_max': 0.02, 'name': 'B06'}, {'eo:central_wavelength': 782.5, 'eo:common_name': 'rededge', 'description': 'Visible and Near Infrared', 'eo: full_width_half_max': 0.02, 'name': 'B07'}, {'eo:central_wavelength': 835.1, 'eo:common_name': 'nir', 'description': 'Visible and Near Infrared', 'eo: full_width_half_max': 0.11, 'name': 'B08'}, {'eo:central_wavelength': 864.8, 'eo:common_name': 'nir08', 'description': 'Visible and Near Infrared', 'eo: full_width_half_max': 0.02, 'name': 'B08a'}, {'eo:central_wavelength': 1613.7, 'eo:common_name': 'swir16', 'description': 'Short Wave Infrared', 'eo: full_width_half_max': 0.09, 'name': 'B11'}, {'eo:central_wavelength': 2202.4, 'eo:common_name': 'swir22', 'description': 'Short Wave Infrared', 'eo: full_width_half_max': 0.18, 'name': 'B12'}], 'roles': ['data']}\n----------------------------------------\n  - metadata: {'href': 'https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2024/09/19/S2A_20240919_latn518lonw0008_T30UXC_ORB094_20240919144023_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref_meta.xml', 'type': 'application/xml', 'size': 18453, 'location': 'on_disk', 'roles': ['metadata']}\n----------------------------------------\n  - saturated_pixels: {'href': 'https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2024/09/19/S2A_20240919_latn518lonw0008_T30UXC_ORB094_20240919144023_utm30n_osgb_sat.tif', 'type': 'image/tiff; application=geotiff', 'size': 1774631, 'location': 'on_disk', 'roles': ['data']}\n----------------------------------------\n  - thumbnail: {'href': 'https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2024/09/19/S2A_20240919_latn518lonw0008_T30UXC_ORB094_20240919144023_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref_thumbnail.jpg', 'type': 'image/jpeg', 'size': 40710, 'location': 'on_disk', 'roles': ['thumbnail']}\n----------------------------------------\n  - topographic_shadow: {'href': 'https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2024/09/19/S2A_20240919_latn518lonw0008_T30UXC_ORB094_20240919144023_utm30n_osgb_toposhad.tif', 'type': 'image/tiff; application=geotiff', 'size': 215956, 'location': 'on_disk', 'roles': ['data']}\n----------------------------------------\n  - valid_pixels: {'href': 'https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2024/09/19/S2A_20240919_latn518lonw0008_T30UXC_ORB094_20240919144023_utm30n_osgb_valid.tif', 'type': 'image/tiff; application=geotiff', 'size': 319023, 'location': 'on_disk', 'roles': ['data']}\n----------------------------------------\n</pre> <p>We can see that this returns a lot of information. We can extract the thumbnail URL and use that to plot the image overview.</p> In\u00a0[11]: Copied! <pre>tn_url = None\n\nfor item in items[:2]:  # Process only the second item\n    \n    if not item.assets:\n        print(\"  No assets available.\")\n    else:\n        for asset_key, asset in item.assets.items():\n            if asset_key == \"thumbnail\":\n                tn_url = asset.href  # Directly access the href attribute\n    \n# Here we open the remote URL, read the data and dislay the thumbnail \nwith urllib.request.urlopen(tn_url) as url:\n    img = Image.open(BytesIO(url.read()))\n\ndisplay(img)\n</pre> tn_url = None  for item in items[:2]:  # Process only the second item          if not item.assets:         print(\"  No assets available.\")     else:         for asset_key, asset in item.assets.items():             if asset_key == \"thumbnail\":                 tn_url = asset.href  # Directly access the href attribute      # Here we open the remote URL, read the data and dislay the thumbnail  with urllib.request.urlopen(tn_url) as url:     img = Image.open(BytesIO(url.read()))  display(img) In\u00a0[12]: Copied! <pre># code to show all thumbnails\n\nthumbnail_urls = []\n\nfor item in items:\n    if not item.assets:\n        continue\n    if \"thumbnail\" in item.assets:\n        thumbnail_urls.append(item.assets[\"thumbnail\"].href) # collect link to thumbnails\n\nprint(f\"Number of assets with thumbnails: {len(thumbnail_urls)}\")\n\n# Ask for start and end thumbnails\nstart = int(input(\"Start image: \"))\nend   = int(input(\"End image: \"))\n\n# Clamp the range\nstart = max(0, start)\nend   = min(len(thumbnail_urls), end)\n\nselected_urls = thumbnail_urls[start:end]\n\nprint(f\"Showing thumbnails {start} to {end-1}\")\n\nimages = []\nfor url in selected_urls:\n    with urllib.request.urlopen(url) as u:\n        img = Image.open(BytesIO(u.read())) # get the thumbnails\n        images.append(img)\n\n# Determine grid size (roughly square)\ncols = 5\nrows = math.ceil(len(images) / cols)\n</pre> # code to show all thumbnails  thumbnail_urls = []  for item in items:     if not item.assets:         continue     if \"thumbnail\" in item.assets:         thumbnail_urls.append(item.assets[\"thumbnail\"].href) # collect link to thumbnails  print(f\"Number of assets with thumbnails: {len(thumbnail_urls)}\")  # Ask for start and end thumbnails start = int(input(\"Start image: \")) end   = int(input(\"End image: \"))  # Clamp the range start = max(0, start) end   = min(len(thumbnail_urls), end)  selected_urls = thumbnail_urls[start:end]  print(f\"Showing thumbnails {start} to {end-1}\")  images = [] for url in selected_urls:     with urllib.request.urlopen(url) as u:         img = Image.open(BytesIO(u.read())) # get the thumbnails         images.append(img)  # Determine grid size (roughly square) cols = 5 rows = math.ceil(len(images) / cols) <pre>Number of assets with thumbnails: 13\n</pre> <pre>Start image:  1\nEnd image:  6\n</pre> <pre>Showing thumbnails 1 to 5\n</pre> In\u00a0[13]: Copied! <pre># Plot thumbnail matrix\nfig = plt.figure(figsize=(15, 3 * rows))\n\nfor i, img in enumerate(images):\n    ax = fig.add_subplot(rows, cols, i + 1)\n    ax.imshow(img)\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n</pre> # Plot thumbnail matrix fig = plt.figure(figsize=(15, 3 * rows))  for i, img in enumerate(images):     ax = fig.add_subplot(rows, cols, i + 1)     ax.imshow(img)     ax.axis(\"off\")  plt.tight_layout() plt.show() In\u00a0[16]: Copied! <pre># EPSG:4326 \u2192 EPSG:27700\ntransformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:27700\", always_xy=True)\nxc, yc = transformer.transform(lon, lat)\n\n\ndates, ndwi_vals = [], []\n\n# Extract NDWI for each scene \nfor item in items:\n\n    asset = item.assets[\"cog\"]\n    href = asset.href\n\n    # Extract band ordering from eo:bands list\n    bands = asset.extra_fields[\"eo:bands\"]\n    band_index = {b[\"name\"]: i + 1 for i, b in enumerate(bands)}\n\n    # Need B03 (green) and B08 (nir)\n    g_idx = band_index[\"B03\"]\n    n_idx = band_index[\"B08\"]\n\n    # Sample the COG\n    try:\n        with rasterio.open(href) as src:\n            green = list(src.sample([(xc, yc)], indexes=g_idx))[0][0]\n            nir   = list(src.sample([(xc, yc)], indexes=n_idx))[0][0]\n    except:\n        continue\n\n    # Skip invalid values\n    if green &lt;= 0 or nir &lt;= 0:\n        continue\n\n    ndwi = (((green/100) - (nir/100)) / ((green/100) + (nir/100))) # Working on the reasoning that the data needs rescaling by 100 \n\n    # Store result\n    dates.append(item.datetime)\n    ndwi_vals.append(float(ndwi))\n</pre> # EPSG:4326 \u2192 EPSG:27700 transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:27700\", always_xy=True) xc, yc = transformer.transform(lon, lat)   dates, ndwi_vals = [], []  # Extract NDWI for each scene  for item in items:      asset = item.assets[\"cog\"]     href = asset.href      # Extract band ordering from eo:bands list     bands = asset.extra_fields[\"eo:bands\"]     band_index = {b[\"name\"]: i + 1 for i, b in enumerate(bands)}      # Need B03 (green) and B08 (nir)     g_idx = band_index[\"B03\"]     n_idx = band_index[\"B08\"]      # Sample the COG     try:         with rasterio.open(href) as src:             green = list(src.sample([(xc, yc)], indexes=g_idx))[0][0]             nir   = list(src.sample([(xc, yc)], indexes=n_idx))[0][0]     except:         continue      # Skip invalid values     if green &lt;= 0 or nir &lt;= 0:         continue      ndwi = (((green/100) - (nir/100)) / ((green/100) + (nir/100))) # Working on the reasoning that the data needs rescaling by 100       # Store result     dates.append(item.datetime)     ndwi_vals.append(float(ndwi))  <p>Finally we plot the data using <code>hvplot</code> which allows the user to interact with the chart.</p> In\u00a0[17]: Copied! <pre>df = pd.DataFrame({\n    \"date\": dates,\n    \"ndwi\": ndwi_vals\n})\n\ndf.hvplot.line(\n    x=\"date\",\n    y=\"ndwi\",\n    title=\"NDWI Timeseries at WWT London\",\n    ylabel=\"NDWI\",\n    xlabel=\"Date\",\n    marker=\"o\",\n    grid=True,\n    width=700,\n    height=350,\n)\n</pre> df = pd.DataFrame({     \"date\": dates,     \"ndwi\": ndwi_vals })  df.hvplot.line(     x=\"date\",     y=\"ndwi\",     title=\"NDWI Timeseries at WWT London\",     ylabel=\"NDWI\",     xlabel=\"Date\",     marker=\"o\",     grid=True,     width=700,     height=350, ) <pre>WARNING:param.main: marker option not found for line plot with bokeh; similar options include: []\nWARNING:param.main:marker option not found for line plot with bokeh; similar options include: []\n</pre> Out[17]: In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"documentation/training-materials/examples/Timeseries/#timeseries-plotting","title":"Timeseries Plotting\u00b6","text":""},{"location":"documentation/training-materials/examples/Timeseries/#exploring-the-resource-catalogue","title":"Exploring the Resource Catalogue\u00b6","text":"<p>Now we are ready to investigate the Resource Catalogue. First off, we need to import the packages we'll use.</p>"},{"location":"documentation/training-materials/examples/Timeseries/#timeseries","title":"Timeseries\u00b6","text":""},{"location":"documentation/troubleshooting/api-key/","title":"Troubleshooting API key issues","text":"<p>Bug</p> <p>If you continue to experience issues, then please contact us at enquiries@eodatahub.org.uk.</p>"},{"location":"documentation/troubleshooting/api-key/#check-you-are-using-the-correct-key","title":"Check you are using the correct key","text":"<p>Please make sure that the Token ID and API Key have been entered in the correct places.</p> <p>When you generate an API key from your Hub workspace, you will be given a Token ID and API key, as shown below. The API key should be used when connecting to your Hub workspace via the QGIS Plugin or Jupyter Notebooks. Always check you have copied across the correct character string when asked for your API key.</p> <p></p>"},{"location":"documentation/troubleshooting/api-key/#receiving-403-status-code-access-denied","title":"Receiving 403 Status Code: Access Denied","text":"<p>If you experience a 403 error when trying to order data through the notebook, please get a new Hub workspace API token and replace the old one you were previously using. This is a known reoccurring bug with API keys. The best workaround for this error is to get a new API key.</p> <p>This can be done by going to Workspaces &gt; Credentials &gt; Request New Token</p> <p></p>"},{"location":"documentation/troubleshooting/api-key/#check-the-expiry-date-on-your-currently-active-api-key","title":"Check the expiry date on your currently active API key","text":"<p>You can find this by visiting the workspaces tab and selecting credentials from the left-hand bar. Check the expiry date on the key. This can be viewed in your workspace within the Credentials tab, under Active Tokens. If it has expired, please create a new key.</p> <p></p>"},{"location":"documentation/troubleshooting/notebook-issues/","title":"Troubleshooting Notebook issues","text":"<p>Bug</p> <p>If you continue to experience issues, then please contact us at enquiries@eodatahub.org.uk.</p>"},{"location":"documentation/troubleshooting/notebook-issues/#file-loadsave-error-for","title":"File load/save error for  <p>This is a known bug and is being actively worked on. When attempting to open or save a file the error is shown. The user resolution is to refresh the browser page. For a file load error there is no risk to the user, the file should load successfully after refresh. For file save, there is a risk to lose recent data. This is mitigated by the autosave feature (enabled by default) but when refreshing the page any unsaved content will be lost, so save often until this bug has been resolved.</p> <p></p>","text":""},{"location":"documentation/troubleshooting/notebook-issues/#problems-authenticating-the-connection-to-your-workspace","title":"Problems authenticating the connection to your workspace <p>If you are struggling to connect to your workspace within the Jupyter notebooks environment, this could be related to the API key you are using to connect with, which you have likely stored within an environment file. Follow this guidance to troubleshoot issues related to API keys.</p>","text":""},{"location":"documentation/troubleshooting/workflows/","title":"Troubleshooting workflows","text":"<p>Bug</p> <p>If you continue to experience issues, then please contact us at enquiries@eodatahub.org.uk.</p>"},{"location":"documentation/troubleshooting/workflows/#why-is-my-workflow-failing-immediately","title":"Why is my workflow failing immediately?","text":"<p>This is often seen where the workflow deployment was successful but the required executables were not generated correctly. A common error response is that the Workflow Runner gives a \"ModuleNotFound\" error. In the first instance please try deleting and redeploying the workflow using the DELETE <code>/processes/workflow-id</code> endpoint and then redeploy. If this still doesn't work, please try deploying the workflow from a local file, rather than from a URL.</p> <p>Another common occurrence is the wrong inputs being provided. You can see warnings for missing inputs by checking either of the status or results endpoints for your job once it has started running. You can also check the required inputs by using the <code>/processes/&lt;process-id&gt;</code> endpoint for the process you wish to interrogate.</p>"},{"location":"documentation/troubleshooting/workflows/#why-are-my-outputs-not-being-handled-by-the-stageout-step","title":"Why are my outputs not being handled by the STAGEOUT step?","text":"<p>Often we have seen the case where the STAC type is defined as \"catalog\" but it needs to be \"Catalog\" to be fully conformant and to work with the STAGEOUT step. Also check your STAC output is fully compliant with the standards in the stac-spec.</p>"},{"location":"documentation/troubleshooting/workflows/#why-cant-i-access-my-logs","title":"Why can't I access my logs?","text":"<p>If you are receiving log links from the execute status endpoint but then get a 404 error when attempting to visit the page, please ensure your workflow ID meets the following requirements:</p> <ul> <li>Has no more than 26 characters</li> <li>Does not contain any underscores</li> </ul>"},{"location":"documentation/troubleshooting/workflows/#why-is-my-workflow-failing-with-few-log-details","title":"Why is my workflow failing with few log details?","text":"<p>Another common occurrence is to forget to specify the resources your workflow requires to run. If you miss this in your CWL specification, the workflow steps will run with the default setting which is 1 CPU and 1Gb of RAM. This may not be enough for the processing requested by your workflow and so you may need to specify increased limits. You can specify these limits in your CWL at either the Workflow or Command Line Tool level using the ResourceRequirements requirements object. This tells the Workflow Runner what resources to allocate to the pods running your workflow processes.</p> <p>For example, to specify a step needs 2 CPUs and 4Gb (3096 Mb) or RAM you can include the following in your requirements section:</p> <pre><code>requirements:\n- class: ResourceRequirement\n  coresMin: 2\n  ramMin: 4096\n</code></pre>"},{"location":"documentation/visualisation/configuring-requests/","title":"Configuring TiTiler requests","text":""},{"location":"documentation/visualisation/configuring-requests/#what-is-titiler","title":"What is TiTiler","text":"<p>TiTiler allows for dynamic and customisable visualisation of geospatial data. By adjusting parameters in a URL, you can control how your data is displayed. This guide will walk you through the key parameters, explaining what they do and how to use them with examples from the DataHub, including how to create visualisations for use in third-party software like QGIS.</p> <p>Requests are primarily made to OGC Tile endpoints in the format .../tiles/{z}/{x}/{y} where:</p> <ul> <li>{z} is the zoom level.</li> <li>{x} is the horizontal position of the tile.</li> <li>{y} is the vertical position of the tile.</li> </ul> <p>The following parameters can be added to the query string (after the ?) to customise the output.</p>"},{"location":"documentation/visualisation/configuring-requests/#working-with-different-data-types","title":"Working with Different Data Types","text":"<p>The DataHub hosts different types of data, and the TiTiler endpoint you use depends on the data format:</p> <ul> <li>Cloud Optimised GeoTIFFs (COGs): For single-band or multi-band raster data in .tif format. These requests use the /core/cog/ endpoint. This is the most common endpoint for visualising individual image files.</li> <li>Multidimensional Data (NetCDF, Zarr, Kerchunk): For data with more than two dimensions (eg. time, depth). These requests use the /xarray/ endpoint.</li> </ul> <p>Note</p> <p>While a /stac/ endpoint exists for interacting with STAC Items directly, for clear and direct visualisation, we recommend using the asset URL within the /cog/ or /xarray/ endpoints as shown in the examples below.)</p>"},{"location":"documentation/visualisation/configuring-requests/#for-single-band-data","title":"For Single-Band Data","text":"<p>Band/Variable Selection (bidx / variable):</p> <ul> <li>&amp;bidx=... for COGs (e.g. &amp;bidx=1 for the first band).</li> <li>&amp;variable=... for xarray data (e.g. &amp;variable=analysed_sst).</li> </ul> <p>Rescaling (rescale):</p> <ul> <li>&amp;rescale=MIN,MAX improves contrast by mapping the data's raw values (e.g. temperature) to the full colour range.</li> </ul> <p>Colourmap (colormap_name):</p> <ul> <li>&amp;colormap_name=... applies a colour palette (e.g inferno). This is ideal for single-band data.</li> </ul>"},{"location":"documentation/visualisation/configuring-requests/#example-esa-sea-surface-temperature-sst","title":"Example: ESA Sea Surface Temperature (SST)","text":"<p>This URL visualises the analysed_sst variable, rescales the data to a specific temperature range (271K to 305K), and applies the inferno colourmap.</p> <p>https://eodatahub.org.uk/titiler/xarray/tiles/{z}/{x}/{y}?url=https://eocis.org/data/sst-cdrv3-climatology/kerchunk/D321-ESACCI-L4_GHRSST-SSTdepth-Climatology-GLOB_CDR3.0-v02.0-fv01.0-kerchunk.json&amp;variable=analysed_sst&amp;rescale=271,305&amp;colormap_name=inferno</p>"},{"location":"documentation/visualisation/configuring-requests/#for-multi-band-colour-composites-eg-true-colour-images","title":"For Multi-Band Colour Composites (eg True Colour Images)","text":"<p>Software like QGIS often requires multiple bands to render a colour image. While colormap_name is great for single bands, it is not suitable for creating true-colour (RGB) composites. For this, you must use bidx to select multiple bands and color_formula to style them.</p> <p>Options:</p> <ul> <li>Band Selection (bidx):<ul> <li>Select the bands you want to map to Red, Green, and Blue. For Sentinel-2, the true-colour bands are Red (B4), Green (B3), and Blue (B2). You must specify them in order: &amp;bidx=4&amp;bidx=3&amp;bidx=2.</li> </ul> </li> <li>Colour Formula (color_formula):<ul> <li>This is a powerful parameter that applies colour correction algorithms to enhance the image. A good general-purpose formula is Gamma RGB [value] Saturation [value] Sigmoidal RGB [contrast] [brightness].</li> <li>The formula helps to brighten the image and increase contrast, which is often necessary for raw satellite data.</li> </ul> </li> </ul>"},{"location":"documentation/visualisation/configuring-requests/#example-true-colour-sentinel-2-image-for-qgis","title":"Example: True Colour Sentinel-2 Image for QGIS","text":"<p>This URL creates a visually enhanced, true-colour composite from a Sentinel-2 scene. It selects the Red, Green, and Blue bands and applies a gamma, saturation, and sigmoidal contrast stretch. This type of URL is robust for use in QGIS as an \"XYZ Tile\" layer. (Note: %20 is the URL-encoded version of a space character.)</p> <p>https://eodatahub.org.uk/titiler/core/cog/tiles/WebMercatorQuad/{z}/{x}/{y}?url=https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/21/S2B_20231121_latn536lonw0052_T30UUE_ORB123_20231121122846_utm30n_TM65_vmsk_sharp_rad_srefdem_stdsref.tif&amp;bidx=4&amp;bidx=3&amp;bidx=2&amp;color_formula=Gamma%20RGB%203.5%20Saturation%201.5%20Sigmoidal%20RGB%2015%200.55</p>"},{"location":"documentation/visualisation/configuring-requests/#using-titiler-with-qgis-wmts-capabilities","title":"Using TiTiler with QGIS: WMTS Capabilities","text":"<p>For more integration with desktop GIS, you can provide a WMTS Capabilities XML document. This allows QGIS to understand the layer, its projection and available parameters.</p> <p>You can construct a WMTS URL by taking a tile URL and changing the path to /WebMercatorQuad/WMTSCapabilities.xml. This URL will generate an XML file that you can load directly into QGIS as a WMTS layer. It contains the same multi-band selection and colour formula as the tile example above</p>"},{"location":"documentation/visualisation/configuring-requests/#example-wmts-for-a-true-colour-sentinel-2-image","title":"Example: WMTS for a True Colour Sentinel-2 Image","text":"<p>https://eodatahub.org.uk/titiler/core/cog/WebMercatorQuad/WMTSCapabilities.xml?url=https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/21/S2B_20231121_latn536lonw0052_T30UUE_ORB123_20231121122846_utm30n_TM65_vmsk_sharp_rad_srefdem_stdsref.tif&amp;bidx=4&amp;bidx=3&amp;bidx=2&amp;color_formula=Gamma%20RGB%203.5%20Saturation%201.5%20Sigmoidal%20RGB%2015%200.55</p>"},{"location":"documentation/visualisation/endpoints/","title":"TiTiler visualisation endpoints","text":"<p>This section describes the key TiTiler endpoints you\u2019ll use to visualise data within the EO DataHub. If you\u2019d like to explore them directly, visit the Swagger docs.</p>"},{"location":"documentation/visualisation/endpoints/#titiler-core-for-cog-and-geotiff","title":"TiTiler Core (For COG and GeoTIFF)","text":""},{"location":"documentation/visualisation/endpoints/#info","title":"/info","text":"<p>Returns metadata about the raster, such as its bounds, coordinate reference system (CRS), number of bands, and data type.</p> <p><code>https://eodatahub.org.uk/titiler/core/cog/info?url=https://some-bucket.org/myfile.tif</code></p>"},{"location":"documentation/visualisation/endpoints/#statistics","title":"/statistics","text":"<p>Provides statistics (min, max, mean, std) and histograms for each band. You can use this to determine rescale values for better visualisation.</p> <p><code>https://eodatahub.org.uk/titiler/core/cog/statistics?url=https://some-bucket.org/myfile.tif</code></p>"},{"location":"documentation/visualisation/endpoints/#tileszxy-default-webmercatorquad","title":"/tiles/{z}/{x}/{y} (default WebMercatorQuad)","text":"<p>Fetches a 256\u00d7256 tile at zoom level z, tile coordinates x/y, in Web Mercator projection (EPSG:3857). This is the classic \u201cXYZ\u201d tile approach. You can change the projection using <code>/tiles/{tileMatrixSetId}/{z}/{x}/{y}</code> (e.g., WebMercatorQuad, WorldCRS84Quad)</p> <p><code>https://eodatahub.org.uk/titiler/core/cog/tiles/5/15/10?url=https://some-bucket.org/myfile.tif&amp;bidx=1&amp;rescale=0,255</code></p>"},{"location":"documentation/visualisation/endpoints/#map","title":"/map","text":"<p>Returns an interactive HTML page that displays a web map for the specified dataset. You can see the bounding box, zoom in/out and adjust styling parameters.</p> <pre><code>https://eodatahub.org.uk/titiler/core/cog/map\n    ?tile_scale=1\n    &amp;url=https%3A%2F%2Fdap.ceda.ac.uk%2F...\n    &amp;bidx=1\n    &amp;rescale=100%2C200\n</code></pre>"},{"location":"documentation/visualisation/endpoints/#preview","title":"/preview","text":"<p>Returns a small preview image of the entire dataset\u2019s extent (e.g. 512\u00d7512). Good for quickly inspecting how the data looks.</p> <p><code>https://eodatahub.org.uk/titiler/core/cog/preview?url=https://some-bucket.org/myfile.tif&amp;bidx=1&amp;colormap_name=viridis</code></p>"},{"location":"documentation/visualisation/endpoints/#titiler-xarray-multi-dimensional","title":"TiTiler XArray (Multi-dimensional)","text":""},{"location":"documentation/visualisation/endpoints/#xarrayinfo","title":"/xarray/info","text":"<p>Returns information about the dataset\u2019s variables, dimensions, and attributes</p>"},{"location":"documentation/visualisation/endpoints/#xarraytileszxy","title":"/xarray/tiles/{z}/{x}/{y}","text":"<p>Retrieves tiles in Web Mercator (EPSG:3857) by default. You can specify:</p> <ul> <li>&amp;variable= to select a particular variable</li> <li>&amp;multiscale= for multi-resolution data</li> <li>&amp;decode_times= for time-indexed data</li> </ul> <pre><code>https://eodatahub.org.uk/titiler/xarray/tiles/5/15/10\n    ?url=https://mybucket.org/path/to/dataset.zarr\n    &amp;variable=temperature\n    &amp;rescale=0,300\n    &amp;colormap_name=magma\n</code></pre>"},{"location":"documentation/visualisation/endpoints/#preconfigured-wmts-for-mosaic-datasets","title":"Preconfigured WMTS (for Mosaic Datasets)","text":"<p>For certain public datasets, the EO DataHub team provides a fully mosaiced WMTS layer. Rather than visualising individual scenes, you get a mosaic covering a region (e.g. an entire country) or time period.</p>"},{"location":"documentation/visualisation/endpoints/#how-we-set-it-up","title":"How We Set It Up","text":"<p>In a STAC Collection\u2019s JSON, we add a \"renders\" section which tells TiTiler how to combine assets, choose bands, rescale them, and define tile levels. For example:</p> <pre><code>{\n  \"renders\": {\n    \"rgb\": {\n      \"title\": \"RGB\",\n      \"assets\": [\"cog\"],\n      \"bidx\": [1, 2, 3],\n      \"rescale\": [\n        [0, 100],\n        [0, 100],\n        [0, 100]\n      ],\n      \"resampling\": \"nearest\",\n      \"tilematrixsets\": {\n        \"WebMercatorQuad\": [0, 30]\n      }\n    }\n  }\n}\n</code></pre> <p>When this is configured, TiTiler recognises it can serve a mosaic of all relevant scenes for that date or dataset.</p>"},{"location":"documentation/visualisation/endpoints/#accessing-the-wmts","title":"Accessing the WMTS","text":""},{"location":"documentation/visualisation/endpoints/#getcapabilities-document","title":"GetCapabilities Document","text":"<p>Append <code>/wmts?request=GetCapabilities&amp;service=WMTS</code> to the catalogue URL for the chosen dataset. For example:</p> <pre><code>https://eodatahub.org.uk/api/catalogue/stac/catalogs/supported-datasets/ceda-stac-catalogue/wmts\n    ?request=GetCapabilities\n    &amp;service=WMTS\n</code></pre> <p>This returns the WMTS GetCapabilities XML, describing available layers.</p>"},{"location":"documentation/visualisation/endpoints/#add-to-gis","title":"Add to GIS","text":"<p>In QGIS or another tool, add a WMTS layer by pointing to this URL. You can then select the layer (e.g. sentinel2_ard for a particular date), and the software will automatically fetch the mosaic tiles.</p>"},{"location":"documentation/visualisation/endpoints/#dataset-coverage","title":"Dataset Coverage","text":"<p>Different public datasets might have their own mosaic endpoints. Keep an eye on the EO DataHub documentation for a list of which catalogues and datasets are available in this manner.</p>"},{"location":"documentation/visualisation/introduction/","title":"Introducing data visualisation","text":""},{"location":"documentation/visualisation/introduction/#what-is-titiler","title":"What is TiTiler","text":"<ul> <li>TiTiler serves geospatial data as interactive web tiles.</li> <li>Supports Cloud Optimised GeoTIFFs (COGs), STAC items, Kerchunk, NetCDF and Zarr.</li> <li>Enables dynamic map rendering without downloading entire datasets.</li> </ul> <p>Tip</p> <p>Useful Links</p> <p>TiTiler Documentation | EO DataHub TiTiler Swagger API</p> <p>Data visualisation in EO DataHub focuses on turning raw Earth Observation data into map tiles and imagery that can be easily viewed and analysed. TiTiler is the tool that we use to power this.</p> <p>TiTiler is a dynamic tile server created by DevelopmentSeed built on FastAPI and Rasterio/GDAL, designed to support cloud-optimized geospatial data and serve it as web map tiles (Building a Dynamic Tile Server Using Cloud Optimized GeoTIFF(COG) with TiTiler). In practice, this means you can take a large geospatial file (like a satellite image or a multi-band dataset) and TiTiler will handle just the portion needed for the current map view.</p>"},{"location":"documentation/visualisation/introduction/#key-features-of-titiler","title":"Key Features of TiTiler","text":"<p>TiTiler supports Cloud Optimised GeoTIFFs (COGs), as well as STAC (SpatioTemporal Asset Catalog) items and even multi-dimensional data via Xarray. It can handle multiple projections (through OGC TileMatrixSets) and offers various output image formats (PNG, JPEG, GeoTIFF, etc.) (TiTiler). It also implements OGC WMTS standards for integration with GIS clients.</p> <p>Note</p> <p>For more details on TiTiler\u2019s design and capabilities, the EO DataHub instance of TiTiler has its API documented via Swagger at EO DataHub TiTiler API for those interested in the raw REST endpoints.</p>"},{"location":"documentation/visualisation/preparing-data/","title":"Preparing data for visualisation","text":"<p>Learn recommended data formats (COG, Kerchunk, Zarr) for efficient visualisation in EO DataHub.</p> <p>Formats Supported:</p> <ul> <li>Cloud Optimised GeoTIFF (COG): ideal for imagery data.</li> <li>NetCDF and Zarr: for multi-dimensional datasets.</li> <li>Kerchunk references: Optimises large NetCDF or HDF5 files.</li> </ul> <p>Recommendations:</p> <ul> <li>Convert GeoTIFF to COG for faster access.</li> <li>Use Zarr or Kerchunk for NetCDF files to improve performance.</li> </ul> <p>If you plan to use TiTiler to visualise outputs created by the Workflow Runner, it's important to prepare your data in a format that ensures efficient access and display.</p> <p>Here we discuss two common considerations: COG vs GeoTIFF for imagery and Kerchunk vs NetCDF for multi-dimensional data.</p>"},{"location":"documentation/visualisation/preparing-data/#cloud-optimised-geotiff-cog-vs-standard-geotiff","title":"Cloud Optimised GeoTIFF (COG) vs standard GeoTIFF","text":"<p>A GeoTIFF is a georeferenced raster image format. A COG is a regular GeoTIFF that has been internally organised for efficient cloud-based access. COGs are tiled and have built-in overviews - this means that when TiTiler requests a portion of the image, a COG allows the server to fetch just the relevant tiles instead of the whole file.</p> <p>A traditional GeoTIFF is not optimised for cloud access might require reading a large chunk of the file even for a small area or zoom level. When attempting to view a GeoTIFF using TiTiler, you may notice TiTiler responses taking considerably longer compared to COGs.</p> <p>To check if your TIFF is a GeoTIFF or COG, you can use this Python function to locally load the file and check: Python Script to check if a TIFF is a COG</p> <p>Tip</p> <p>if you plan to visualise large raster data on DataHub, converting it to a COG is highly recommended.</p> <p>Many GIS tools can convert GeoTIFFs to COG (by re-saving with internal tiling and overviews).</p>"},{"location":"documentation/visualisation/preparing-data/#kerchunk-vs-netcdf-and-other-data-formats","title":"Kerchunk vs NetCDF (and other data formats)","text":"<p>If your data is multi-dimensional (for example, time series or 3D variables) often stored in formats like NetCDF, you should consider how to optimise it for cloud access. TiTiler's XArray extension can read NetCDF or Zarr datasets directly, but reading a raw NetCDF over HTTP can be slow, especially if the file is large and not internally tiled.</p> <p>Kerchunk is a technique that creates an external reference file (JSON) describing the byte ranges of data inside a NetCDF \u2013 treating it like a virtual Zarr store. Using a kerchunk-generated reference, TiTiler (via XArray) can access just the needed chunks of data from cloud storage without loading the entire file. This approach is useful for huge climate or model datasets.</p>"},{"location":"documentation/visualisation/preparing-data/#summary-preparing-your-data","title":"Summary \u2013 preparing your data","text":"<ul> <li>Use COGs for imagery (each band or product as a separate COG file, or as assets in a STAC catalog) whenever possible for raster data.</li> <li>For multi-band or multi-variable datasets (like scientific data cubes), consider using Zarr or Kerchunk references so that TiTiler can read them chunk-wise.</li> <li>Ensure your data is in a supported projection (commonly Web Mercator EPSG:3857 or WGS84 EPSG:4326) or be aware that TiTiler will reproject on the fly for tiling.</li> <li>Also verify the data has proper georeferencing (coordinate reference system and bounds), as TiTiler relies on this info to position tiles correctly.</li> </ul>"},{"location":"documentation/visualisation/serving-data/","title":"Serving data with TiTiler","text":""},{"location":"documentation/visualisation/serving-data/#titiler-extensions-in-datahub","title":"TiTiler Extensions in DataHub:","text":"<ul> <li>Core: Directly visualise individual raster files (COGs).</li> <li>STAC: Use STAC items directly for quick asset visualisation.</li> <li>XArray: Supports multi-dimensional NetCDF and Zarr datasets.</li> </ul>"},{"location":"documentation/visualisation/serving-data/#using-titiler-for-visualisation","title":"Using TiTiler for Visualisation:","text":"<ul> <li>Serve Public Data hosted on the Platform</li> <li>Serve Public Data hosted on third-party websites</li> <li>Serve Private Data including Workflow outputs and Commercially ordered data</li> </ul> <p>Depending on where your data resides and who should access it, there are slight differences in how you use TiTiler.</p>"},{"location":"documentation/visualisation/serving-data/#public-data","title":"Public Data","text":"<p>Public being that the data is openly accessible via a URL without special credentials (for example, a COG on a public AWS S3 bucket or an open HTTP link). To visualise public data, you can directly use TiTiler\u2019s endpoints with the data\u2019s URL. For instance, if you have a COG available at https://some-domain.com/mydata.tif, you can use the DataHub TiTiler endpoint to request tiles or a preview by providing this URL (via a url= query parameter). No authentication is required in this case, and anyone with access to the DataHub platform can potentially visualise that data.</p>"},{"location":"documentation/visualisation/serving-data/#example-1-a-preview-of-a-public-sentinel-2-ard-cog-from-the-ceda-archive","title":"Example 1: A Preview of a Public Sentinel 2 ARD COG from the CEDA Archive","text":"<p><code>https://eodatahub.org.uk/titiler/core/cog/preview?url=https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2023/11/21/S2B_20231121_latn536lonw0052_T30UUE_ORB123_20231121122846_utm30n_TM65_vmsk_sharp_rad_srefdem_stdsref.tif&amp;bidx=1&amp;rescale=0%2C255&amp;colormap_name=bone</code></p> <p></p>"},{"location":"documentation/visualisation/serving-data/#example-2-an-xyz-tile-request-of-eocis-sea-surface-temperature","title":"Example 2: An XYZ Tile Request of EOCIS Sea-Surface Temperature","text":"<p><code>https://eodatahub.org.uk/titiler/xarray/tiles/3/6/4?url=https://eocis.org/data/sst-cdrv3-kerchunk/2024/03/20240331120000-ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_ICDR3.0-v02.0-fv01.0-kerchunk.json&amp;variable=analysed_sst&amp;tileMatrixSetId=WebMercatorQuad&amp;scale=1&amp;multiscale=false&amp;reference=true&amp;decode_times=true&amp;consolidated=false&amp;rescale=271%2C305&amp;colormap_name=twilight&amp;return_mask=true</code></p> <p></p>"},{"location":"documentation/visualisation/serving-data/#private-data","title":"Private Data","text":"<p>Private data refers to files that are not publicly accessible \u2013 for example, data in your EO DataHub workspace object or block store. DataHub allows you to visualise these too, but you need to authenticate so that TiTiler can securely fetch the data on your behalf.</p> <p>Typically, DataHub uses a workspace-scoped API Key. When you sign up to DataHub, you can retrieve an API key that represents your credentials inside the Workspace page (See nav bar at the top). To use TiTiler with private data, you will include this API key as a Bearer header in your requests. Providing this key proves to the system that you have permission to access the data in your workspace.</p>"},{"location":"documentation/visualisation/serving-data/#example-3-a-private-zarr-stored-in-a-users-workspace","title":"Example 3: A Private ZARR stored in a user's workspace","text":"<pre><code>curl 'https://eodatahub.org.uk/titiler/xarray/tiles/WebMercatorQuad/5/15/10?url=https://workspaces-eodhp-staging.s3.eu-west-2.amazonaws.com/&lt;YOUR-WORKSPACE&gt;/xarray-zarr-directory/&amp;variable=data&amp;scale=1&amp;multiscale=false&amp;reference=false&amp;decode_times=true&amp;consolidated=false&amp;rescale=0,255&amp;return_mask=true&amp;colormap_name=magma' \\\n  -H 'Authorization: Bearer &lt;WORKSPACE API KEY&gt;' \\\n  --output test.png\n</code></pre>"},{"location":"documentation/visualisation/third-party-tools/","title":"TiTiler and third-party tools","text":"<p>TiTiler\u2019s outputs are standard web map images, so many third-party tools can work with them.</p>"},{"location":"documentation/visualisation/third-party-tools/#qgis-desktop","title":"QGIS Desktop","text":"<p>You can add layers from the EO DataHub directly into QGIS. For public raster data, we recommend using the WMTS endpoint when available. For individual files, you can use an XYZ Tile connection.</p> <p>We support many different datatypes (GeoTIFF, COG, Kerchunk, ZARR and NetCDF). To begin, please ensure that the format of what you want to visualise corresponds with one of those file formats.</p> <p>You will then need to use our platform-hosted TiTiler to generate an XYZ Tile URL. You can use this notebook to learn how to construct the URLs. Alternatively, you can refer to the official TiTiler documentation.</p>"},{"location":"documentation/visualisation/third-party-tools/#adding-as-an-xyz-layer-for-single-cog-files","title":"Adding as an XYZ Layer (for single COG files)","text":"<p>In QGIS, navigate to Layer -&gt; Add Layer -&gt; Add XYZ Layer.</p> <p></p> <p>To ensure compatibility, it's best to create a true color RGB image by specifying three bands (for natural color in a Sentinel-2 image).</p> <p>Enter a URL formatted like the example below. Notice it uses bidx=4&amp;bidx=3&amp;bidx=2 to select the Red, Green, and Blue bands and a color_formula to enhance the image.</p> <p><code>https://eodatahub.org.uk/titiler/core/cog/tiles/WebMercatorQuad/{z}/{x}/{y}?url=https://dap.ceda.ac.uk/neodc/sentinel_ard/data/sentinel_2/2025/05/12/S2C_20250512_latn590lonw0055_T30VUL_ORB023_20250512170944_utm30n_osgb_vmsk_sharp_rad_srefdem_stdsref.tif&amp;bidx=1&amp;color_formula=Gamma%20RGB%206%20Saturation%200.8%20Sigmoidal%20RGB%2025%200.35&amp;bidx=2&amp;bidx=3</code></p> <p></p> <p>If you are serving data from your private Workspace, then:</p> <ol> <li>Add in your Authentication Configuration by clicking the green arrow next to Configurations.</li> <li>Give it a name and select API Header from the dropdown.</li> <li>As the Header key: authorization</li> <li>As the Header value: Bearer  <p>Tip</p> <p>If you don't have an API token, visit the Workspaces -&gt; Credentials -&gt; New API Token to generate a workspace-scoped token.</p> <p></p> <p>Save the dialog boxes and add the layer which should now be serving the data directly from the platform.</p>"},{"location":"documentation/visualisation/third-party-tools/#arcgis-pro","title":"ArcGIS Pro","text":"<p>The following guide walks through how to pull your Hub data into ArcGIS Pro via TiTiler.</p> <p>Within your ArcGIS Pro map project, navigate to the Layer group in the Map tab, and find Add Data.</p> <p></p> <p>From the dropdown menu, select From Path.</p> <p></p> <p>The Add Data From Path pop out will appear. Input your URL into the text box. You can use the following example link to try this out with some open land cover data.</p> <p></p> <p><code>https://eodatahub.org.uk/titiler/core/cog/tiles/WebMercatorQuad/{z}/{x}/{y}?scale=1&amp;url=https://dap.ceda.ac.uk/neodc/esacci/land_cover/data/land_cover_maps/v2.0.7/ESACCI-LC-L4-LCCS-Map-300m-P1Y-2015-v2.0.7.tif&amp;bidx=1&amp;rescale=0%2C300&amp;colormap_name=rainbow</code></p> <p>Leave the Service Type as Automatic, and click Add. You shouldn\u2019t need to add any Custom Request Parameters. Your image should then be visualised as a layer within the Contents panel. If the data doesn\u2019t appear on the map immediately, the image could be taking a while to render. Try Zooming in and out on the map to trigger the rendering of the data, and soon your image should be loaded into the Map pane.</p> <p></p>"},{"location":"documentation/visualisation/third-party-tools/#leaflet-openlayers-web-mapping-libraries","title":"Leaflet / OpenLayers (Web Mapping Libraries)","text":"<p>If you\u2019re building a custom web app, you can use Leaflet\u2019s L.TileLayer with the URL template pointing to DataHub\u2019s TiTiler. For example:</p> <pre><code>L.tileLayer('https://eodatahub.org.uk/titiler/cog/tiles/{z}/{x}/{y}?url=&lt;...&gt;&amp;rescale=...').addTo(map);\n</code></pre> <p>This would integrate your dataset as a layer on a Leaflet map. OpenLayers and MapLibre GL have similar capabilities for XYZ tiles. Remember to include any required parameters (like API key, band selection, etc.) in the URL template.</p>"},{"location":"documentation/visualisation/third-party-tools/#python-api-usage","title":"Python API usage","text":"<p>If you want to retrieve a tile or image programmatically (say in a Python notebook for analysis), you can use the requests library to call the TiTiler endpoint.</p> <p>You can use this Notebook for reference: https://github.com/EO-DataHub/eodh-training/blob/main/presentations/Workshop/3_202502_workshop_DataVis.ipynb</p> <p>For example:</p> <pre><code>import requests\n\nTITILER_PREVIEW_URL = f'https://eodatahub.org.uk/titiler/core/cog/preview'\nTITILER_PREVIEW_PARAMS = {\n    'url': WORKFLOW_OUTPUT_ASSET,\n    'bidx': 1,\n    'rescale': '9,255',\n    'colormap_name': 'rain_r'\n}\n\nresp = requests.get(url, params=params)\nwith open(\"preview.jpg\", \"wb\") as f:\n    f.write(resp.content)\n</code></pre> <p>This would download a preview JPEG of the specified image. The /preview endpoint returns a single composite image showing the whole dataset area. If you want tiles, you\u2019d use the <code>/tiles/...</code> URL as shown in earlier examples.</p>"},{"location":"documentation/visualisation/third-party-tools/#using-folium","title":"Using Folium","text":"<p>You can actually stream XYZ tiles directly from Python using Folium within a notebook. For example, if you want to stream a COG Asset you could generate one like so:</p> <p>'''py COG_PREVIEW_PARAMS = {     'url': sentinel2_ard_cog_asset.href,     'bidx': 1,     'rescale': '9,255',     'colormap_name': 'rain_r' }</p> <p>COG_OGC_URL = 'https://eodatahub.org.uk/titiler/core/cog/tiles/WebMercatorQuad/{z}/{x}/{y}'</p> <p>COG_XYZ = COG_OGC_URL + '?' + '&amp;'.join([f'{k}={v}' for k, v in COG_PREVIEW_PARAMS.items()])</p>"},{"location":"documentation/visualisation/third-party-tools/#create-the-folium-map","title":"Create the Folium Map","text":"<p>m = folium.Map(location=[54.5, -4.5], zoom_start=6)</p>"},{"location":"documentation/visualisation/third-party-tools/#add-the-titiler-layer","title":"Add the TiTiler layer","text":"<p>folium.raster_layers.TileLayer(     tiles=COG_XYZ,     attr=\"TiTiler\",     name=\"Sentinel 2 ARD Scene\",     overlay=True ).add_to(m)</p>"},{"location":"documentation/visualisation/third-party-tools/#add-a-layer-control","title":"Add a layer control","text":"<p>folium.LayerControl().add_to(m)</p>"},{"location":"documentation/visualisation/third-party-tools/#display-the-map","title":"Display the map","text":"<p>m <pre><code>Similarly for Multi-Dimensional Datasets (ZARR, NetCDF, Kerchunk), the only difference would be a slight adjustment to the URLS, see:\n\n```py\nXARRAY_PARAMS = {\n    'url': cmip6_kerchunk_asset.href,\n    'variable': 'rsus',\n    'rescale': '-50,100',\n    'colormap_name': 'plasma',\n    'reference' : 'true'\n}\n\nXARRAY_OGC_URL = 'https://eodatahub.org.uk/titiler/xarray/tiles/WebMercatorQuad/{z}/{x}/{y}'\n\nXARRAY_XYZ = XARRAY_OGC_URL + '?' + '&amp;'.join([f'{k}={v}' for k, v in XARRAY_PARAMS.items()])\n\n# Create the Folium Map\nm = folium.Map(location=[54.5, -4.5], zoom_start=6)\n\n# Add the TiTiler layer\nfolium.raster_layers.TileLayer(\n    tiles=XARRAY_XYZ ,\n    attr=\"TiTiler\",\n    name=\"Sentinel 2 ARD Scene\",\n    overlay=True\n).add_to(m)\n\n# Add a layer control\nfolium.LayerControl().add_to(m)\n\n# Display the map\nm\n</code></pre></p> <p>If you are serving Private datasets from a Workspace, then make sure to include the DataHub API key you got from the Workspaces like so:</p> <p><code>headers={'Authorization': f'Bearer {WORKSPACE_API_KEY}'}</code></p>"},{"location":"documentation/workflow-runner/access/","title":"Workflow workspace access","text":"<p>While workflow inputs can be freely defined in the CWL definition, and the workflow itself can load and process data however required, including loading it from external sources, the Workflow Runner also provides some advanced functionality to load inputs from various sources more simply, including loading private workspace data that is hosted on the Hub.</p> <p>The Workflow Runner provides functionality to load STAC Items in an additional STAGEIN step that is run ahead of any workflow steps, whenever a particular input type is used in the workflow. We will discuss this additional functionality later in this page.</p> <p>The simplest example is just to pass in a public URL pointing to a STAC item as a Directory input, the Workflow Runner will call that URL and download the data as a STAC Catalog, provided it is public, ready for processing. This URL can of course be data taken directly from the DataHub Resource Catalog, for example this Sentinel2_ARD item could be used as an input.</p> <p>However, if you wish to pass in data from your own workspace, for example when trying to process private data, the WR supports passing in data files from both S3 Object Stores and AWS Block Stores.</p>"},{"location":"documentation/workflow-runner/access/#workflow-runner-stage-in","title":"Workflow Runner Stage In","text":"<p>In order to make sure the STAGEIN step is run for your workflow, you need to define an input with the type set to <code>Directory</code>.</p> <pre><code>inputs:\n  stac:\n    label: the image to convert as a STAC item\n    doc: the image to convert as a STAC item\n    type: Directory\n</code></pre> <p>This will trigger the STAGEIN step to run before your workflow executes, loading and saving the STAC Item locally in it's own STAC catalog.</p> <p>The simplest example when using the Workflow Runner STAGEIN step is to define the STAC Item via a public URL. An example workflow can be found here. Note, the STAC Item must be available publicly (as JSON), as well as any assets within the item you wish to download for processing.</p> <pre><code>{\n  \"inputs\": {\n    \"stac\": \"https://raw.githubusercontent.com/EOEPCA/convert/refs/heads/main/stac/eoepca-logo.json\"\n  }\n}\n</code></pre> <p>In our first example we will pass in a public URL pointing to a STAC item as a Directory input, the Workflow Runner will make a request to that URL and download the data, constructing a new STAC Catalog, ready for processing in your workflow steps. The directory containing this new STAC Catalog will then be passed as an input to your workflow, which can then find and use the Catalog dataset.</p> <pre><code>stacItemFile=\"$(cat \"${dir}/catalog.json\" | jq -r '[.links[] | select(.rel == \"item\")][0].href')\"\n</code></pre> <p>This URL can also be for data available publicly on the DataHub, for example this Sentinel2_ARD item could be used as an input in the same way.</p> <p>However, if you wish to pass in data from your own workspace, for example when trying to process private data in your block or object stores, or in your private catalog within the Resource Catalogue, the Workflow Runner can also make authorized requests to these locations, without any additional user configuration.</p> <p>Info</p> <p>When we refer to \"your workspace\" we mean the workspace you are using to execute the workflow, i.e. the one which you are authenticated as when calling the execution endpoint. This does not depend on who deployed the workflow, only who is executing it, should you be calling a public workflow.</p> <p>Given the same <code>stac</code> example above, you could also pass in a STAC item from your Resource Catalog private catalog, which will be prefixed by <code>api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;</code>.</p> <pre><code>{\n  \"inputs\": {\n    \"stac\": \"https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/catalogs/processing-results/catalogs/path/to/stac/item\"\n  }\n}\n</code></pre> <p>Or you can pass in data from your S3 Object Storage by providing an S3 URI, prefixed with the bucket name and workspace name, for example <code>s3://workspaces-eodhp/&lt;workspace&gt;/...</code>.</p> <pre><code>{\n  \"inputs\": {\n    \"stac\": \"s3://workspaces-eodhp/&lt;workspace&gt;/item.json\"\n  }\n}\n</code></pre> <p>You can also mount Object Store data via HTTPS, using the EODatahub workspaces domain.</p> <pre><code>{\n  \"inputs\": {\n    \"stac\": \"https://&lt;workspace&gt;.eodatahub-workspaces.org.uk/files/workspaces-eodhp/path/to/item.json\"\n  }\n}\n</code></pre> <p>You can also stage data directly from your Block Store, which is mounted to the STAGEIN pod as a file system. This data is mounted at the <code>/workspace/pv-&lt;workspace&gt;/</code> path, so you can access your data by providing a file path prefixed with such path.</p> <pre><code>{\n  \"inputs\": {\n    \"stac\": \"/workspace/pv-&lt;workspace&gt;/path/to/item.json\"\n  }\n}\n</code></pre>"},{"location":"documentation/workflow-runner/access/#accessing-data-within-workflows","title":"Accessing Data within Workflows","text":"<p>We have discussed above how using the STAGEIN functionality of the Workflow Runner can allow you to mount private STAC data into your workflow for processing. However, you can also load private data directly within your workflows, without relying on the STAGEIN step and also allowing you to load non-STAC data.</p>"},{"location":"documentation/workflow-runner/access/#accessing-s3-data-within-your-workflows","title":"Accessing S3 data within your workflows","text":"<p>The Workflow Runner automatically assigns AWS access credentials to your workflow during execution. This means your workflow will automatically assume these credentials and you are free to use them within your workflow steps. For example, you can instantiate a Boto3 client as in the below example, without specifying any credentials inputs to the function.</p> <pre><code>import boto3\n\n# Access is granted based on the service account of the running pod\ns3_client = boto3.Client(\"s3\")\n</code></pre>"},{"location":"documentation/workflow-runner/access/#accessing-workspace-data-within-your-workflows-via-https","title":"Accessing Workspace data within your workflows via HTTPS","text":"<p>The Workflow Runner automatically generates and provides credentials to access your workspace data within your workflow steps via HTTPS, either via access to S3 using HTTPS or through the Resource Catalogue API. These credentials are available in the Workflow Runner and need to be requested as an environment variable, <code>WORKSPACE_TOKEN</code>, within your workflow steps, so you will need to defined this variable as in this example. Ensure</p> <p>This access is granted based on an additional workspace access token input, which is scoped to the workspace executing the workflow (whether executing a standard workflow or a user service). So first make sure your workflow sets this environment variable in its CWL definition.</p> <p>Info</p> <p>To test this workflow locally, you can hardcode a token into the CWL script, rather than setting it as <code>&lt;&lt;REPLACE&gt;&gt;</code> as is required by the workflow runner to replace this variable.</p> <p>To set the environment variable in your CWL, copy the following extract. This will set the <code>WORKSPACE_TOKEN</code> environment variable in your workflow container which you can use for any authenticated HTTPS requests to the Hub.</p> <pre><code>EnvVarRequirement:\n   envDef:\n     WORKSPACE_TOKEN: &lt;&lt;REPLACE&gt;&gt;\n</code></pre> <p>You can then load and pass this token in the authorization header as a Bearer token, when making HTTPS requests to the Hub that need to be authorized. For example when accessing private workspace data in your catalog in the Resource Catalogue. A full workflow and Python script to test this access is provided here.</p> <pre><code>import os\nimport requests\n\nWORKSPACE_ACCESS_TOKEN = os.getenv(\"WORKSPACE_TOKEN\", \"your_api_token_here\")\n\nurl = \"https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/path/to/item\"\nresponse = requests.get(url, headers={\"Authorization\": f\"Bearer {WORKSPACE_ACCESS_TOKEN}\"})\n</code></pre>"},{"location":"documentation/workflow-runner/access/#accessing-workspace-data-within-your-workflows-via-mounted-block-store","title":"Accessing Workspace data within your workflows via Mounted Block Store","text":"<p>The Workflow Runner also mounts your workspace Block Store into each step of your workflow. This means you can access files in your Block Store just as if using local files within your workflow.</p> <p>The Block Store is mounted at a specific file prefix, <code>/workspace/pv-&lt;workspace&gt;/...</code>, so you can access files here just as you would any other files in your workflow. This code first loads the files in the Block Store mount and then loads an example JSON file as a dictionary. You could add some logging statements to this example to print out the available files and file contents.</p> <pre><code>import os\n\nworkpace = \"&lt;workspace&gt;\"\n\nblock_store_dir = f\"/workspace/pv-{workspace}\"\n\nfiles = os.listdir(block_store_dir)\n\nif files:\n  with open(f\"{block_store_dir}/file.json\", \"r\") as f:\n    json_file = f.read()\n    json_dict = json.loads(json_file)\n</code></pre>"},{"location":"documentation/workflow-runner/access/#managing-datasets-in-your-workspace","title":"Managing datasets in your workspace","text":"<p>To first manage datasets in your workspace Object and Block Stores you can use the Notebooks (Jupyter Notebooks) application on the Hub. This allows you to upload data and create directories to organize your data as you wish. Make sure you open the Notebook in the correct workspace, which you can select from the drop-down at the top when starting your server.</p> <p>You are also able to harvest datasets into your catalog within the Resource Catalogue, using the data loader functionality, available in the left-side menu on the workspaces UI. This will allow you to load STAC directly into your catalog and access this via the Resource Catalogue API.</p> <p>Once you have the files you want to use in your Stores or Resource Catalogue you can construct a workflow that accesses this data either:</p> <ul> <li>Via STAGEIN when loading STAC Items for processing</li> <li>Via workflow access using S3 credentials, HTTPS or Block Store mounts</li> </ul>"},{"location":"documentation/workflow-runner/application-packages/","title":"Application packages","text":"<p>On the EO DataHub Workflow are defined in the form of OGC Application Packages. These packages require a script to be defined in Common Workflow Language (CWL), which is a YAML formatted file defining a service as a sequence of steps, including inputs and ouputs, both globally for the entire workflow, as well as those for each step within the workflow. Most CWL files will include references to Docker images specifying the processing that is done in each step of the workflow.</p>"},{"location":"documentation/workflow-runner/application-packages/#set-up","title":"Set up","text":"<p>An example CWL script, defining a Hub-compliant workflow, can be found here. This script takes as inputs: a URL to an image file, a function name and a scale factor, provided as a percentage. It then executes the chosen function on the image. In this case this function is a resize by the provided scale factor. This image is then output, along with a STAC catalogue defining the resulting image asset.</p> <p>Some best practices when creating application packages can be found here and include requirements that a workflow needs to comply with in order to work correctly with the Workflow Runner API. These practices should be followed when considering the type of inputs and outputs your workflow will produce, as well as how these are handled by the workflow upon completion.</p> <p>Specific restrictions of note which must be followed:</p> <ul> <li>Your final outputs must be captured in a <code>glob</code> command and must capture a directory containing a catalog.json file as well as any other outputs you wish to export, including STAC items and assets, such as GeoTIF or NetCDF files.</li> <li>You may define defaults in the workflow but these are not currently used during execution, so you must also provide these values again when executing a workflow.</li> <li>It is recommended you include resource requirements as defined below to ensure your workflow is allocated sufficient resources to run correctly on the Hub. By default this is set to 1 CPU and 1 GB of RAM. Note, that if your workflow attempts to assume more resources than defined, the workflow will fail with an Out of Memory error, so ensure you specify enough RAM. Increased resources could also lead to increased costs incurred by the execution request.</li> <li>Your workflow must produce a catalog.json STAC Catalog which points to your generated STAC items, these must be captured in the output of the CWL using a \"glob\".</li> <li>CWL inputs are passed as command line strings and there is a limit in how long these can be. Currently on the cluster this is 2621440 bytes, meaning longer command line arguments will lead to errors. Instead you should attempt to load such data from a file, either in S3 or your workspace Block Store, for example using the STAGEIN functionality discussed later. B elow is an example of defining resource requirements for a workflow step, here the user is specifying a limit of 1 CPU core and 512 MB of RAM. These can be specified for each step in your workflow and you can increase these as required.</li> </ul> <pre><code>- class: CommandLineTool\n    id: convert\n    requirements:\n      ResourceRequirement:\n        coresMax: 1\n        ramMax: 512\n</code></pre> <p>The Workflow Runner also includes two built-in steps that are run automatically, when required, for each workflow.</p> <ul> <li>A STAGEIN step which is run whenever a defined workflow includes a Directory definition pointing to a STAC item. This step extracts the STAC data from the provided location and downloads it ready for local processing. The data provided in the Directory input can come from either a URL, workspace Block Store or S3 Object store. Any Directory inputs must point to a STAC item (or Feature) and the STAGEIN step then extracts all Assets within this Item and constructs a new STAC Catalog containing the Item and any assets. This new Catalog is indexed locally with local file hrefs. See this example workflow that will invoke the STAGEIN step providing a URL to a STAC Feature.</li> <li>A STAGEOUT step which is always run for a workflow and has the responsibility of extracting the workflow outputs and placing them in the correct Workspace S3 Object Store location, ready for harvesting and ingesting into the Resource Catalogue. This leads to a major requirement for OGC Application Packages: that they produce valid STAC catalog outputs containing a catalog.json file and any number of STAC Features and assets. You can see a simple function that constructs a minimal STAC Catalog and STAC Items here. Upon workflow completion the generated STAC Catalog (catalog.json) file must be present in the set of results files, and it must point to any STAC Features you also wish to be output from the workflow. An example catalog.json file can be seen below:</li> </ul> <pre><code>{\n  \"stac_version\": \"1.0.0\",\n  \"id\": \"catalog\",\n  \"type\": \"Catalog\",\n  \"description\": \"Root catalog\",\n  \"links\": [\n    {\n      \"type\": \"application/geo+json\",\n      \"rel\": \"item\",\n      \"href\": \"item.json\"\n    },\n    {\n      \"type\": \"application/json\",\n      \"rel\": \"self\",\n      \"href\": \"catalog.json\"\n    }\n  ]\n}\n</code></pre> <p>And an example of a STAC Feature, item.json, generated from the same workflow:</p> <pre><code>{\n  \"stac_version\": \"1.0.0\",\n  \"id\": \" item -1728909682.980245290\",\n  \"type\": \"Feature\",\n  \"geometry\": {\n  \"type\": \"Polygon\",\n  \"coordinates\": [\n    [\n      [-180, -90],\n      [-180, 90],\n      [180, 90],\n      [180, -90],\n      [-180, -90]\n    ]\n    ]\n  },\n  \"properties\": {\n    \"created\": \"2024-10-14T12:41:22.980Z\",\n    \"datetime\": \"2024-10-14T12:41:22.980Z\",\n    \"updated\": \"2024-10-14T12:41:22.980Z\"\n  },\n  \"bbox\": [-180, -90, 180, 90],\n  \"assets\": {\n    \" item\": {\n      \"type\": \"image/png\",\n      \"roles\": [\"data\"],\n      \"href\": \"item.png\",\n      \"file:size\": 19133\n    }\n  },\n  \"links\": [\n    {\n      \"type\": \"application/json\",\n      \"rel\": \"parent\",\n      \"href\": \"catalog.json\"\n    },\n    {\n      \"type\": \"application/geo+json\",\n      \"rel\": \"self\",\n      \"href\": \"item.json\"\n    },\n    {\n      \"type\": \"application/json\",\n      \"rel\": \"root\",\n      \"href\": \"catalog.json\"\n    }\n  ]\n}\n</code></pre> <p>It is vital that these outputs are generated correctly and are captured in the workflow outputs as the Workflow Runner uses the links in these files to ensure the outputs are captured and harvested into the Resource Catalogue.</p> <p>Info</p> <p>While a STAC Catalog and Item are required as outputs, as these are used by the STAGEOUT step to gather the outputs, you do not need to provide a STAC Collection. If you omit this, the STAGEOUT step will generate a Collection automatically, using the jobID for the execution to generate the Collection ID <code>col_&lt;jobID&gt;</code>.</p> <p>If you do provide STAC Collections in your outputs, and link to these from your Catalog.json file, these will be harvested as they are, without the need for a new collection to be generated by the STAGEOUT.</p>"},{"location":"documentation/workflow-runner/application-packages/#advanced-stac-outputs","title":"Advanced STAC Outputs","text":"<p>The Workflow Runner will harvest your STAC Catalog exactly as found in the outputs, including the IDs of your Catalog, collections and items. This means you can generate data deterministically in your workspace catalogue, so ensure the IDs you use are useful to you, as this will specify where you can find your outputs in your section of the Resource Catalogue. You are also able to leave your Catalog and Collection IDs blank, see example below, if you wish to have the STAGEOUT generate them based on the jobID of your execution. In this case your Catalog and Collection IDs will be rewritten to <code>cat-&lt;jobID&gt;</code> and <code>col-&lt;jobID&gt;</code> respectively.</p> <pre><code>{\n  \"stac_version\": \"1.0.0\",\n  \"id\": \"\",\n  \"type\": \"Catalog\",\n  \"description\": \"Root catalog\",\n  \"links\": [\n    {\n      \"type\": \"application/geo+json\",\n      \"rel\": \"item\",\n      \"href\": \"item.json\"\n    },\n    {\n      \"type\": \"application/json\",\n      \"rel\": \"self\",\n      \"href\": \"catalog.json\"\n    }\n  ]\n}\n</code></pre>"},{"location":"documentation/workflow-runner/application-packages/#workflow-restrictions","title":"Workflow Restrictions","text":"<p>There are certain restrictions that the Workflow Runner requires when attempting to deploy a workflow to the Hub:</p> <ul> <li>Any steps in your top level workflow cannot begin with the keywords <code>node_stage_in</code> or <code>node_stage_out</code> as these are reserved for the Workflow Runner internal steps</li> <li>Your workflow cannot include the following inputs, as, again, they are reserved for internal use only<ul> <li>STAGE_AWS_ACCESS_KEY_ID</li> <li>STAGE_AWS_SECRET_ACCESS_KEY</li> <li>STAGE_AWS_SESSION_TOKEN</li> <li>CALLING_WORKSPACE_ACCESS_TOKEN</li> </ul> </li> </ul> <p>Note</p> <p>Attempting to deploy a workflow that does not meet these criteria will result in a 400 Bad Request response, with the content detail clarifying what caused the error.</p>"},{"location":"documentation/workflow-runner/authentification/","title":"Authentication","text":"<p>The Workflow Runner uses workspace-scoped API Tokens to authorize access to workflows and processing. This means any requests are made for a specific workspace, determining access to workflows as well as specifying which workspace a workflow will be executed in.</p> <p>In order to make use of the Workflow Runner API, you will need to make authorized requests using a workspace-scoped API Token. These can be generated from the Workspaces UI. Make sure you first login to the Hub and then select the workspace you wish to use from the list on the left of the page. You can be a member of multiple workspaces, so this may be a substantial list. Always ensure you have selected the correct workspace which you want to use to access the Workflow Runner, as this will determine which workflows you have access to as well as who will be charged for your processing usage. You can confirm the workspace in the <code>scope</code> of the token once you have requested a new token.</p> <p>To use this token in your API requests you should send it in the Authorization header of your requests, as a Bearer token, for example when sending a request with CURL</p> <pre><code>curl https://staging.eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/processes/&lt;workflow-id&gt; -H 'Authorization: Bearer &lt;api-token&gt;'\n</code></pre> <p>Or with the Python requests library</p> <pre><code>import requests\n\nresponse = requests.get( \n  \"https://staging.eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/processes/&lt;workflow-id&gt;\", \n  headers={\n    \"Authorization\": \"Bearer &lt;api-token&gt;\"\n  }\n)\n</code></pre> <p>All workflows are registered to a specific workspace in which they were deployed. Therefore, to access workflows, you will need to specify the workspace you are trying to access within the URL, as follows: </p> <pre><code>https://staging.eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/processes\n</code></pre> <p>By default you will have access to any workflows defined in your workspace, so provided your workspace-scoped token aligns with that in the URL, you can access and make changes to the workflows within that workspace. Any workflows you deploy will be set as private meaning only those who can access the workspace that defined them, and therefore can take out workspace-scoped tokens for that workspace, can access them. However, you can also publish workflows for other users to access and execute. To do this you will need to update the access policy for your workspace to set the workflow either as public or as a user-service, meaning other users can access and execute the workflow from their own workspaces.</p> <p>Further guidance on updating access policies can be found here in the Hub documentation.</p>"},{"location":"documentation/workflow-runner/getting-started/","title":"Getting started with Workflows","text":"<p>To get started with the Workflow Runner (WR) a number of example EODH workflows and a Jupyter Notebook that will guide you through deploying and executing them are provided in this repository. The examples take you through deploying, viewing and executing workflows using Python in a Jupyter Notebook. You can deploy workflows from URLs pointing to CWL scripts as well as from local files, passed in as the data parameter of the HTTP request.</p>"},{"location":"documentation/workflow-runner/getting-started/#setting-up-your-account","title":"Setting Up Your Account","text":"<p>Before you start using the WR you will need to create a Hub account and then set up an account and workspace under manage account. You account will need to be approved by a Hub Administrator, and can take a few hours to be created but if your workspace is still not available after a number of hours, please contact the Hub Admin team.</p>"},{"location":"documentation/workflow-runner/getting-started/#some-basic-requests","title":"Some Basic Requests","text":"<p>Once you have a workflow you wish to run on the Hub, you will need to send a number of HTTP requests to first deploy your workflow and then execute it with your inputs. These steps will use the convert-url workflow defined here, so download this file and save it locally under the file name <code>convert-url-app.cwl</code>. This is a Common Workflow Language definition for our workflow which uses a Docker image created from this repo. This workflow accepts an image as an input and resizes it by a given percentage scale factor.</p> <p>We will build up an HTTP file that looks something like this file, to allow you to repeat these steps in future and deploy and execute other workflows. While you are able to send these HTTP requests using CURL or Postman, it is recommended you use an HTTP client in your IDE to allow you to send these requests directly from your code. The following examples will assume you have the HTTP client installed in your IDE, but you can of course rewrite the requests to work with any extensions you wish, as they are standard HTTP requests.</p>"},{"location":"documentation/workflow-runner/getting-started/#set-environment-variables-to-be-used-in-requests","title":"Set environment variables to be used in requests","text":"<p>Create an HTTP file called <code>workflow-runner.http</code> and copy the following code into it.</p> <pre><code>@api_token = &lt;your-token&gt;\n\n@scheme = https\n@hubDomain = eodatahub.org.uk\n@api = api/catalogue/stac/catalogs/user/catalogs\n@workspace = &lt;workspace-name&gt;\n\n@executeStatus = {{executeProcess.response.headers.Location}}\n</code></pre> <p>The variables defined here, preceded by <code>@</code>, are usable in subsequent requests by encasing them in double curly braces, e.g. <code>{{&lt;variable&gt;}}</code>, so that they are correctly replaced when the request is run.</p> <p>Now extract an API Token from the Workspaces UI by selecting Credentials from the left menu. Select <code>Request New Token</code> and copy the generated API Key into your HTTP file. You can also set the workspace to be your workspace name.</p> <p>The bottom line sets up some variables that will be set by some of the responses to our requests, making it easier to send some later requests, rather than having to copy links from the responses, so leave these as they are for now.</p>"},{"location":"documentation/workflow-runner/getting-started/#set-up-your-first-requests","title":"Set up your first requests","text":"<p>Next we can start writing some basic requests which will help set up the Hub to execute our workflow.</p> <p>Copy the below into your HTTP file to set up a request that will list the available processes you have in your workspace. It is important that you include <code>###</code> after each request to properly separate them and ensure they compile with the extension.</p> <pre><code>###\n# @name listProcesses\nGET {{scheme}}://{{hubDomain}}/{{api}}/{{workspace}}/processes HTTP/1.1\nAuthorization: Bearer {{api_token}}\nAccept: application/json\n###\n</code></pre> <p>This request sends a GET request to get the list of available processes in the selected workspace. We also define a number of headers on the lines below the request, e.g. the Authorization header containing your API Token and the Accept header specifying the Content Type we expect to receive in response.</p> <p>By default, this will return a workflow called <code>echo</code> that shows the sort of details you will get in a response to this request. You must send your API Token in this request as the workflows in your workspace will be private, meaning only you are able to view, execute and make changes to them.</p> <p>The next request will allow us to view some more details about this workflow, so copy this into your file and try running it.</p> <pre><code>###\n# @name getProcess\nGET {{scheme}}://{{hubDomain}}/{{api}}/{{workspace}}/processes/echo HTTP/1.1\nAuthorization: Bearer {{api_token}}\nAccept: application/json\n###\n</code></pre> <p>You will get a JSON response providing some more details about the workflow, including the ID, description and any inputs and outputs. This is only an example workflow so it isn't very useful for us now, but we can use this request to get details about any other workflows we have access to, so feel free to try sending this again exchanging the <code>echo</code> workflow ID for any other ID you wish to view.</p>"},{"location":"documentation/workflow-runner/getting-started/#deploying-a-workflow","title":"Deploying a Workflow","text":"<p>Now we are ready to try deploying our <code>convert-url</code> workflow to the Hub Workflow Runner in preparation for doing some executing on the Platform. The following request will deploy your workflow from a local file, so make sure you have your <code>convert-url-app.cwl</code> file in the same directory as your <code>workflow-runner-http</code> file. Copy the following block into your file.</p> <pre><code>###\n# @name deployProcessFile\nPOST {{scheme}}://{{hubDomain}}/{{api}}/{{workspace}}/processes HTTP/1.1\nAuthorization: Bearer {{api_token}}\nAccept: application/json\nContent-Type: application/cwl+yaml\n\n&lt; convert-url-app.cwl\n###\n</code></pre> <p>This request will take the local CWL file and deploy the script to the Workflow Runner, ready for future execution. This should result in a <code>204</code> response meaning the workflow was correctly deployed and is ready for execution. You can check the workflow is now available by using the request from above to view more details about the workflow.</p> <pre><code>###\n# @name getProcess\nGET {{scheme}}://{{hubDomain}}/{{api}}/{{workspace}}/processes/convert-url HTTP/1.1\nAuthorization: Bearer {{api_token}}\nAccept: application/json\n###\n</code></pre>"},{"location":"documentation/workflow-runner/getting-started/#executing-a-workflow","title":"Executing a Workflow","text":"<p>Now that we have confirmed the workflow was successfully deployed it is ready for us to execute on the Hub. To do this we need to provide some inputs as defined in the CWL script. The <code>convert-url</code> workflow expects three inputs:</p> <ul> <li>fn: The function to compute on the inputs</li> <li>url: The URL to the image we wish to process</li> <li>size: The scale factor, as a percentage, to rescale the image by</li> </ul> <p>Here, we will compute the <code>resize</code> function on the image located here and rescale it by 50%, constructing the following request.</p> <pre><code>###\n# @name executeProcess\nPOST {{scheme}}://{{hubDomain}}/{{api}}/{{workspace}}/processes/convert-url/execution HTTP/1.1\nAuthorization: Bearer {{api_token}}\nAccept: application/json\nContent-Type: application/json\nPrefer: respond-async\n\n{\n  \"inputs\": {\n      \"fn\": \"resize\",\n      \"url\":  \"https://raw.githubusercontent.com/EO-DataHub/eodhp-ades-demonstration/refs/heads/main/DataHub-Logo.PNG\",\n      \"size\": \"50%\"\n  }\n}\n###\n</code></pre> <p>You will get a <code>201</code> response from the API stating that the job has been created which means your workflow is now executing. The JSON response will also include the jobID for this execution, which we can use to view more details about the Job while it is progressing, so make sure you keep this for the following requests. You should also note that the location of the status endpoint is included in the <code>Location</code> header of the response, for example https://staging.eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs//jobs/. <p>The line at the top of our HTTP file extracts this header automatically into the executeStatus variable: {{executeProcess.response.headers.Location}}, more on this in the next section.</p>"},{"location":"documentation/workflow-runner/getting-started/#monitoring-workflow-progress","title":"Monitoring Workflow Progress","text":"<p>After executing a workflow you can monitor its status using the Location header returned from the previous request which we now have saved into the <code>executeStatus</code> variable, as follows.</p> <pre><code>###\n# @name getExecuteStatus\nGET {{executeStatus}} HTTP/1.1\nAuthorization: Bearer {{api_token}}\nAccept: application/json\n###\n</code></pre> <p>This request uses the variable we defined above, but we can also construct this URL manually using the jobID we recorded above.</p> <pre><code>###\n# @name jobStatus\nGET {{scheme}}://{{hubDomain}}/{{api}}/{{workspace}}/jobs/&lt;jobID&gt; HTTP/1.1\nAuthorization: Bearer {{api_token}}\nAccept: application/json\n###\n</code></pre> <p>This request returns JSON defining some details about the workflow execution. This includes the current status, such as <code>running</code> or <code>successful</code>, and a message giving additional details, such as <code>execution submitted</code>, <code>delivering outputs, logs and usage report</code> or <code>ZOO-Kernel successfully run your service!</code></p> <p>Upon workflow completion this endpoint will also return logs generated during workflow execution. These will be populated under the <code>links</code> field. Look for those with a title beginning with <code>Tool log ...</code>. You can open these links in a browser to view the full log files generated by your workflow.</p> <p>Next, we can view the workflow outputs by calling the results endpoint provided by the API.</p> <pre><code>###\n# @name jobResults\nGET {{scheme}}://{{hubDomain}}/{{api}}/{{workspace}}/jobs/&lt;jobID&gt;/results HTTP/1.1\nAuthorization: Bearer {{api_token}}\nAccept: application/json\n###\n</code></pre> <p>You'll notice this endpoint is simply the status endpoint from before but with the <code>/results</code> prefix appended to the end.</p> <p>This request returns a STAC Collection containing the STAC Items generated by your workflow. These items are listed under the <code>links</code> in the Collection, with each link pointing to the location in the S3 Object Store. Look for the item link and open one of these <code>href</code> in your browser to view the raw STAC item generated by the workflow. The images generated by your workflow can be found in the assets of these items. For our example workflow, look for the PNG file asset in the STAC Item, which will contain the <code>.png</code> suffix in the filename. You can download and open this file and see that it is indeed the rescaled image from our inputs.</p>"},{"location":"documentation/workflow-runner/getting-started/#additional-endpoints","title":"Additional Endpoints","text":"<p>The Workflow Runner also includes some other useful endpoints which we will briefly discuss here. But please feel free to try them out should you wish to advance your understanding.</p>"},{"location":"documentation/workflow-runner/getting-started/#listing-jobs","title":"Listing Jobs","text":"<p>You are able to list all the jobs that have been executed in your workspace using the following request</p> <pre><code>###\n# @name getJobs\nGET {{scheme}}://{{hubDomain}}/{{api}}/{{workspace}}/jobs HTTP/1.1\nAuthorization: Bearer {{api_token}}\nAccept: application/json\n###\n</code></pre>"},{"location":"documentation/workflow-runner/getting-started/#deleting-jobs","title":"Deleting Jobs","text":"<p>You can also delete a job while it is running, should it be taking longer than expected, for example.</p> <pre><code>###\n# @name deleteJob\nGET {{scheme}}://{{hubDomain}}/{{api}}/{{workspace}}/jobs/&lt;jobID&gt; HTTP/1.1\nAuthorization: Bearer {{api_token}}\nAccept: application/json\n###\n</code></pre>"},{"location":"documentation/workflow-runner/getting-started/#updating-a-workflow","title":"Updating a Workflow","text":"<p>You can also update a previously deployed workflow, should you wish to change the version or some other details in the CWL definition. Make sure the workflow ID in your request URL matches that defined in the CWL script. <pre><code>###\n# @name updateProcessFile\nPUT {{scheme}}://{{hubDomain}}/{{api}}/{{workspace}}/processes/convert-url HTTP/1.1\nAuthorization: Bearer {{api_token}}\nAccept: application/json\nContent-Type: application/cwl+yaml\n\n&lt; convert-url-app.cwl\n###\n</code></pre></p>"},{"location":"documentation/workflow-runner/getting-started/#deploying-via-url","title":"Deploying via URL","text":"<p>You can also deploy and update workflows by passing the URL to your workflow, rather than first downloading it and then sending it as data in the request. To do this you need the URL to the raw CWL file located at a public address.</p> <pre><code>###\n# @name deployProcessURL\nPOST {{scheme}}://{{hubDomain}}/{{api}}/{{workspace}}/processes HTTP/1.1\nAuthorization: Bearer {{api_token}}\nAccept: application/json\nContent-Type: application/json\n\n{\n  \"executionUnit\": {\n    \"href\": \"https://raw.githubusercontent.com/EO-DataHub/ades-workflow-examples/refs/heads/main/working-url/convert-url-app.cwl\",\n    \"type\": \"application/cwl\"\n  }\n}\n</code></pre>"},{"location":"documentation/workflow-runner/getting-started/#deleting-a-workflow","title":"Deleting a Workflow","text":"<p>Finally, you can remove workflows from your workspace, should you no longer want it to be available to you.</p> <pre><code>###\n# @name deleteProcess\nDELETE {{scheme}}://{{hubDomain}}/{{api}}/{{workspace}}/processes/convert-url HTTP/1.1\nAuthorization: Bearer {{api_token}}\nAccept: application/json\n###\n</code></pre>"},{"location":"documentation/workflow-runner/getting-started/#more-examples","title":"More Examples","text":"<p>The following repositories are also available to demonstrate WR functionality:</p> <ul> <li>ades-workflow-examples \u2013 a set of workflows that demonstrate both successful executions as well as a range of failure modes to present the logs that will be available in each case. This repository also incudes shell scripts and Dockerfiles demonstrating how the workflows were created.</li> <li>eodhp-ades-workspace-access \u2013 a set of workflows that demonstrate workspace data access within workflows. One shows that workspace S3 data can be accessed within a workflow, and the other demonstrates GDAL functionality within a workflow step.</li> <li>Integration with pyeodh \u2013 you are also able to deploy and execute workflows using the pyeodh client. This Jupyter Notebook includes some example functions to demonstrate this functionality.</li> </ul> <p>The examples in these repositories can be used as a template to start writing your own CWL and workflow scripts and the code will also get you familiar with the endpoints provided by the WR and the sort of results you might expect.</p>"},{"location":"documentation/workflow-runner/introduction/","title":"Introducing the Workflow Runner","text":""},{"location":"documentation/workflow-runner/introduction/#what-is-the-workflow-runner-and-how-can-i-use-it","title":"What is the Workflow Runner and how can I use it?","text":"<p>The Workflow Runner (WR) component of the DataHub allows users and applications to execute workflows using the processing available on the Hub. This allows for complex computation to be carried out on datasets remotely, with the outputs available to the users and applications via the Resource Catalogue (RC) and Workspace Object Storage (S3) upon completion.</p> <p>Interaction with the WR is handled via an API, with endpoints provided to deploy, update, delete and execute workflows as well as for monitoring the execution of a job once started. Logs are also available upon workflow completion. Swagger API documentation is available for the WR API here. This page can be used to be view the provided endpoints, including any expected inputs, as well as send requests to the API, should you wish to try using the WR. Note, to send requests you will need a Hub account and workspace set up, documentation is available here to get you started.</p> <p>The main process when using the workflow runner is as follows:</p> <ul> <li>User generates access credentials</li> <li>User deploys their workflow to a workspace via the WR API</li> <li>User executes their workflow providing any inputs as required by the workflow specification</li> <li>User monitors their workflow execution</li> <li>Upon workflow completion, user can view workflow outputs in their workspace - either in the Resource Catalogue or S3 object store</li> </ul>"},{"location":"documentation/workflow-runner/introduction/#advanced-functionality","title":"Advanced Functionality","text":"<p>Users are also able to define access policies for their workflows. This allows for two additional workflow types to be defined by workflow developers.</p>"},{"location":"documentation/workflow-runner/introduction/#public-workflows","title":"Public Workflows","text":"<p>Public workflows are processes which other users can access and execute in their own workspace, independent of the owning workspace. The new user can provide their own inputs and will then be able to view the outputs in their own workspace stores. Workflows are private by default but can be configured to be public in a workspace access policy file.</p>"},{"location":"documentation/workflow-runner/introduction/#user-services","title":"User-Services","text":"<p>A user-service is a workflow that is available publicly to other workspaces, but will only be executed within the owning workspace. This allows workflow developers to publish their workflows for other Hub users, providing a service to these users. This ensures that any processing usage is recorded against the user-service owner, rather than the calling users. It also means the workflow can access data that is private to the user-service owning workspace, that the calling user would otherwise not have access to.</p>"},{"location":"documentation/workflow-runner/introduction/#configuring-workflows","title":"Configuring Workflows","text":"<p>By default, all workflows are private to users in the workspace that deployed the workflow. If you wish to configure one of your workflows to be public or a user-service, this can be achieved by uploading your own access-policy file via the Hub Data-Loader, in the workspaces page. For any other help on this, please contact a Hub administrator.</p>"},{"location":"documentation/workflow-runner/local-testing/","title":"Local testing","text":"<p>Before deploying a workflow to the DataHub Workflow Runner you can test your CWL script locally using packages such as cwltool. This allows you to run the workflow locally on your computer, without waiting for Hub scheduling and log handling. You can provide the inputs you wish to use on the Hub, but we suggest you use a smaller dataset for initial testing to avoid long run times. This can help with early bug discovery and fixing. You should also ensure the output results match the required STAC format and that all results are generated in the correct directory. If you want to closely mimic the Hub invocation you can define a .json file with your inputs as below.</p> <pre><code>{\n  \"inputs\": {\n      \"fn\": \"resize\",\n      \"url\": \"https://raw.githubusercontent.com/EO-DataHub/test-workflow-store/main/EO-Data-Hub-logo_colour_425px.png\",\n      \"size\": \"50%\"\n  }\n}\n</code></pre> <p>Then pass these inputs into your workflow when executing it on the command line.</p> <pre><code>cwltool &lt;workflow-filename&gt;.cwl#&lt;workflow-id&gt; inputs.json\n</code></pre> <p>You must remember to specify the name of a step in your workflow, or otherwise set it to the id of the top-level workflow. For example in this workflow, the top-level workflow has the id \u201cconvert-url\u201d as this is specified in the workflow class.</p> <pre><code>$graph:\n  # Workflow entrypoint\n  - class: Workflow\n    id: convert-url\n    label: convert url app\n    doc: Convert URL\n</code></pre>"},{"location":"documentation/workflow-runner/monitoring-workflows/","title":"Monitoring workflows","text":"<p>Once you have executed a workflow, there are a few endpoints provided in the API that allow you to monitor the progress of the execution, as well as view outputs and resulting log files, to see additional details about the workflow execution.</p>"},{"location":"documentation/workflow-runner/monitoring-workflows/#endpoints-for-job-monitoring","title":"Endpoints for Job Monitoring","text":"<p>Once you have executed your workflow, using the <code>/execution</code> endpoint, you will see a \u201cjobID\u201d key in the JSON HTTP response, as shown below. This provides the UUID for the job that is running your workflow. You can then use this jobID to view details about the status of your workflow execution, as well as view the resulting collection upon completion.</p> <pre><code>{\n  \"id\": \"c7bdbb50-1477-11f0-922f-c217930f6509\",\n  \"jobID\": \"c7bdbb50-1477-11f0-922f-c217930f6509\",\n  \"type\": \"process\",\n  \"processID\": \"workflow-test\",\n  \"created\": \"2025-04-08T12:48:33.675Z\",\n  \"started\": \"2025-04-08T12:48:33.675Z\",\n  \"finished\": \"2025-04-08T12:49:31.731Z\",\n  \"updated\": \"2025-04-08T12:49:31.509Z\",\n  \"status\": \"running\",\n  \"message\": \"ZOO-Kernel accepted to run your service!\",\n  \"links\": [\n    {\n      \"title\": \"Status location\",\n      \"rel\": \"monitor\",\n      \"type\": \"application/json\",\n      \"href\": \"https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/c7bdbb50-1477-11f0-922f-c217930f6509\"\n    }\n  ]\n}\n</code></pre> <p>In this response, you can see the <code>Status location</code> link which provides a URL at which you can view the status of your job, following the below format.</p> <pre><code># View job status\n/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/&lt;jobID&gt;\n</code></pre> <p>Using this status endpoint, once your workflow execution has completed, you will be able to find some additional links.</p> <pre><code>[\n    {\n      \"title\": \"Result location\",\n      \"rel\": \"http://www.opengis.net/def/rel/ogc/1.0/results\",\n      \"type\": \"application/json\",\n      \"href\": \"https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/c7bdbb50-1477-11f0-922f-c217930f6509/results\"\n    },\n    {\n      \"href\": \"https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/c7bdbb50-1477-11f0-922f-c217930f6509/workflow-step.log\",\n      \"title\": \"Tool log workflow-step.log\",\n      \"rel\": \"related\",\n      \"type\": \"text/plain\"\n    },\n    {\n      \"href\": \"https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/c7bdbb50-1477-11f0-922f-c217930f6509/node_stage_out.log\",\n      \"title\": \"Tool log node_stage_out.log\",\n      \"rel\": \"related\",\n      \"type\": \"text/plain\"\n    }\n]\n</code></pre> <p>These links will point you to URLs enabling you to view the output results and any log files, the first of which is the output results endpoint.</p> <pre><code># View output results\n/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/&lt;job_id&gt;/results\n</code></pre> <p>The jobID will also be used to view logs when they are available via HTTPS, should you wish to see more details about you workflow execution. The log links are returned from the execute status endpoint above, but follow this format:</p> <pre><code># View job log file\n/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/&lt;job_id&gt;/&lt;filename&gt;.log\n</code></pre>"},{"location":"documentation/workflow-runner/results/","title":"Workflow results","text":"<p>The final step of each workflow execution is the STAGEOUT which is executed automatically by the Workflow Runner. This steps handles the files generated by the workflow and prepares them to be ingested into the workspace Object Storage and Resource Catalogue sub-catalog.</p>"},{"location":"documentation/workflow-runner/results/#harvesting-outputs","title":"Harvesting Outputs","text":"<p>The STAGEOUT determines which files to harvest by first locating the catalog.json file that must be generated by the workflow execution. The STAGEOUT then parses this file and identifies any contained STAC Collections, Items and Assets. You workflow does not need to generate STAC Collections, and you are free to just link directly to your STAC Items from the STAC Catalog if you wish instead of including any Collections. A Collection will be added automatically by the STAGEOUT, with the ID <code>col_&lt;jobID&gt;</code> based on the jobID of the job that generated these outputs.</p> <p>The STAGEOUT corrects some of the links in the STAC Catalog to ensure the links are correct and also updates any Asset links to ensure they are available from the Object Store via HTTPS.</p> <p>These files are then exported to a workspace-specific directory within the workspace Object Store, depending on the workspace which invoked the workflow execution. After this step, the outputs are harvested into the calling workspace sub-catalog within the Resource Catalogue. Any workflow outputs are currently saved to a new sub-catalog within the \u201cprocessing-results\u201d sub-catalog of the workspace catalog, depending on the ID of the generated catalog.json file. You can then view these outputs in the Resource Catalogue API, for example at <code>/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/catalogs/processing-results/...</code>. You can also access outputs directly from the Object Store either via an S3 client or HTTPS, for example <code>https://&lt;workspace&gt;.eodatahub-workspaces.org.uk/files/workspaces-eodhp/processing-results/...</code>.</p>"},{"location":"documentation/workflow-runner/results/#advanced-output-functionality","title":"Advanced Output Functionality","text":"<p>By default, the STAC Catalog, Collections and Items harvested from workflow outputs will retain the IDs as set in the actual workflow generated files. This means that should a workflow generate STAC outputs with the same ID on each run, the data will be overwritten in the catalogue after each execution. This can allow you to run the same workflow regularly with the same output IDs, meaning the data in the generated catalog will always be updated to the latest outputs. This might be useful if you want a single source of truth that always contains the latest data outputs.</p> <p>Should you wish for your STAC Collections and Items to be added to the same Catalog, alongside the previous results, you will need to ensure these are assigned unique IDs, to avoid overwriting. You could add a timestamp to the ID or just add a UUID as a suffix.</p> <p>If instead you wish for the Workflow Runner STAGEOUT to assign IDs to your Catalog and Collections, based on the jobID for the job that generated the outputs, you can leave these IDs blank, setting them to \"\", and they will be rewritten as <code>cat_&lt;jobID&gt;</code> and <code>col_&lt;jobID&gt;</code> respectively. Note, this only works for a single Collection output, as otherwise the data will be overwritten within your collections.</p>"},{"location":"documentation/workflow-runner/results/#publishing-outputs","title":"Publishing Outputs","text":"<p>By default, any data within a workspace catalog is private to users who are members of the workspace, i.e. those who can extract tokens scoped to that workspace. Therefore, only users with access to the workspace that called the workflow are able to access the results. You will need to be authenticated before attempting to view the results, whether using the API or a browser. The same goes for any HTTP requests when trying to access the data in the Object Store, such as the generated asset files.</p> <p>You are able to publish data in a private workspace data store, both in the Object Store and the Resource Catalogue. You can do this using the Data Loading functionality offered in the Workspaces UI.</p>"},{"location":"documentation/workflow-runner/supported-endpoints/","title":"Supported endpoints","text":"<p>The Workflow Runner offers a number of endpoints via an API to interact with the available workflows, deploying new ones to your workspaces, executing them and monitoring their progress. The easiest way to learn the available functionality is to view the Swagger Documentation for the API, which includes all the available endpoints and allows you to send requests directly from the Swagger UI. Note, you will need to authorize with a workspace-scoped API token via the Authorize switch in the top right.</p>"},{"location":"documentation/workflow-runner/supported-endpoints/#available-endpoints","title":"Available Endpoints","text":"<p>The following endpoints are currently supported by the Workflow Runner API. The first line of each defines the request format and any subsequent lines define the different types of HTTP requests supported and the expected response.</p>"},{"location":"documentation/workflow-runner/supported-endpoints/#get","title":"GET","text":"<ul> <li> <p><code>https://eodatahub.org.uk/api/docs/workflow-runner</code></p> <ul> <li>Get - Workflow Runner Swagger Docs</li> </ul> </li> <li> <p><code>https://eodatahub.org.uk/api/docs/workflow-runner/ping</code></p> <ul> <li>Get - Status endpoint to check API availability</li> </ul> </li> <li> <p><code>https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/&lt;jobID&gt;/results</code></p> <ul> <li>GET - View workflow results collection for specified job</li> </ul> </li> <li> <p><code>https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs</code></p> <ul> <li>GET - View all jobs in specified workspace</li> </ul> </li> <li> <p><code>https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/&lt;jobID&gt;/filename.log</code></p> <ul> <li>GET - View specified log file for given job</li> </ul> </li> <li> <p><code>https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/&lt;jobID&gt;</code></p> <ul> <li>GET - View job status</li> </ul> </li> </ul>"},{"location":"documentation/workflow-runner/supported-endpoints/#post","title":"POST","text":"<ul> <li><code>https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/processes/&lt;workflowID&gt;/execution</code><ul> <li>POST - Execute workflow using provided inputs</li> </ul> </li> </ul>"},{"location":"documentation/workflow-runner/supported-endpoints/#delete","title":"DELETE","text":"<ul> <li><code>https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/&lt;jobID&gt;</code><ul> <li>DELETE - Remove job job</li> </ul> </li> </ul>"},{"location":"documentation/workflow-runner/supported-endpoints/#get-post","title":"GET / POST","text":"<ul> <li><code>https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/processes</code><ul> <li>GET - View all workflows in given workspace</li> <li>POST - Deploy new workflow to the workspace</li> </ul> </li> </ul>"},{"location":"documentation/workflow-runner/supported-endpoints/#get-post-delete","title":"GET / POST / DELETE","text":"<ul> <li><code>https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/processes/&lt;workflowID&gt;</code><ul> <li>GET - View workflow details for specified workflow</li> <li>PUT - Update workflow in the workspace</li> <li>DELETE - Un-deploy workflow from the workspace</li> </ul> </li> </ul>"},{"location":"documentation/workflow-runner/workflow-logs/","title":"Workflow Logs","text":"<p>Once your workflow execution has completed, either successfully or with a failure, you will be able to view logs detailing the execution process. Links to logs are provided in response to the job status request mentioned in the Getting Started guide and can be viewed in the browser. Note, it can sometimes take up to 30 seconds for the logs to be generated and returned by this endpoint, so please send the request again if the links are initially missing.</p> <p>The below gives an example response to the status endpoint, which contains links to the logs generated for this workflow execution. This include a log for the built-in STAGEOUT step, titled <code>Tool log node_stage_out.log</code>, as well as the workflow step itself, here titled <code>Tool log test-step.log</code>.</p> <pre><code>{\n  \"id\": \"c7bdbb50-1477-11f0-922f-c217930f6509\",\n  \"jobID\": \"c7bdbb50-1477-11f0-922f-c217930f6509\",\n  \"type\": \"process\",\n  \"processID\": \"workflow-test\",\n  \"created\": \"2025-04-08T12:48:33.675Z\",\n  \"started\": \"2025-04-08T12:48:33.675Z\",\n  \"finished\": \"2025-04-08T12:49:31.731Z\",\n  \"updated\": \"2025-04-08T12:49:31.509Z\",\n  \"status\": \"successful\",\n  \"message\": \"ZOO-Kernel successfully run your service!\",\n  \"links\": [\n    {\n      \"title\": \"Status location\",\n      \"rel\": \"monitor\",\n      \"type\": \"application/json\",\n      \"href\": \"https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/c7bdbb50-1477-11f0-922f-c217930f6509\"\n    },\n    {\n      \"title\": \"Result location\",\n      \"rel\": \"http://www.opengis.net/def/rel/ogc/1.0/results\",\n      \"type\": \"application/json\",\n      \"href\": \"https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/c7bdbb50-1477-11f0-922f-c217930f6509/results\"\n    },\n    {\n      \"href\": \"https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/c7bdbb50-1477-11f0-922f-c217930f6509/test-step.log\",\n      \"title\": \"Tool log test-step.log\",\n      \"rel\": \"related\",\n      \"type\": \"text/plain\"\n    },\n    {\n      \"href\": \"https://eodatahub.org.uk/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/jobs/c7bdbb50-1477-11f0-922f-c217930f6509/node_stage_out.log\",\n      \"title\": \"Tool log node_stage_out.log\",\n      \"rel\": \"related\",\n      \"type\": \"text/plain\"\n    }\n  ]\n}\n</code></pre> <p>A number of log files are produced when a workflow finished executing, these include:</p> <ul> <li>node_stage_in.log \u2013 logs from the STAGEIN step of the workflow, these can be useful if your inputs to the workflow are incorrect or not accessible to the Workflow Runner</li> <li>node_stage_out.log \u2013 logs from the STAGEOUT step of the workflow, these are useful when your workflow failed to generate the expected STAC files or they were generated incorrectly</li> <li>.log files \u2013 these are generated for each step in your CWL workflow and contain the stdout from the step execution code, helping you to determine errors within the steps of your workflow. It is suggested you try running your workflow locally before deploying to the Hub to avoid workflow errors as much as possible."},{"location":"documentation/workflow-runner/workflow-logs/#logging-authorisation","title":"Logging Authorisation","text":"<p>Logs require authentication and authorisation to view and only the workspace which initially requested the workflow execution, known as the calling workspace as described here, can view the associated status and logs.</p>"},{"location":"documentation/workflow-runner/workflow-logs/#logging-details","title":"Logging Details","text":"<p>In some cases, you will also see logging details included in the \"message\" key of the status response, although the separate logs will include more details about the workflow execution.</p> <p>If you receive any logging responses that are not clear to you, please don't hesitate to copy the response and share it with a Hub administrator for further assistance.</p> <p>Note</p> <p>Log handling is still under development, and more are available to Hub admins, so if you require additional log details please reach out to one of the Hub administrators providing the deploying workspace, workflowID and jobID for your executed workflow and they may be able to provide more information.</p>"},{"location":"documentation/workflow-runner/workflow-types/","title":"Workflow types","text":"<p>While most workflows will be deployed and executed within the same workspace, the Hub also offers functionality to publish workflows for other users to view and execute, with their own inputs. There are two main ways to publish workflows, one being to make them public and the other is to mark them as user-services.</p>"},{"location":"documentation/workflow-runner/workflow-types/#workflow-access-policies","title":"Workflow Access Policies","text":"<p>The Workflow Runner supports public workflows and user-services, allowing other workspaces to find and execute these workflows. This is configured by the deploying workspace when a user defines an access policy configuration file for that workflow, specifying the following:</p> <ul> <li>A workflow is public - available to all Hub users, executed within the calling workspace, with access only to the calling workspace.</li> <li>A workflow is a User-service - available to all Hub users but only executable within the owning workspace, not the calling workspace, with some additional access to the user-service owner's workspace.</li> </ul>"},{"location":"documentation/workflow-runner/workflow-types/#public-workflows","title":"Public Workflows","text":"<p>The Workflow Runner supports public workflows, allowing all workspaces to find and execute this workflow. When a workflow is set as public, it will be available to all Hub users and executed within the calling workspace.</p> <p>These workflows are available by searching for any workflows under the workspace that owns the workflow. For example, workspace X wishes to make use of a workflow defined by workspace Y. This workflow will be available at <code>api/catalogue/stac/catalogs/user/catalogs/Y/processes</code>, the Workflow Runner then checks the access policy for the workflow and if the workflow if public, workspace X can view and execute the workflow. The workflow will be executed entirely within workspace X, with inputs also coming from workspace X, if required.</p> <p>This workflow will be executed entirely within the calling workspace, meaning access is restricted in all steps to data within this workspace, irrespective of the deploying workspace.</p>"},{"location":"documentation/workflow-runner/workflow-types/#user-services","title":"User-Services","text":"<p>When a workflow is set as a User-service it is available to all Hub users but only executable within the owning workspace, not the calling workspace. This means the resource usage is attributed to the owning workspace, allowing developers to deploy their workflows as public services which they can then bill users for (future functionality).</p> <p>These workflows will appear to all users who attempt to find them under the owning workspace, but they will be executed in the deploying workspace. Inputs can still be provided from the calling workspace, and outputs will be delivered to the calling workspace as well.</p> <p>If workspace X wishes to use a user-services deployed by workspace Y, the workflow will be available at <code>api/catalogue/stac/catalogs/user/catalogs/Y/processes</code> and when executed it will be run within workspace Y, allowing initial inputs, if required, to be loaded form workspace X.</p> <p>The workflow steps in a user-service execution will have some additional access to the user-service owner's workspace, meaning reading file from HTTPS, S3 or the Block Store will work just the same, whether the workflow is called by the deploying workspace, or some other workspace.</p> <p>User-Service owners will also be able to view the status, results and logs from these workflows, even if they were not the original calling workspace, this should help with debugging where required. </p> <p>Info</p> <p>If no workflow configuration is provided, or it is invalid, the workflow defaults to private and can only be run by the someone with a workspace-scoped token scoped to the deploying workspace. </p>"},{"location":"documentation/workflow-runner/workspace-usage/","title":"Workspace usage","text":"<p>When you execute a workflow, it is automatically run within a dedicated workspace namespace in the cluster. This namespace is associated with the workspace you use to execute a workflow which also determines:</p> <ul> <li>The access a workflow has to data hosted on the Hub.</li> <li>Billing information for resource usage (coming soon)</li> </ul>"},{"location":"documentation/workflow-runner/workspace-usage/#workspace-scoped-tokens","title":"Workspace-Scoped Tokens","text":"<p>Workflows require workspace-scoped access tokens to access and execute. These tokens can be generated in the workspaces UI under <code>Credentials</code> in the left menu panel. It is the workspace defined in this token that will be used to determine workflow access when attempting to view the available workflows in a workspace. This workspace will also be used when executing a workflow, specifying the calling workspace, and often the executing workspace as well. This determines the data you are able to load within the workflow as well as where the workflow outputs will be saved.</p>"},{"location":"documentation/workflow-runner/workspace-usage/#workspace-interaction","title":"Workspace Interaction","text":"<p>When a workflow is executed, up to three workspaces can be involved: </p> <p>The deploying workspace:</p> <ul> <li>This workspace is defined in the URL of the workflow call: <code>/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/...</code></li> <li>This workspace defines the workspace that original defined and deployed the workflow to the Hub. This is the only workspace that can make changes to the workflow, including removing it from the workspace itself</li> <li>This workspace can also make changes to the access policy for the workflow, including setting it public or configuring it as a user-service, both allowing other workspaces to access and execute the workflow</li> </ul> <p>The calling workspace:</p> <ul> <li>This workspace is defined in the workspace-scoped token uses when making requests to the Workflow Runner, look at the scope of your API token which will look something like <code>workspace:&lt;workspace&gt;</code></li> <li>This workspace also specifies the data hosted on the Hub that the workflow will be able to access in the STAGEIN, either via HTTPS, S3 or via the mounted Block Store</li> <li>This workspace also configures where the workflow outputs will be harvested. It is in the private workspace catalogue that is associated with this workspace that the workflow outputs will be ingested. So outputs will be available in the processing-results sub-catalog within the workspace private catalog, accessed via the Resource Catalogue API: <code>/api/catalogue/stac/catalogs/user/catalogs/&lt;workspace&gt;/catalogs/processing-results/...</code></li> <li>This workspace also defines which workspace will be able to monitor the status of the workflow, using the status endpoints, as well as access the logs and workflow outputs as returned by the results endpoint.</li> </ul> <p>The executing workspace:</p> <ul> <li>This workspace is configured by the Workflow Runner itself, based on the calling workspace and the workflow access policy, and does not need to be provided by the user</li> <li>This workspace defines the namespace in which the workflow will actually be executed, specifying the data accessible to the steps of the workflow, via HTTPS, S3 or Block Store, as well as recording resource usage against the workspace to be used for accounting and billing services (future release)</li> <li>In most cases, this workspace will be set equal to the calling workspace, as workflows will usually be executed within the same workspace that original called for the execution</li> <li>For public workflows, the executing workspace will be equal to the calling workspace, as these workflows are taken from the deploying workspace and executed in the calling workspace, with inputs and workflow steps being allowed access to the calling workspace's private data</li> <li>For user-services, the executing workspace will be equal to the deploying workspace, as these workflows are executed within the user-service owner's workspace (which is equal to the deploying workspace, as discussed above). Workflow inputs can be taken from the calling workspace's private data while the executing steps can only access the deploying workspace's private data</li> </ul> <p>Note</p> <p>These workspaces work together to check workflow access, check the data access for a workflow inputs and execution, as well as recording resource usage for accounting and billing.</p> <p>Tip</p> <p>You only need to configure the deploying workspace and calling workspace, the Workflow Runner will automatically determine the executing workspace based on the workflow configuration.</p>"}]}